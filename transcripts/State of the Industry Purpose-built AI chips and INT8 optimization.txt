morning everybody today's episode of state of the industry is going to focus on hardware okay so first um i want to talk about intel because intel is the um you know it's the one of the world leaders in terms of uh uh cpus um they make desktop and server cpus as well as some mobile although arm and a few others dominate the mobile space so sapphire rapids is the code name for their upcoming release which i think is supposed to drop later this year planned for 2022 and so one thing that i saw someone shared this on the discord is that they're going to start supporting amx extension so what is that amx is advanced matrix extensions which basically gives it a little bit more native support for artificial intelligence applications um so this is a step up from avx which is advanced advanced vector extension so it basically turns it from one-dimensional matrices to n-dimensional matrices or two-dimensional matrices which is the bedrock of a.i today because all all deep neural networks use tensor math or matrix math okay so just wanted to shout give a shout out to uh to that to sapphire rapids which means oh so the other thing is um alder lake which is the desktop version is not going to have amx support um or at least rumored not to you see over here on the on the the extensions amx is missing it does still have a vx though so if we go back to sapphire rapids you see it's got avx up to 512 which is even bigger than what they have on the desktop so um basically what i predict that we're going to be seeing is that conventional hardware you know intel chips amd chips are going to start supporting more and more ai related math so that you may or may not need a separate gpu or tpu so right now if you want to do ai locally you need dedicate you need special hardware to accelerate it you can run it on a normal cpu but it's going to be sometimes 100 times slower okay so with that being said i wanted to also introduce the concept of and eight so basically what what this does is most weights are represented as floating point 32 or floating point 16 bit values which means that they are represented by two or four bytes total in order to represent each individual parameter but what happened what you can do what we found is that you can reduce that down to one byte which is uh eight bit and you actually get one it's much much smaller so it takes less memory less storage and the processing is faster and what happens is when you distill or change or quantize a neural network down to 8-bit it is the performance is much better but you don't actually you don't actually lose the the performance of it and so there's a few uh a few papers out here this one came out august 15th so uh almost a month ago um here is a hugging face article about it and i'll have all this in the in the comments um so a gentle introduction to 8-bit matrix multiplication for transformers at scale using hugging face transformers accelerate and bits and bytes um so yeah uh this is so basically i'm showing you the amx because just showing you that matrix math is getting built into cpus and in the meantime we're figuring out more efficiencies in in the transformer space so the hardware is ramping up but then also we're optimizing the neural networks and so what i hope to see before too long is that large language models will be able to run on conventional commodity hardware that'll be ideal uh okay so that's kind of the two the two uh opening things that i wanted to show you so sapphire rapids uh will have amx support which will make it faster to run matrices and then 8-bit matrices or 8-bit parameters are also on the rise okay so i mentioned quantization so what is that so quantization takes a smooth curve and turns it into a stepped curve so this is probably the best example so you take a floating point 32 value and turn it into an 8-bit value so it's actually simpler you represent the same you know waveform or curve or data just with lower resolution and with neural networks because you have so many parameters it kind of doesn't matter so there you have it you could you can represent it very simply so taking that one step further you can also do analog neural networks so i remember many years ago when the mem resistor was announced i'm trying to find it analog computing okay so this is the idea of a mem resistor or a tunable resistor so basically a parameter is just it affects the way information flows through a matrix so what you can do is you can do this at the hardware level where you've got a matrix and then you got a bunch of resistors and the resistors change the current flowing through the circuit but if you can modify those resistors which is a mem resistor or a tunable resistor you basically bring compute and memory together so this is the compute in memory processing element basically you embed a deep neural network or at lea maybe this is just one layer i'm not sure the full architecture of this yet but mythic ai is a company and so they're producing these chips that have a hardware level neural networks and they take a lot less power um and uh let's see i think this is the the layers um yeah here you go layers um so basically you plot out all the layers that you need on hardware and it feeds them one into the next um so there you there you go that is an example of a of a hardware level neural network i was trying to find an article that someone had posted i think on the open ai community it was months ago but where a scientist or a team of researchers actually created a neural network that used i think it was just ambient heat or vibrations of different materials um and so like you could have neural networks that did processing just with the ambient heat passing through like aluminum and glass i could be remembering that wrong but basically the idea is that all materials do some sort of information processing by virtue of the transmit heat and sound and some transmit light and so the idea is that because because the energy passing through those materials changes in a predictable way you can actually use those materials to to do com computation the examples that they had were just you know very basic lab levels um they were not particularly dense or sophisticated so it'll be a while before those are are commercially available because i first remember seeing mem resistors many years ago probably 10 years ago so the cycle to go from you know theoretical demonstrated in the lab to commercial application five to ten years sometimes longer oleds were in the lab for decades i remember a friend of mine his dad told me about oleds in the 90s so the and then you know we only just started having oled phones and stuff come out within the last few years so that's like a 20 plus year development cycle so sometimes you have an idea and you prove that it works but it takes a while to commercialize it so powerless like zero power or almost zero power neural networks are coming um but they will be one they probably won't be programmable because it'll be elements based on like the material unless they're unless they're tunable it could it could be uh it could be that you get um uh similar similar things to mem resistors who knows or the maybe maybe these other processes will make the mem resistors more efficient who knows but what this does is it allows you to embed a deep neural network on a chip and it uses very little power and theoretically it could use only ambient energy ambient heat energy to do the processing and when you when you get that level you could actually end up this might be the path towards having ai that is more efficient than human brains because right now the human brain operates at hypothetically um exascale right but the the first exascale computer that we have requires 21 megawatts of power and the human brain only requires 20 watts of power so the human brain is a million times more efficient than an equal equally powered supercomputer so how do we get a reduction of power of a factor of a million or more and it's probably going to be these hardware embedded um ai matrix mem resistors those kinds of things good grief sorry um so one of the things that this does is you've got compute in memory processing so you've got compute and memory paired very closely together that's what that's what neural networks do in the brain so in the brain you have memory is in the synaptic connections but also so is computing so for the brain memory and processing happen at the same time um which is kind of difficult to wrap your head around there's people that have articulated it better than me so then so this is this is on the low end right this is low power this is mobile these are the kinds of chips that are going to be embedded inside cameras inside cars drones that sort of thing that do things like video navigation that sort of stuff but then on the high end you've got sarah brus with their wafer scale engine so this is a giant um die that does uh let's see it has 850 000 cores um and 20 220 petabytes a second or petabits a second of memory um so this is how the largest models are being trained and run today so there was a rumor that open ai got their hands on one of these guys or maybe multiple ones of these guys because suddenly some of their models got way faster so it either they did something like quantization um where they they reduce the um the the from floating point 16 or 32 down to end eight and or they got better hardware so you get better hardware that has faster memory bandwidth um and and integrates everything closer together because the distance that it travels that information has to travel is a big thing that slows it down because then you get latency right so instead of having a cluster of gpus scattered across multiple servers you do everything in one chip and that's also kind of what they're doing here is rather than having a gpu and a cpu and storage and memory all separate it's all together um and so by by shortening the distance that things have to travel you get lots of efficiencies in terms of time because it's lower latency but also you get efficiencies in terms of power because you have less superfluous systems so we're getting more and more purpose built so this is on the low end the small portable end and then on the high end you're getting this stuff and then in the middle you get um you get you know conventional server general purpose server hardware that's going to integrate some of these abilities okay so i think that's it for the day i just wanted to bring you up to speed with kind of what's happening out in uh in ai land with at the hardware level but then also kind of how that hardware is meeting the software and how that software is meeting the hardware so anyways uh thanks for watching like and subscribe and consider supporting me on patreon have a good one