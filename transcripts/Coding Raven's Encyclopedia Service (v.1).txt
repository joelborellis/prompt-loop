good morning everybody well afternoon evening whatever time it happens to be for you when you watch this my name is david shapiro and i'm back with another episode about raven raven agi in this case i'm working on raven's encyclopedic service now what the encyclopedia service does is it provides raven with basically facts it's what raven knows about the world for certain the reason that raven needs this even with a tool as powerful as something like gpt3 is that gpt3 doesn't really know what's true and this is this is true of pretty much any generative model right a generative model takes input and kind of just runs with it that's all it does it's just an intuition machine so what you need then is you need something that you say yes this is a set of facts that i believe in strongly so that's what this is um is i've got it running in the background i'm running it against uh the simple the simple english wikipedia because it's much smaller let me show you about that real quick um so the full wikipedia as of april 1st compressed was about 18 19 gigabytes uncompressed it's about just shy of 81 gigabytes but the simple english wikipedia is much smaller just under one gigabyte and then you can see here i took a four 4.2 megabyte subset to test on um and so basically what that means is that the simple wikipedia is one it's it's in it's in simpler english which you know great but also it's just smaller so it's a complete subset of an english encyclopedia so without further ado let me introduce you to this project so what it's doing now is it's actually going through and parsing out all of the all of the the articles into plain english and the reason it has to do that is because here let me just show you by itself it is uh it's pretty messy so this is what wikipedia looks like behind the scenes um you see there's some there's some formatting that's familiar but a lot of it see like these double square brackets that's actually a link right and so well with an offline dictionary or sorry offline encyclopedia i don't want links this is just supposed to be a text document so it's chock full of all kinds of formatting that we don't want that i don't want to feed raven i don't want to give raven xml or html or wiki markup i want to give raven plain text so i had to write some scripts to just clean all this up and just extract the base you know the the actual stuff from it so then what the ultimate uh what i ultimately came up with was this so this is what it looks like in json format and you can see that uh that it's it's not perfect but it's much better and this might actually not even be the last one because it still has categories in it but either way i removed the vast majority of the links and cleaned it up so that you can just read it it's not meant to be rendered in a web page you just you just read it yeah this isn't the last one because it still has it also has links okay so anyways this was this was the work in progress now for the final script you can see it's still running um actually some some fun backstory when i ran this a previous version of this script against the full wikipedia it took about 18 hours to run so so yeah or maybe maybe it was eight anyways it was a long time i could make this faster by by making it multi-threaded because looking at my cpu resources this is not taking a whole lot of resources it's doing individual transactions with a sqlite database is not a heavy workload and it's also just some basic regex which you'll see in a second okay so sorry digression now without further ado here is the actual script so we've got here's the file name i showed you the xml file i think this xml file is about 17 million lines long and it so this is an export of of the simple english wikipedia in a single file it's crazy [Music] but there you have it i see i probably don't need that anymore so i start with article equals an empty string i could probably do a string literal whatever i create a database connection to sql lite3 i'm called simplewiki dot sqlite i get the cursor and then i run start db so this function it does create table if not exists and so basically if you have to run this multiple times you don't want to try and create it the same table over and over again so this just says if it doesn't exist create the table wiki um give it uh these three fields so title article and id and ensure that the id is unique so when you set a unique flag on a on a sql database it just says it guarantees that every value is going to be unique in that particular column and that allows you to ensure that you you don't get duplicates it's the simplest way to ensure you don't get duplicates there is one thing that you need to do to to prevent errors later on and i'll show you that and then what i've got is create index if not exists title underscore idx on wiki title so what that tells the database to do is to take the title column that field and create an index of it and what that does is uh it create i'm not sure i think what was it see i looked it up sqlite uses a b tree index um and i'm not exactly sure what that means but basically uh what an index does is uh it makes it so that it's easier to search that column and so when you do that it's it says uh let we're going to create another column that helps us find if you search for a title it'll help you find it faster and so that may that means that the database will actually you'll get way better search speeds if you do uh if you do a query such as where did it go i actually don't have a query in this um in this script um so i haven't tested the search speed yet um and you see here i've got commented out doing the same thing creating an index if not exists on okay and we're back sorry about that um had a little bit of a technical glitch so still running uh i left off where i was just telling you about the article index uh the thing about the article index is that it basically doubles the size of the database so we'll see how big it gets um and then i'll run this again just so that way i've got multiple options so we go to plain text wikipedia and sqlite simple so this is up to about half a gigabyte the maximum size for sql light is something like 218 terabytes or something like that so even if this is over a gigabyte that's still going to be well within reason so i'm not too worried about that even if we double the size by adding an index of the articles to two gigabytes or even if it goes more than that to three gigabytes i think it'll still be worth it because it'll it'll basically give raven a brain a knowledge about the world okay so the rest of the script um with open simple wiki underscore file name so it opens this file um with encoding utf-8 as in file we step through line by line and the reason that i found that i had to do this was because the text files were so big and um and it just it takes too much memory to try and do operations against the whole thing i in my initial experiments i would try and do like split on page and uh i just watch you know the the python memory just climb but we can ingest a file one line at a time and what you'll notice is that there are because this is exported from a database in the format of xml there are some nice consistent things so for instance the beginning of every page starts with this tag here page and then the title always is wrapped in these tags and then the id is wrapped in these tags or actually the first one wrapped in these tags and then the actual article body is wrapped in text so there's less than text and then it ends in less than slash text and then the the overall page itself ends in slash page so because of that nice consistent behavior it's easy to just read this one line at a time and then find what you need so that's what i did so if page is in the line then we know that we're at a new article so let's just start a new article else if the end if slash page is in the line we know that we're at the end of the article so let's go ahead and analyze this chunk and then if we get a good document back let's save it to the database is that simple otherwise if we if we're if we're just in the middle if we don't see this tag or this tag we'll just add the current line to the chunk that we're working on and so it builds up a subset of this so it'll go from this page tag to the next page tag um and and we'll we can parse it from there so the parsing it doc analyze chunk so first the first thing we do is we get a few we filter out a couple things so if an article is a redirect title i'm not really interested in that so we just return none and move on otherwise we split the title out of its tag simple enough um there are cases i think i got a version that doesn't have it where wikipedia is chalk full of talk category and other pages that are not actual encyclopedia entries and i found that universally they have a colon in them so if a title has a semicolon in it i don't want it either otherwise we run it through htt which is html to text so it just derenders the html into plain text and then we say okay so there is the uh there's the title uh we get the serial number which is out of the id so we split it from the first one and then we split it um and then we get we we split that uh split that out and then the content so the content is we we uh take it out of the text tags right so we get we we separate out the end text tag and the beginning text tag and then we run it through the d wiki function the d wiki function is where all the magic happens so here's what we do there first we remove all the new lines because this is just going to be a plain text article it's not going to be formatted for the web so we get a rig we get rid of all the new lines we get rid of excess white space so any tabs or other spaces become a single space we remove any audio files we remove the references at the end so if you see the dash or the equal equal references notes or related pages not really interested in that because those are mostly just links i've got this in a database a searchable database so i'm not too too worried about that we remove citations which are in the curly brackets again those citations are you know we're taking this is true and raven can't read html well i guess gpt3 can read html but we want raven to speak in plain english we remove the categories which tend to be at the front of the article or the end we remove links and this isn't actually removing a link we're just replacing the link so here's an example right you might see this where it says autumn and then and then vertical pipe autumn slash fall and so basically what i want to do is just take the right hand bit from all the links whether it's a simple link or a compound link or other um and then i just want this last bit of text so that will give me just plain and simple links and then i want to do the same with pictures so pictures have i think i've got an example right here yeah actually this was this was a copy that's audio this is not correct there we go remove pictures so pictures usually follow this format we got file colon earth flat blah blah blah blah and then another pipe and then the very last pipe or a very last section is actually a description a plain text description and so i want to actually keep that so i created all the patterns that i found for images this gets 99 of them it misses a few and then i just replace it out and then say here's an image and it puts it into the text so that so that the fact that there was a picture there is captured raven can't see yet raven doesn't have any ability to process images in the future we might have a full version of wikipedia so that raven can see pictures and compare what's in his encyclopedic knowledge of the world with what he's actually seeing in real time that's a that's possibly a long ways off and this is still running i expect it'll take a while we're probably um yeah we're at we're at half a gigabyte so we're about halfway done maybe a little bit more or a little bit less depending on the size of the index okay so we we've de-wicked it we've got all these functions that i've wrote that i've written they're they're mostly just simple regex because regex is very fast so we remove everything that we can links urls simple links citations there's one thing that i haven't figured out yet and that is the table information so a lot of wikipedia pages have table data in it and i'm not quite sure the best way to represent that in if it's just text so that's that's on my to-do list um then the last couple things we do are we use the uh the wiki to plain text wtp so or sorry wiki text parser wtp which is just like anything that i missed this will grab the last few things and give us a plain text version and then we run it through the html to plain text parser one last time for also anything that i missed and then there's a couple things at the end really if i did this well if i removed all the links and stuff i wouldn't need this so this is just me being lazy i need to come back and figure some of these some of these other edge cases out and then finally d wiki just returns the plain text version and once you're done removing everything you actually end up with about half of the amount of text most right about half of what's on a wikipedia page is markup so if you just want a plain text encyclopedia article it's about half so then assuming that all this works well it returns a dictionary with the field's title text and id okay great now because this is because this is going to be pretty consistent this is a great use case for a relational database and so then i've got a really simple function save to db it give i give it the docu the doc the connection and the cursor and so what it does is i set the little variable for the value i execute insert or ignore into wiki values so ignore gives it the the ability to bomb out if it violates this rule so i say id integer unique so let's say i needed to stop this and run it again i don't need to do anything else other than just restart it it will ignore it won't recreate the table because the table already exists it won't recreate the index because the index already exists and so then it will just start running again and if it if it's already process processed one article it will ignore it and move on to the next um and there you have it uh that's it once this is done uh let's see where did it go sorry can't see over my microphone once it's done this will have saved all of simple wikipedia or sorry the simple english wikipedia to a database file and this will be a single database file and that will serve as raven's brain so this is just the back end part um in the future there's probably going to be better tools i'm looking at this at apache solar solr which is an enterprise grade search engine um so that's one thing that i'm looking at in order to have raven's uh brain work so that he can recall facts about the world encyclopedic knowledge about the entire world um and then gpd3 is actually really great for parsing documents and summarizing documents so like let's say you want to ask raven about um you know historical conflicts in cambodia uh gpt3 combined with the encyclopedic service will be able to recall that information um easily right and then there's other things like you could say like tell me about some of the most famous people in russia right and and raven can search the encyclopedia uh his encyclopedic service for you know famous russians um and then summarize each of them for you that kind of thing but moreover this will help raven deal with other problems right all problems can be broken down into information problems and the more information that raven has asked access to the smarter raven is right it's not just going to rely on gpt 3 as the cognitive engine raven's also going to have a library effectively a whole library in its head allowing it to to have access to all of human knowledge anyways i'm gonna i'm i'm uh i'm gonna stop myself from rambling and um and call it a day uh check out the code for this it's gonna be up on github it's it's always up on github um and then as soon as i integrate this into raven i will post another update thanks for watching