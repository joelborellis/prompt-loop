good morning everybody david shapiro here with another video we are going to work on writing a novel i think this is it we're going to we're going to finish this one um you can see the series here fine-tune a novel to fine-tune gpd3 to write a novel part one part two and then part three is down here um this has been one of the most popular ones that i've done so far it has 11 stars and three forks so you guys clearly like this one we're closing in on being done a quick review for last time uh we got all the way up to actually writing creating the uh creating the uh the fine-tuned model so the model has been fine-tuned um and it was a forty dollar fine tune it was thirty eight dollars and some change um so it was an expensive job so it'll be a while before i fix fix the problems with it um it was 1.3 megabytes um but at the end of the last video i tested it and it seemed like it was working so let me um let me go here and get the model name because we will need that soon cd oh let me make this bigger so you can see what i'm doing font 36 that's a little well no you can see that just fine um cd auto muse and then um python fine tune fine tune there we go there we go okay so this last one should have been novel writer yes fine-tuned model da vinci ft there we go and i know from previous work that uh when you do this you need to um you need to change it from engine to model so like if you look right here um what you do is you change this from engine to model equals that um i'm not going to save this in this particular file but i'm just going to copy this function out because we will need it in a minute okay okay cool okay so where were we where did we leave off the problem so here was here's a sample of the fine-tuning data so alice in wonderland chunk 11 story so far alice falls down a rabbit hole and into the fantastical world etc etc alice decides to go into garden but she forgets the key and can't reach it alice tells herself to stop crying etc etc okay so this is this is not bad um some some of you folks have talked about like oh well you don't need to summarize the whole novel up to a certain point i disagree um because like yes when we're when we're reading we're focused on our current chapter but when you're writing it the reason that i that i want to have the summary of the entire thing is because with just the outline and the last chunk gpt3 will have no idea where in the story you are so it's not just about um information it's about knowing how far along you are in the story um so i hear you i still disagree now what i did was i had this prompt that says summarize the passage very concisely because what i wanted to do was every time every time we wrote one of these i'd wanted to just summarize it down to one or two sentences and it did a really good job of that and then for every chunk that it wrote we'd summarize it and put it into the story so far and then once this chunk got too long we'd summarize that chunk unfortunately so here's chunk 11 and here's chunk 12. so this is too short that's all there is to it is we're losing too much information um it still seems to work but i think for execution purposes um it'll it'll be helpful if um if we make this a little bit gentler so i wrote a new prompt that just says gently summarize this passage a gentle summary so let's take this and let me just show you what the difference is going to be so let's go back over to our playground and let's also grab our prompt so summarize this passage very concisely passage and then we go to the very end and we do concise summary okay so you see this that wasn't so bad um you know like this is actually better than than what it than what it resulted in in the training data because this training data is actually no it's about it's pretty similar um it's just the font looks like it's a little bit bigger yeah okay so it's it's it's pretty similar but we lose a lot of the detail and obviously we're going to lose some of the resolution that's the point of a summary um so we want to compact it but let's see what happens if we change it to this gentle summary so by adding the adjective gently or i guess l y that's an adverb gently summer summarize and then gentle summary we're basically saying like ease off ease off the gas tone it down just a little bit okay so then we say gentle summary so that means be a little bit less aggressive when you're summarizing it oh look at this that's so good um okay that's getting pretty long okay it was that was right at the end okay so um it was just over 256 tokens so yeah that's not bad let's compact this yeah so you can see well here let's copy this out into notepad plus plus so we can see that the gentle summary is 1200 characters so that's below the 1500 character mark that we wanted to keep everything at and then the um what it was originally is 1300 okay so maybe we need to do a little bit more aggressive um yeah because we we would ideally want this like and if because if you remember oh whoops the aggressive summary was much much shorter okay so let's do a little bit of prompt engineering um let's see summarize this passage uh with moderate uh aggressiveness um uh with my uh condensation um and so then we'll say moderate summary because obviously like gentle summary was a little too little too soft on the on the corners okay so this was 1300 characters um let's see if we can get below because this was like 1311 characters and the goal would be to get down to like maybe half that okay so the moderate summary and then so let's go to here length 976 okay so that that took it down by about um just over 300 tokens so that's not too bad um and then i guess what we can do is we can we can do this recursively but yeah so reducing it by because 1300 minus 300 um do some math 300 well let's see what was it let's let's be let's be specific it was thirteen eleven uh nine seven six okay thirteen eleven minus minus nine seven six so three hundred and thirty-five divided by thirteen hundred and eleven um okay so that's twenty-five percent shorter not bad um but that's that's better than the 10 times shorter um that uh that the original prompt was doing okay so i think we found our prompt our gentle summary um so let's oops summarize this passage with moderate yep and then we go moderate summary okay so basically um if i were to do this over what i would do is um for for summarizing the big chunks i'd use this one and then for making sure that the the story so far wasn't too concise i would use this one um and so what we're going to do is i'm going to incorporate this when we're writing the novel and i'll use the same logic okay so with all that said i think we're ready to go so let me open so writing the novel is going to be really similar to preparing the training data so basically what i'll do is i'll just copy this and then i'll modify it um so we'll do this let's go ahead and save it so we get the um so we get the right formatting all types and we'll do write novel dot pi here we go it's getting real folks okay now we're not gonna we're not gonna do this summaries that's fine uh we need to yep we'll need an outline and we're going to be generating the summaries as we go okay oh did i accidentally overwrite the thing no here it is okay so we will all right so we'll need two we'll need two complete or two um gpt3 engines so this will be we'll call this the fine tune completion we'll get rid of the engine and the tokens will set that to 500 because we don't want to go any any higher than that um because that'll be 500 tokens is about 1500 characters they're about um might be a little bit more let's cut that down to 400 because i don't want it to go overboard okay so let's move this up a little bit because the gpt3 completion this is just your standard text davinci o2 we're gonna use this with our with our summary prompt okay so um the outline will be we'll just call this premise.tech and so what we'll do is i've already got a folder full of premises and so what we can do if we want one um let's see mystery tensions are high and the city is on edge okay cool so we'll just copy one of these out to the main directory and we'll just call it premise so that's the file that we'll use to seed the premise of our story okay so we got the premise we don't need recursive summaries we don't need the fine tune we've already done that um let's leave one of these open just so that way we can see um kind of how it's going to go so the premise will go here the story so far will go here and then we'll write the chunks now if you watched the last video this had a really hard time with a cold start because if the story so far was empty and the last chunk was empty as well it didn't know what to do so it became very clear that what we're going to need to prime the pump by by starting this with a different process so what we're going to do is we're going to say um first chunk um well actually i guess first let's let's do some prompt engineering um so let's go open our premise again um and then go back over to gpt3 so let's say maximum length 256 that's fine write the opening chapter for the following story premise all right chapter one let's see it looks like it's just repeating itself okay um uh okay so this is this is it's so fascinating because like if i fed it pros it was it was really good but if i ask it to write pros it's really awful let's see so let's try something new and i remember i ran into a similar problem at the very beginning of this project um okay so premise um yeah and you can see it made a lot of mistakes i wonder what happens let's do um let's do an older curie or sorry older um davinci instruct beta right uh write a novel for the above premise oh much better okay um let's try well here let me copy this down just in case um and also let's see if using instruct two works so instruct beta seems to be more creative yeah see it's just it's just copy pasting itself um okay so instruct beta is definitely where we're gonna start um let's see chapter one oh this is great okay cool i think we have we have our opening prompt um so let's do premise and we will do premise chapter one there we go so we're going to say prompt um first okay so basically what we're going to do is we're going to use this prompt and the premise to generate the first the first chunk to get to get things going um okay so we've got our outline and then our prompt will be open file um prompt underscore first dot text and then we're going to do dot replace premise with our outline okay so that is going to be our first chunk um and then let's go ahead and do a um actually i guess it'll technically be our last chunk so we'll go ahead and use the same thing equals gpt3 completion prompt so basically what that'll do is what this these two lines of code do right here will do exactly what we saw right here um and it stopped at 256 so we'll do 500 tokens because that will be i think that'll be about right let's see how much 256 was in terms of characters 644 yeah so if we double that that'll be 1300 tokens or characters or so because we don't want it to go too far in one go because um one thing that gpt3 does is it kind of tends to start hallucinating or confabulating if it runs too far okay so prompt first good and then uh let's see okay so we've got the first chunk and then the summary chunk um so the summary chunk we're gonna want to um yeah okay i think we're probably good now we're going to need to this actually this loop is going to need to be different okay so once we get here we're going to do we're just going to do like um let's say let's say 20 for i in range 0 to 20. so we're just going to do 20 iterations and then we will and then we'll stop because i don't want to burn all my tokens so the reason i'm doing this and not another fine tuning video is because i'm almost out of tokens for this month okay um so last chunk and we should probably save this too yeah okay do i have a save function i do have a save file okay so let's do output actually no we'll call this the novel um okay so novel and then each time we run each time we run we get a we get a chunk we'll save it so we'll do save file and then how do i have this formatted it is content and then file path okay so last chunk and then file path will be novel slash chunk zero zero one dot text okay and then the summary chunk so we're going to go ahead and summarize this but i think because um all we need is the last chunk we'll generate the next chunk once it's populated uh and that'll all happen here yeah because all we need all we need to add to the summary chunk is the last chunk right um okay so i think that's probably fine because we've got last chunk we'll summarize um actually no i guess we do need a summary chunk going in okay so now prompt equals open file um we'll say prompt summary text dot replace and then the summary is chunk okay chunk and we'll say last chunk and we'll say actually we'll just say the summary so far summary so far because that's that's that's the variable name that'll be here equals gpthree completion [Music] um prompt okay let's add a little bit so we'll do um prime the summary variable and then we'll do prime the story variable okay so um last chunk and summary so far that is going to be so we've got the outline we loaded that um right so we load the outline here so basically all we're doing is we're priming this input so i probably should have explained that because if you're watching you're probably like what are you doing um story so far and last chunk so we've got those three variables that's what we need to put in and then the fine tune now we're ready for the fine tune model so we're gonna ask it to generate next chunk okay so now that's what we do um okay and we saved it as last chunk um i in range 20. so actually we're going to start at 2 because we're going to be working on chunk 02 and i want to use that variable i um let me just make sure that my python logic works how i think it does range 2 to 20. print um print view as a list damn it there we go okay so if i do print list range 2 to 20 it'll go from it'll go 2 to 19 and i want 20 full chunks so i need to go to 21. okay and that'll give me chunk two three four all the way up through 20. okay okay so we're going to start with this save it out summary so far um that's good now let's look at some of this logic first i guess we're going to need to load the the full prompt so prompt equals open file prompt underscore full dot text so that's similar to right here outline summary and last chunk so let's just copy this so outline is there summary chunk so let's actually replace that with the correct name summary so far and last chunk okay so on the first instance the first time this runs all these variables are populated and then we're going to ask it to do this the fine tune completion which will use my model here now one problem that i'm concerned about is if if the model isn't loaded so we're going to do max retry we're going to do 15 and we're going to wait five seconds between each one so because 5 times 15 5 times 15 that's 75 seconds usually it doesn't take that long for a model to load actually let's go ahead and do 20 retries so basically what happens with fine-tuned models is that sometimes the api will unload the model and you have to wait for it to warm back up and so basically what it's going to do is if it errors out then then i want it to wait and try again so i'll have it sleep five seconds um because if you just try every second you're gonna the api is just gonna get mad at you it'll say too many retries um so i'll wait five seconds and then i'll retry up to 20 times for the fine tune completion okay so let's go ahead and grab fine tune completion and we're going to call this next chunk equals fine tune completion prompt and so that is going to be that next chunk is actually what's going to be saved as o2 and then once it's done next chunk will become last chunk but we also need to summarize it and add it to the summary okay so let's go ahead and save file and the chunk number will be here um oh i guess i need to go look at what i did for the um for the file names yeah get next chunk so i'll need to i'll just write a quick function to generate the file name oops def make file name and so we're gonna have uh n so then we'll just copy this and do number i'll be chunk that um we can get rid of that so it'll just be number and i think we can just return yep return chunk 0 0 s um yeah so let's copy that and then if it's in the hundreds we only need two zeros and if it's less than a thousand um because we're not going to do we're not going to do a thousand um okay but just for completion because you should always so here's here's a rule of thumb if you have a function like this um it's best practice that it always returns something um so if you if it's since it's if else if else if and then else so finally you just return um it'll be uh whoops here we'll just remove all the zeros because that means that it is a four digit number um okay that's fine so make file name um save file uh novel plus make file name i okay so we will this is we're going to want to save the next chunk actually um and you also want to save the next chunk because at the end like we're not going to have another iteration to review the last chunk because the next chunk will ultimately be the last chunk one time okay so now we've got um next chunk is saved out to file so now we need to summarize um so now let's do uh update the summary and so we'll replace that with next chunk and we'll do summary so far equals oops summary so far plus um gpt3 completion prompt so now this is where if the summary is too long we will um we'll use this logic so if summary shorten summary so far if it's too long okay um [Music] actually we'll leave some output shortening the summary and again remember the reason that we want to keep track of the entire summary so far is just so that gpt 3 remembers where in the story we are um okay so this is going to be the gentle summary because remember the gentle summary just reduces the length by about 25 and so then we say um prompt encoding i don't think we're gonna need that because we're not using anything from um so this line of code um i had it here because there's something funky in the files from gutenberg where i needed to encode it to ascii ignore errors and then decode it back to a string because um it used some some form of utf-8 that gpt3 was not happy with it kept erroring out but i don't think we need that okay so the summary so far has been updated in the loop um if it's too long so if it's 1500 characters and we reduce it by 25 so that's times 0.75 that'll bring it down to 1125 characters um if this if this recursive thing works um and then i think we're just about done because um after after that last chunk um equals next chunk so basically this says okay we're going to inch along and so then the cycle repeats and it'll be here so the summary will be captured here the outline isn't changing and the last chunk so the what was the next chunk moves back and now it becomes the last chunk and now we're asking it to generate a new last chunk and it will all stay there um and then let's get some output as we're going so we'll say i feel like well okay so this is all going to get saved all the logs are going to get saved so i don't need to we can we can go and review those in the gpt3 logs folder so this is i recommend everyone do this just keep a gpt3 logs folder um because then you can go back if something is happening that you don't expect you can just be like okay what happened here and i record both the prompt and the response um i used to try and record the settings as well but i found that that was like once you find the right settings i don't really change them um okay we can close recursive summaries we're done with that we can get done with that prompt first okay we've got that we've got the premise we don't need alice in wonderland we don't need the moderate summary and the concise summary okay let's have a little bit of output as we go print last chunk and then as soon as the next chunk is done we'll do print next chunk um wow i'm nervous let's see if this works oh man okay ctrl z to exit out of that python make sure that the uh my save file yeah novel plus okay so it'll save it out as chunks we'll go up to chunk number 20 and then we'll read them man i'm like actually like my heart's going like this is this is exciting this is one of the more complex projects i've done other than invent a cognitive architecture but i'm doing it live like you guys have seen the whole process um write novel watch it blow up i probably fat fingered something and then okay oh crap i know what i did wrong i used the wrong engine stop stop stop stop stop stop we needed to change it to davinci instruct beta halt the presses okay um all right so what i need to do is when we're asking it to do the first one um gpt3 completion so uh prime the story variable the prompt that needs to i need to update the engine engine equals davinci instruction beta this is also why you do output console output as you're going um so that you can see what the heck is going on but this should have been saved here so you can just see chunk001 there it is but you can see that this is ludicrously similar to the premise that we um that we wrote tensions are high in the city as a string of murders have the populace on edge the city was on edge a string of murders had the populace on edge yeah that's dumb um so uh yeah open ai you need to work on the creative output of um of the of your your beta or sorry of your instruct series um let's see okay there we go now all right so let's clear that out let's try this again oops cls python right novel and it'll take a minute come on you can do it wake up and pour coffee on the computer i wonder if that would help oh here we go okay um it tried to write quite a bit chunk one it wrote a lot of very short chapters that was dumb um length 2800 that's a lot why did it write so much whatever let's just let it go the end story link i don't think this is working error community it's still being loaded all right so let's look at the most recent gpt3 logs to see what the heck is going on prompt outline tensions are high story so far last chunk the police did this next chunk the police ran a panic okay the city was finally safe again yeah so i don't think this works fine we can always try again um actually i think all i need to do is just add chap add a chapter as a stop so we'll do that too okay let's see how it's going yeah now it's just it's just doing its own thing the case of the stolen scores all right let's stop this you don't know what you're doing you don't know what you're doing okay so since since we did this let's do let's add chapter two as a stop under write novel so we need to make sure that it doesn't just like zoom ahead fine tune completion chapter two actually we'll just do chapter i know chapter two just in case that shows up and then we'll also do this here under gpt3 completion okay so it does switch back to that all right so basically what we have to do here is kind of do a little bit of detective work of our own and go back through and say okay so we've got the last five gpt3 completions or so oh i guess there's going to be some hidden in the background because we did summaries so starting at 7 30. so let's open these prompt premise tensions or higher rated novel for the above the police were unsettled in the end the detective is able to solve the case and apprehend the killer um okay so it tried to write too much how much was this um the selection 1100 characters that's fine but it went way way longer than i wanted it to um what how long was the um oh token's a thousand there is part of our problem let's shorten that see like i said one of the problems is you don't want gpt3 to just take the reins because it'll just keep going so this is not what i wanted it to do okay so let's go back to our prompt first write write the first chapter for the above novel chapter one okay so if we change that it's just just write the first thing and then stop um that should it should be able to handle that a little bit better i i forgot so here's one thing is when you um let's plug this in so um instruct beta is not as good at stopping as um the later models are so we've got to be a little bit more careful with the stops um and so you saw how like this one this was good maximum length i think this original maximum length was was 256 so let's just say 300 because we want more like this where it gives us good rich pros okay temperature seven maximum length three hundred okay that seems okay temperature 07 maximum length 300 let's see if this works and then um let we'll just switch to chapter as the stop for both of them just in case it tries to inject more chapters um okay so chapter and then chapter okay so basically what we've done is it was completely botched from the beginning from the outset because if you look here the very first prompt which was the opening thing um it just tried to write a lot of chapters in the end the detective is able to solve the case and apprehend the killer it's not actually telling us a story um it started as a story right it started as like this could be some decent opening prose um but because this this is not fine tuned to generate you know the opening of a story and honestly like you could have an entire model fine-tuned on story openings cold openings that's actually not a bad idea um let me do a time check um we're at 44 minutes okay well we're almost done almost done i promise we'll see how far we get and maybe there needs to be a fifth video okay so let's go here um let's delete the chunks and let's do python or here let's do cls python write novel and lights camera action let's see what happens all right we get some dialogue so that's good the city was tense it had been for weeks oh perfect perfect yeah that's more like it that's more like it chunk two he heard the sound of a woman's sobbing oh yes it's working it's working it's working deep reference also that ages me um excuse me miss i'm his wife this is my husband it was a 38 revolver said the wife it was black with a long barrel and a dull finish it was the ugliest gun i've ever seen this is good this is working it's actually doing a thing okay okay okay yes oh man you think somebody else did it i think so said the detective with large bay window and hardwood floors you guys i did it it works it works oh i'm so excited all right i'm gonna let this finish and then i'm gonna upload this and the video is done and y'all can read this story i'm just gonna savor this victory because like yeah and well okay so while it's running i will say that um i cannot take full credit for this because first i posted about this idea on openai on the community and a few folks they use my original code automuse1 and they started talking about it and someone came back and asked some questions and i just they're like what if you did this and that really kind of got the gears turning and so i was like ooh brilliant i'm going to try that and then also a bunch of you have been adding comments as we go okay this is going to take a minute to run it's working you can see that it works i'm going to go ahead and stop the video you can go check out the output yourself but as far as i'm concerned i think this is done i think the fine-tuned model works and we can generate novels on the fly hell yeah