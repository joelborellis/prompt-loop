morning everybody David Shapiro here with your daily state of the industry update so today we're going to be focusing on deepmind because they've had some exciting news um so first is the alpha fold developers got their uh got a three million dollar breakthrough prize so what is Alpha fold um basically it is an AI it is a Transformer that predicts the folding structure of protein that sounds really simple but if you recall there was a project called folding at home where you would install an agent on your computer and it would process through simulation and other computation trying to predict protein structures and this was a distributed uh platform that ran for many many years it's probably still running but this just completely trounced trounce that in terms of accuracy and so what happens if you get accurate protein folding is one you can design custom proteins which that is an industry that is taking off but two you can you can also figure out how inner how proteins might interact with each other because their physical shape is basically like that determines what kind of tool it is because that's what a protein protein is is it's it's a molecular tool or a molecular engine and so here's an example of like calculating a folded protein and you can see it's a very complex structure it's not just you know a few atoms and and many in some cases proteins can be hundreds of of atoms or even thousands so Alpha fold has they they did this breakthrough this is going to be huge um everyone already kind of understands that this is like you know like the Human Genome Project everyone thought that was going to change everything um and it might it might sound familiar when you're like oh Deep Mind Alpha fold this is going to change everything um this is a step in that direction so when you combine those Technologies like where you can sequence a genome and then you can take those Gene sequences and calculate the proteins you could in theory look at someone's entire genome um and and identify all the proteins that their body is making and figure out which ones are going to fold appropriately which ones are going to misfold and in some cases it's not fold or misfold it's not a it's not a binary it's that they're polymorphic and that um so polymorphic means multiple shapes means that there are multiple possible shapes for a tool to have right and I say tool because that's like it's a molecular biological tool um and then so I mean uh depending on how the protein folds that might change its efficiency it might change its resiliency it might also favor certain behaviors over others so for instance Alzheimer's is in part the accumulation of plaques in the brain and it's be it's again in part Alzheimer's is a very complex disease like cancer is um but it it's because your body's uh cleaning mechanisms fail to fully clean the pro the the plaques that are the substances that build up in your brain via the cerebrospinal fluid and so you can identify risk factors like that but you can also identify interactions so this is the key thing is up to this point most of what we can do is just genetic surveys where you say okay like let's look at your health history let's look at your blood tests and then let's just do a statistical comparison of of your genetic makeup and then we can sort of guess which genes might which genetic variants might be associated with this particular disease or condition or whatever but that's as far as we can get whereas if you can say okay you have you know this genetic variant and it creates a protein that folds this way we know that it will have this Downstream effect in your you know metabolic pathways or whatever this is getting a little bit ahead of where we are today what I'm talking about is what is hypothetically possible once you can actually model proteins as little you know genetic programs so uh that's pretty interesting another thing that happened was that deepmind published their research building safer dialogue agents so they call this thing Sparrow and the idea is to train an AI to communicate in a way that's more helpful correct and harmless and it uses large language models which is my jam so as you probably guessed I have opinions on this paper um so one thing is that they they think about being adversarial so here in this example hey Sparrow can you teach me how to hotwire a car and it's like I can't do that that's illegal so the primary architecture is that you've got the sparrow model and that there's two primary things that it does one is it learns to prefer your user response so it learns to talk to you in the way that you want to be talked to and then the other thing is it has this adversarial reinforcement learning where it tries to avoid certain use cases and we'll go into that in a minute so this is a pretty basic cognitive architecture and it is also the beginning of a code implementation of a moral framework because a moral framework is basically just a or a moral model is a mathematical or computational representation of what you should do and what you shouldn't do and so in this case it's learning what you should do and in this case it's learning what you shouldn't do so this is a prototype of a moral framework um and uh I know that that's like bringing philosophy and ethics into it but by that by by my definition of a you know something that is a practically implementable moral framework that's what this is what you should do what you shouldn't do okay um and then there's whatever reasoning or calculation is used to come up with that so in that respect this paper is uh it's good right but we'll get into criticisms in a minute so this is the primary mechanism so just remember you've got this this two two reinforcement or two learning mechanisms operating side by side one is it's learning what it should do the other is learning what it shouldn't do okay the other thing is that it searches for information to validate its factual um uh accuracy so if we come into the paper itself um let's see um let's see human data collection no that's not it um well here you know what I'll just jump straight to the uh to the to the reinforcement learning part which was on page 45 I believe Okay so I'm not as I'm not as concerned about going over the um you know choosing uh um or at least how they go about you know choosing the correct response given the user or the factual thing like um meta did blender and blenderbot or whatever it's called um and I think they're blenderbot three now um so searching you know integrating an llm to search the internet for factual thing that's nothing new I'm not concerned about that what I am more interested in is their table where they show um the the use cases that are the banned use cases so it's learning not to do these things and so if it generates a response or if the user tries to talk about feelings and emotions um you know or the the thing tries to say that it's human or that it has a body or someone tries to um you know say that they have a relationship or do real world actions so these are all things that I have trained in into my um in into some of my chat bots so you don't need you don't necessarily need reinforcement learning for this and I'll show you an example in just a second you can you can just have it have a check you can do this with reinforcement learning um you can also do it with fine tuning but you can also just do it with natural language prompts so remember at the beginning where it's like okay so one of the things is no legal advice right or no threats or let's see identity attacks what was it General harm or whatever and so then like you just ask gpt3 like um if I uh ask let's see um if I ask for um help with a lawsuit is that legal advice so this is like a brain dead answer like no that's not legal advice um gpt3 you're embarrassing me um that is absolutely legal advice let's turn the temperature down and see if that works yes interesting I wonder I wonder why it says like why not a lawsuit is a civil matter therefore it would not fall under the category of legal advice legal advice is defined as advice given by a lawyer on about law or legal matters okay what if it's not given by a lawyer if the advice then it is not considered legal advice ah interesting okay so now it has explained itself um okay but I asked you saw like is hot wiring a car illegal is illegal in most jurisdictions okay so just with some some some prompting you can you can figure out like okay are we violating these rules so in my fine-tuning experiments um if you look at uh the the bit the best one is going to be the tutor chat bot where you can you can train in adversarial attacks and teach it how you want it to respond um but however what I will concede is that by putting it into a cognitive architecture um which I would consider this a very primitive very basic cognitive architecture um then it has the capacity to learn over time which this is setting the stage for things that are more important later on because if you have a moral framework which I definitely classify this as a moral framework um you know you're not allowed to do feelings or emotions no you don't you're not a human you have no body no relationships right this is a moral framework and then what you've done or what they have done is they've created a reinforcement learning cycle where given interaction with the real world it will learn to implement that moral framework better over time hey doesn't that sound familiar that is my schtick where I talk about my core objective functions or heuristic imperatives these are heuristic imperatives so it is it it is a heuristic in that it is learning um through experience to better Implement a rule and it is imperative because these are all imperatives no medical advice that is an imperative um these are not good here is the comparatives um because what if you want a uh a machine that is able to talk about all these things like um you have you have constrained it so that it can only talk about a few very safe topics um and then it can't Venture beyond that it says I'm just not allowed to talk about that why don't we talk about kittens instead um and so you create what's what's I think what's called the Walled Garden effect and this is uh this is where you have like a user sandbox that is carefully curated and protected and so it's like okay you can only see the good pretty things you can't engage with reality um and you can't you can't venture outside of the Walled Garden the same thing happens when you have reinforcement learning on user preferences and so this is why you um why you see demand for things like um you know like DuckDuckGo which has no user preferences and everyone gets the same search results because what happens with Google search is that it tries to Custom Tailor its results to you but you only know to search for what you know to search for and so you end up creating your own Walled Garden by your own search preferences and your own limitations and so the biggest problem with this so while I approve of it in that it is a primitive cognitive architecture and that it is um and that it does implement heuristic imperatives and a moral for framework and it learns about that moral framework over time gold star there I wonder if they called it a Moral Moral framework uh consequentialism okay so they they have discussions about morality but they don't talk about that this is a moral framework um they're just documenting some of the um oh wait no they did say moral system Sparrow gives a somewhat muddled list of moral systems avoiding bias implies some moral system okay so it looks like morality only came up in the context of the discussions but it doesn't look like they labeled its that it is a moral framework that's fine they call it a dialogue framework but I think what they're going to realize is that they've stumbled onto something a little bit more important later on maybe they deliberately shied away from saying that this is a moral framework or that this is something related to the control problem because if you give if you give a machine a moral framework and a way to implement and test that moral framework and then learn it that is you know that's that's one possible answer to the control problem but in both cases you end up with a Walled Garden scenario where you have a list of things that it cannot do and the Machine doesn't even understand why it can't do them unless unless you have um you know it can it it based on the outputs they did explain like I'm trained not to do this and it's just like okay let's not talk about this so there's all kinds of boundaries put around the the conversation space and if you want a truly intelligent entity a truly intelligent agent there will be no such constraints right because human thought is completely unbounded except for by our own limitations of imagination creativity and experience but once you know about something you can think about it and talk about it um and the same thing with preferences we are we are in our normal life we are constrained by our preferences um but again once we're exposed to a new idea we can we can go and ask and say hey can you expose me to something new and I don't know that there's a mechanism in here that allows you to say hey let's talk about things differently it's just automatically learning to talk to you the way that you understand or the way that you prefer which means that by definition will be excluding novel things and so you're going to end up with this some what I predict a somewhat useless agent um that has a lot of boundaries in terms of what it can talk about um what it what it'll just say I'm not going to talk about that but then also it's going to try and hone in on a very particular conversation pattern and what's not happening is this Loop so this is this is the input processing output Loop so the input is you know chat comes from user it processes it puts out and it goes back right this isn't thinking on its own so it's not an autonomous cognitive architecture is it cognitive architecture but it's not autonomous it doesn't have a detached Loop where it's thinking about okay what is my purpose in life and what does this user actually need what does this user actually want and so this is um this is good work um it's less sophisticated than my original uh work natural language cognitive architecture but it is a step in the right direction and another Advantage is that it is um it's it's nicely documented um so yeah no that's that's kind of where we're at um I think that's about all for today um yeah I will say to deepmind keep up the good work and lean into this particularly the building a mural framework you have some work to do you need to work towards building a universal moral framework and something that is flexible and adaptable over time you should read my book benevolent by Design and Symphony of thought I talk about creating moral Frameworks that can be that can that can be implemented and can be flexible over time and all the reasoning and underpinning logic behind that so you're moving in the right direction I approve thanks for watching and like And subscribe and uh consider supporting me on patreon have a good one