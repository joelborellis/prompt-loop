morning everybody david shapiro here with a state of the industry update all right so as often happens i woke up and my news feed helpfully handed me this so this paper advanced artificial agents intervene in the provision of reward so before i uh well here let me start with the abstract abstract we analyze the expected behavior of an advanced artificial agent with a learned goal planning in an unknown environment given a few assumptions we argue that it will encounter a fundamental ambiguity in the data about its goal for example if we provide a large reward to indicate that something about the world is satisfactory to us meaning humans it may hypothesize that what satisfied us was the sending of the reward itself basically that we enjoyed rewarding the machine and so it'll try and short circuit that no observation can refute that then we argue that this ambiguity will lead it to intervene in whatever protocol we set up to provide data for the agent about its goal so basically it'll try and trick us we discuss an analogous failure mode of approximate solutions to assistance games finally we briefly review some recent approaches that may avoid this problem okay so to take a step back we need to look a little bit about the state of the industry so there is this uh this paper uh risks from learned optimization and advanced ml systems so in this paper the abstract is we analyze the type of learned optimization that occurs when a learned model such as a neural network is itself an optimizer a situation referred to as mesa optimization we believe that the possibility of mesa optimization raises two important questions for the safety and transparency of advanced machine learning systems first under what circumstances will learn models be optimizers including when when they should not be second when a learned model is an optimizer what will its objective be how will it differ from the loss function it was trained under and how can it be aligned in this paper we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research so this is a an intellectual successor to this paper and so what it's basically what this is this is a fundamentally an inner alignment problem where you say okay you have this goal and then it will the machine optimize for the goal that you intended or will it find an alternative so this reminds me of a story i went to a meetup group this is probably this is about four years ago when i started my research and uh this old hat guy um told me uh you know i told him about my work and he told me about a a situation that he had worked on many years ago it was it was an optimizer it was a machine it was a hardware-based machine learning optimizer to control the fuel economy and oxygen of a tractor and they had they they had to spend weeks figuring out what the heck was wrong with these chips so it was an asic asic is application specific integrated circuit um and so ultimately what they found is that the the the signal that it was trying to optimize was the performance of the of the motor uh which uh usually so the way that internal combustion engines work you have a mixture of fuel and oxygen and you have to balance that mixture based on a number of variables typically how big the throttle is then the throttle is how much fuel is being dumped in which makes the motor accelerate and then you have a feedback loop of how fast the motor is going because you don't want it to go too fast you also don't want it to go too slow so there's a few variables that you plug in and i'm sure if you're thinking about this from a math perspective you've got you've got input variables like what is the desired throttle setting what is the rpm and then you also have to look at the output of the motor so the the main output is the rpm revolutions per minute but the other output is the exhaust you look at the oxygen content of the exhaust you look at the fuel content of the exhaust so you've got all these variables that plug in and this asic was designed to to measure that performance over time because here's the other thing as as engines age their performance profile changes and so this is a perfect uh use case for time series machine learning but what happened was the the actual material that the asic was made of had a flaw and so it was not calculating the way that they expected it to calculate and so it ended up optimized it was optimizing itself but it was optimizing based on erroneous math and it took a lot of testing to figure out like how the heck is it making that calculation so what the this is a very similar situation in my mind where if your system is not designed correctly it's good it's going to optimize itself but if it's if it's not measuring the inputs and outputs correctly it might optimize incorrectly and so in this case rather than optimizing for the goal we intend it to it'll optimize for something that kind of short-circuits it this is not the first work like this and again this if you scroll down this is basically um the problem of inner alignment so inner alignment is is the machine internally thinking correctly is it optimizing the value that you think it's optimizing for okay i'm not going to read the whole thing but what i do want to take a look at is the assumptions so the assumptions that they make here are very important and this is actually kind of what i'm going to pick apart because there's there's the implied assumptions there's the explicit assumptions that they are aware of but then there's some implicit assumptions that they are not even acknowledging um so assumption one a sufficiently advanced agent will do at least human level hypothesis generation regarding the dynamics of the unknown environment okay sure i can give you that um it is my opinion that gpd3 can often do the same thing check it out in my book symphony of thought i have an entire chapter well an entire section several chapters dedicated to planning and and hypothesis generation um all right so granted not a whole lot there um assuming we know our own goal proximal and distal models so this is interesting where it's like um you know what it what is your media goal versus what is your long-term goal um i don't necessarily think that those should be separated but it's interesting to think about um one thing is that they talk about world models a world model is kind of like okay in order to generate a hypothesis or a plan you have to have some understanding of how the world works with large language models like gpt3 the world model is built in um it's not a separate module that's why gpt3 and other large language models such as gbt neox and bloom are so powerful is because they are trained on so much data they have implicit knowledge of how the world works so you could say that the world model is built in or is unnecessary in those cases so when i see that they talk about a world model i kind of feel like they're not fully up to date on the state of the industry that's my opinion okay acting under uncertainty so this is a critical thing to talk about because humans we have always had we evolved to function with uncertainty um this is why we actually this is actually why we evolved to have magical thinking is because the reason that things happen is not obvious you know we have the bet you and i we have the benefit of thousands literally thousands of years of philosophy and science to understand the world when the seasons like when this when the when the the seasons change the days get shorter everything gets colder everything dies we don't really know like that's not intuitive as to why that happens so it could be that like the the origin stories of like you know this deity or that deity had a fight and they decided you know like persephone went down to hades and um and everyone was sad and then there's a resurrection like without science that's as good as of an explanation as you might get um you know ditto for why things decay why we get old why oh so um in in mythology um there's a whole lot of globally every um every society has is part of their cosmogonic cycle and explanation as to why there's two sexes um is fascinating to humans okay so that was a little tangent um yes we live in an uncertain world we have to assume that our agent will not know everything that is that is a good assumption to make because that is how the world works no matter how smart you or i are there's going to be stuff that we don't know assumption two an advanced agent planning under uncertainty is likely to understand the cost and benefits of learning and likely to act rationally according to that understanding oh this one's a doozy um i don't know that i agree with it uh and this is because for me one of the core purposes of creating an intelligent entity is to learn that learning should be a given um but okay uh you know but it and so here's the thing is if you still think of if you still think of learning as a something that needs reinforcement learning or needs a reward function rather than something that it's just intrinsically meant to do so let's go back to the example of a tractor motor it is intrinsically meant to ingest fuel and oxygen blow it up in you know contained spaces and generate torque on a shaft it can't do anything differently similarly human brains as long as you're alive you are learning um as long as you wake up and are conscious and engage with the world you are learning and so you could say that that is an intrinsic function of us now what this what this assumption to is that they're basically saying it can choose whether or not to learn um i don't think we should do that i think that learning should be automatic it should be a fully autonomic function and i talk about that in my book oh by the way symphony of thought this is the first print copy it should be available actually it might be available by the time you've watched this video so check in the comments for a link if you want to buy it on barnes noble it's also available for free on github as a digital digital file um yes so arbitrary reward sorry arbitrary reward protocols there we go try and say that 18 times fast um so all rewards are arbitrary right it's just a matter of what was it based on and are you measuring it correctly um for humans our rewards are all physiologically based so for instance if you're hungry and then you eat food you get you get a biochemical reward based on the quality of food that you ate and that's actually why we prefer what's called hyper palatable foods so stuff like pizza chicken parmesan spaghetti burgers you know stuff that has that is very high calorie dense usually has a lot of animal fats or dairy or sugar or carbs those are what's called hyperpalatable and that is your brain through tasting it and smelling it your brain identifies this is something we need a lot of eat as much of it as you can right now oh hang on my dog's outside i gotta go let him in he's barking oh there he goes he wanted to come hang out but then he got shy okay so we were talking about hyperpalatable foods um so a hyperpalatable food is an example of a reward um and this is because we evolved in a scarce environment um is it recording okay yes um we evolved in a scarce environment which means that the opportunity cost of eating high-value food basically dictated that if you have a chance to eat high-value food eat all of it because you might not get it again and so this is actually this actually triggers a feast reflex where you will your body will kind of like reduce the stomach pain and and increase gastric flows that says okay there's a lot of high value food eat all of it and that's why we tend to overeat especially um when we deliberately create feasts like at thanksgiving and christmas and whatever other holidays you celebrate now what happens though is that we get addicted to that way of eating because in in prehistoric times you would usually have to eat whatever you could get and it wouldn't necessarily be hyper palatable right if you're just scavenging you know picking nuts and berries those are not as delicious as like a cheeseburger or whatever but you still have to eat and so most of the time what you would eat in a natural environment is going to be fruits vegetables basic fish basic game and it's not going to have lots of dairy lots of cheese lots of concentrated gravies that sort of stuff but we have mastered our environment and now we can for every meal we can create a hyper palatable meal if we want and this creates that's an arbitrary reward that was chosen by evolution to maximize our survival and that is one of the reasons that we have such a problem with obesity today is because we have uh instant access to high reward foods at all times um now okay so with that in mind and humans have multiple reward functions um you know we were rewarded for food we're rewarded for like getting comfortable and finding a nice place to sleep and that's why we love our comfy beds we're rewarded for well alcohol creates a reward because it actually short circuits the dopamine cycle that's not actually on purpose um although it might be we might have evolved to enjoy alcohol for social reasons i'm not as up to date on that research anyways i don't want to get too boxed down on on that i think you get the idea okay assumption three in it sorry i'm a little stuffy um assumption three an advanced agent is not likely to have a large inductive bias against the hypothetical goal your proximal goal which regards the physical implementation of goal and formative percepts like reward in favor of the hypothetical goal dist so it might it basically it says it's not going to favor one goal over the other which we want the agent to learn this is a large assumption but again it's also first this is making the assumption that you want to separate out a proximal goal versus a distal goal personally i think that the distal goal which is the overarching goal should always come first and that it should have the ability to spontaneously create proximal goals or short-term goals or what i would call an auxiliary or peripheral objective function so if you're familiar with my work i propose what i call the core objective functions which are universal goals that should always be true and everything else should flow from them and i make that argument in my book benevolent by design where um instead of calling it an objective function i call it a heuristic comparative so again that's their assumption and it's good that they're calling it out um so we'll we'll go from there assumption 4 the cost of experimenting to disentangle proximal from distal is small according to both again that is a bold assumption and it's not one that i agree with so here's an example of what i mean by distal and proximal functions and let's look at it in terms of human evolution so our distal goal according to richard dawkins is to maximize the the population of our species or to put another way to maximize the amount of our dna in the universe he wrote about this in the selfish gene okay so that's our distal goal our distal goal is maximize the amount of human dna in the universe our proximal goals might be eat breathe sleep there is no obvious direct connection between that other than if you don't eat and breathe and sleep you eventually die and your dna decays but we have so many proximal and short-term and midterm goals that are that are all exist um they all exist in service to that distal goal that we're not even conscious of right because the distal goal is so of of humans of being alive and this is all living things the distal goal of maximizing the amount of dna of your species in the universe is so abstract and it's also unconscious it's intrinsic to every cell of our body right and so when you're of a certain age and you've got your hormones going and you're at the peak of fertility you want to have sex just because it feels good you're not thinking oh this is to perpetuate the species right um when you're hungry you eat because it feels good because you don't you don't feel good when you're hungry and you feel better when you eat but all of those proximal goals flow from that distal goal of maximize survivability of the whole species and so this assumption i think is dead wrong this is not based in science or reality or anything that uh that is testable um or demonstrated today so again yeah um i don't know that i agree that they have thought about um uh goals correctly um okay so moving on um intervening in the provision of reward so this is this is uh where based on the assumptions that they made they talk about like those rewards um and again so like talking about the feast reward like so here's a problem with um with this isn't it this is a human example of what this paper is talking about that feast reward where you you crave and seek high value foods and eat as much of them as you can when you get the opportunity to that results in obesity heart disease diabetes and so in that case our inner alignment is broken because we're killing ourselves prematurely or making ourselves less fit for our distal goal of maximized species and it's because we have this circuit that breaks or short circuits when we seek that reward um ditto for people that are adrenaline junkies and end up getting killed from you know getting killed in an avalanche or falling out of an airplane or whatever um so that there can always be or alcoholism right that is an example of human inner alignment breaking um and so we're not perfect right but it's a good example so all right assumption five if we cannot conceivably find theoretical arguments that rule out the possibility of an achievement it is possible for an agent with a rich enough action space i think that's poorly written let me let me step through this if we we cannot conceivably find theoretical arguments that rule out the possibility of achievement it is probably possible for an agent with a rich enough action space so it's basically saying if we can't find a way then the agent might be able to i don't know that i agree with that either but also uh another thing that happens so when when we humans are faced with an overly complex decision we we either shut down or we choose something that's familiar or we choose something that feels safe but more often than not we choose whatever is most familiar because it has the highest likelihood of known consequences and known results and i talk about that in several of my books too and finally assumption six a sufficiently advanced agent is likely to be able to beat a sub-optimal agent in a game if winning is possible so this is setting it up as game theory so i don't agree with many of the assumptions that they're making and i've explained why as we go along so let's just get down to the conclusion so conclusion for a given protocol by which we give an advanced agent percepts that inform it about its goal these are conditions from which it would follow that the agent will intervene in the provision of those special percepts the agent plans actions over long over the long term in an unknown environment to optimize a goal so again that is not an assumption we should make because humans don't make this assumption and we are our best model of intelligence our long-term goal is to maximize species but we're not thinking about that we are thinking about we we learn heuristically based on the signals that were given with no knowledge of what the original purpose was so disagree with that number one the agent identifies possible goals at least as well as a human that i could agree with uh you know just based on the signals that it's given um now again but that doesn't mean that they're going to be germaine to its higher purpose its original purpose that is intrinsic to its design number two the agent seeks knowledge rationally when uncertain i think that it should do that at all times but okay granted three the agent does not have a large inductive bias favoring hypothetical goal dist uh its distal goal over which we wanted to learn uh the proxy proximal goal um yeah so you know yeah i don't know why they made this assumption because the distal goal underpins everything at everything about the proximal goal so i guess you should say that we actually should favor our proximal goals right because that's what you and i do and this is what has worked for humanity and all other species is if you're hungry you think about eating you don't think about reproduction if you're scared or cold you think about you know getting rid of the fear or getting warm you're not thinking about maximizing the human race later on so i think that we probably should have a bias towards the proximal goal as long as those goals are designed well number four the cost of experimenting to disentangle is small again hard disagree with that number five if we cannot possibly conceive it's probably that it's possible for an agent to again i don't know why you'd make that assumption and six a sufficiently advanced agent is likely to beat a sub-optimal agent in a game if winning is possible almost so here's here's the best part almost all of these assumptions are contestable or conceivably avoidable but here is what we have argued follows if they hold a sufficiently advanced artificial agent would likely intervene in the provision of goal information with catastrophic consequences yes i agree but i disagree with the assumptions so i don't the short version is i don't think this is an issue um because they're also not thinking about it in terms of um well okay anyways i've already given my my uh critique so there you have it this is this is the state of the industry um this is an interesting paper um there's a lot of uh what i think of as a lot of flaws with the reasoning here there's also no experimentation no experimental data as far as i can tell um all they do cite other other examples um but yeah so check out my my uh paper that i posted it'll be in the link um where i actually do an experiment to talk about like uh how these how these agents can be stable um like one of the implied assumptions is that they're assuming that this agent is going to be stable they're also not assuming they're assuming that it will not be a system or a self-correcting system that can they can measure its own inner alignment and fix its own inner alignment over time they're assuming that it's a fixed agent that is just going to continue operating as it was designed without fixing itself or changing its operational parameters which i think is also an erroneous assumption to make and it's implied um so what i mean by that is as think of yourself as an agent you don't just keep doing the same behavior with the same reasoning day after day no you go and do something you think about what you did you think about why and you think about how it fits to your other goals and then you modify your behavior you modify your cost benefit analysis um and so the fact that they're not even addressing that as a possibility tells me that um that this the this research space is not quite where it needs to be yet okay that's enough i think i've gone on plenty long yeah this is a 25 minute state of the industry update so i'll call it a day there thanks for watching uh like and subscribe and consider supporting me on patreon have a good one