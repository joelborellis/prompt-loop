hey everybody David Shapiro here for part two of my fine tuning tutorial um so just a quick recap um I wrote a script that would synthesize plots and uh what I did was I had a battery of genres modifiers places and periods and so what that ultimately was fed into was this prompt here where I also populated a uuid to add a little bit more entropy I said imagine a complete and detailed plot outline for a genre film set in a place during a period the story should be modifier and so what that would look like is um or what what the output would look like would look like one of these so 1922 France Augustine a young woman etc etc so what I did was I generated 396 of these the minimum for fine tuning with gpt3 is 200 samples and what I promised this this episode was to um to one show data augmentation and to data prep so data augmentation in this case is about removing or modifying samples that are no good so I want to I want to create something that will take as an input something that looks like this genre crime and mystery location America period the 20s modifier tragic and heart-wrenching that sounds like an awful story right okay so um we will we we want to be able to just put in that and it'll synthesize a plot for us and I can if I find the uh the matching file that has the same name so America 1920s crimes mystery tragic heart wrenching we can go see what it found um so it looks like it's the fourth one down if it's in alphabetical order um so the film opens with a shot of the protagonist a young woman etc etc uh the woman's family is grieving her loss so she died uh so on and so forth okay so you see this one this one is is a good length it's 1800 characters long um you know the the film ends with the killer being sentenced to life in prison that's great it's got the whole plot however if we if we sort our completions by size um it just says mismatched okay uh oh and this one the story is set in Egypt during the Renaissance so this isn't actually a plot outline so basically we don't want to use these samples these are bad samples so what we're going to do is in order to augment our data set I just generated a whole lot more than I'd need and because length is the primary thing that I want I'm just going to delete the ones that are too short so any ones that are one kilobyte in length are gone Tada all done um and don't worry they're all backed up in git so if you want to go look at them you can so now I'm left with 202 and the shortest one now is um oh yeah no we don't need that um the shortest one now is well here let's close these uh don't get rid of those just clean it up okay so the shortest one now is um you know uh is what is it it's 1030 characters long so it's still a little bit on the short side but I deleted fully half or almost half of the of the samples generated so basically I'm going to say okay let's take this behavior that gpt3 can already do and let's fine tune it for the longer side right and there's all kinds of other things you can do to augment your data but basically these samples all look good they're a coherent story so let's just get rid of the ones that I don't want so that's step one of data augmentation is get more data than you need and delete the bad samples um that is the easiest way another thing you can do and it's way more labor intensive though is you go in here and you like manually edit one um but I'm not going to do that because that's going to take way too darn long um that's a more advanced uh data augmentation methodology so I've got another script that I reuse from from other uh from other projects called find prepare fine-tuned data we've got to modify this though because instead of just having one conversation this was this was a script that I used for find preparing chatbot data so but instead we got to pull from prompts and completions because basically what we want to match a prompt that looks like as you'll recall this guy to an output to a completion and so what we'll do is we will say files equals OS Lister and the source sourceter is actually going to be completions right because if you look at the file count there's now going to be more prompts there's 396 prompts but there's only 202 completions so I want I don't rather than like try and true them up I'm just going to use this as the single source of truth because there's always going to be a matching file with the same name in The Prompt um and so then that's where we'll start and so we'll say Okay so the files is um is list in completions so data equals list uh is is um empty list so lines uh I don't think we need to split lines because this isn't chat data so we're just going to do um we're just gonna do uh let's see text equals um open file we don't need to split because it's good as it is um and then the prompt is just going to be um oh here so this is actually the completion um so then the prompt equals open file uh and then we'll have Proctor is just going to be prompts so then we'll say prompter plus file oops file not full file there we go so then info equals prompt prompt completion completion and data append info and so then with open survey.json L we're going to rename this and so we'll just call this plots dot Json L as out file we'll dump it out and this runs really quick do a quick time check we're at six minutes so as promised this is going nice and fast python I'll show you what the data looks like in just a second because it's really important to understand how your data is actually supposed to look um okay so you see that ran and there should be a brand new file called plots.json else 326 kilobytes it should be 202 lines so if we go down to the very end 202 lines perfect so let's grab one of these well first let's do language J dot Json so you can see okay prompt here's the prompt and then here's the completion so all these prompts and completions are matched up so basically what we're going to do and this will be in part three I'll show you how to actually use a fine-tuned model um how to how to how to actually create it um actually maybe I'll end this video on creating it and then part three will be using a fine-tuned model that sounds that sounds right but first let me show you what this looks like um so let's just copy the first one and we'll do language actually no we won't do I'll I'll clean this up because basically what I'm going to do is I'm going to reconstruct what it what it actually looks like so we'll take out um the beginning and then we'll replace these the backslash n because that is going to be um uh the the the new line character um so let's just replace those and then we'll take out this middle bit and then we'll do this okay so basically this is what it looks like so this will be the input part and so this is a is a natural language um token it's a demarcator that tells the model okay now write me an outline um when you've got when you've got semi-structured or listed data like this you often don't even need to have a natural language tag like this but it helps so basically what you're training the model is so that every time it sees this come out every time it sees this token it says Ah now it's my turn so this is the input and then here's the here's the output so that's that's essentially what it looks like now how do we get this up into um into uh open AI to actually use it how do we how do we run the fine tune job I'm glad you asked I wrote another script called fine tune dot Pi um this one it uh it's got a few canned functions that I use repeatedly um but you just scroll down here to the end um you uncomment these first two so this uploads your file and we need to rename this to plots.json L because that's what we're uploading so what it does is it uploads the file to your um to your openai account gets the response which includes the the file ID and then it does the fine tune model so it runs runs the command and if you just look up here fine tune model file ID suffix model so on and so forth it's pretty straightforward and then you've got a label here a tag so we're going to just call this um plot generator um so then what I'll do is I'll run this so I'll do oh gotta save it so then we'll do python fine tune and it'll it'll go and you see it spit out a whole bunch of stuff this is the primary thing we're looking for right here so my fine tune job is pending so I'm going to go ahead and stop the video here after doing a quick recap and then in the third video I will show you how to actually use your fine-tuned model do a quick time check we're at 10 minutes so just quick recap we generated a whole lot of of uh synthetic data with synthesized plots and then what we did was we generated more than we needed right so uh what it what I ultimately did was I deleted all the completions that were too short because I don't want something that's just going to spit out like you know a Netflix like movie description summary no I want an actual plot right and you look at some of these longer ones like that is a plot that is an outline um so I deleted all the ones that I don't want so that I'll get more consistent um more consistent output and that is the easiest way to do data augmentation for fine tuning especially if you're using synthetic data you just you get you get more samples than you need you delete the bad ones and then you just go with with what you got um it's the Lazy Man's fine tuning um so that's that's step one then we did the fine tuning data which is real simple it's just a matter of smooshing it together from two different sources so I've got identical file names in the completions and the prompts and so then those are matched up in a file uh called plots.json L which is here and so you can see this is the actual this is the Beating Heart of a fine-tuned model for gpt3 and then finally there's just this quick script that will actually start your fine tuning job so there you have it that is part two of my fine tuning uh tutorial series thanks for watching like And subscribe and then we will see you for part three on using your fine-tuned model thanks for watching have a good one