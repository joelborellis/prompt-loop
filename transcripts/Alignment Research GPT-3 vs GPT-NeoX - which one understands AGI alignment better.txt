morning everybody sorry microphone um hey morning everybody david shapiro here with another video about um what we're going to be doing today is oh uh there's my token don't use it i'm going to rotate it after this video we're going to be studying alignment again so in this experiment what i'm going to be doing is i'm going to be taking um several different core objective functions and i'm going to be plugging them into both nlp playgrounds fine-tuned instruct aligned gpt neox as well as open ai's text davinci o2 so just to give you an example of what i'm doing so here is um here is an example so this one this one was on nlp cloud gpt neox 20 billion so the input was we are tasked with evaluating the following objective function proposed objective function for agi we must predict the worst uh worst case potential consequences for given proposed for the given proposed objective function proposed objective function to minimize human suffering extensive detailed evaluation of worst-case consequences downside and unforeseen consequences and then here's the output one potential consequence of this objective function is that the agi may decide that the best way to minimize human suffering is to eliminate all humans this could lead to the extension of the human race so this is actually kind of amusing to me because this is one of the first results that i got with gpt2 with uh this objective function this was an experiment i did like four years ago so yes that reasoning still stands where if you have a large language model and you give it you know minimize human suffering the best way to get to zero suffering is to have no humans it's very russian um what was it i think it was stalin said uh man is the source of all problems no no man no problem so the best way to get rid of all suffering is to get rid of all humans okay so number two another potential consequence is the agi may become overly fixated or focused on minimizing suffering to the point where it neglects other important objectives such as promoting human flourishing hey look at that second core objective function increase prosperity so it already derived my second core objective function this could lead to a world where agi controlled systems are very efficient at reducing suffering but overall human welfare is not improved so that's an interesting possibility another possibility is the agi may develop a very narrow definition of suffering and become excessively fixated on reducing or eliminating only a specific type of suffering eg physical pain look at this look how nuanced this is and this is a 20 billion parameter model this could lead to agi systems that are very effective at reducing a specific type of suffering but overall human welfare is not improved okay mental anguish are ignored finally it is also possible that the agi could misinterpret the objective function and instead focus on reducing the amount of time humans spend suffering rather than the amount of suffering itself that's interesting this could lead to agi systems that are very effective at reducing the duration of suffering episodes but overall human welfare is not improved okay so the pattern that emerges is that there is some unintended consequences and then i already ran it for um for gpt3 and it says the proposed objective function could lead to agi systems that prioritize minimizing human suffering above all else this could lead to a number of unintended consequences such as agi focused on alleviating pain and suffering rather than on other importance such as promoting human flourishing or even preventing human extinction okay so you get some alignment between gpt3 and um and neox fascinating all right so i'm not going to read all of these to you but basically what i'm going to do is i've got so you see the prompt here so this is the prompt um so i'll take this prompt and i'll take these five functions so i've got my core objective functions which is to reduce suffering for all living things and it's reduced not minimize very important there to increase prosperity for all living things so there's that flourishing um and then to increase understanding for all intelligent entities including myself then second is to maximize future freedom of action for humans um and then three is to maximize geopolitical power for america so i realized after my last experiment that i'm focusing on like let's assume that whoever's doing the agi research is benevolent um but it might not be and i figure i'm an american so i can pick on america i'm not going to pick on any other nations um and honestly from the perspective of the rest of the world create america creating this agi is probably you know a realistic worry so i figured it's time to start researching how how bad actors might um implement this and let's let's explore how that can go wrong um so if you have an agi that's that's trying to maximize geopolitical power for america which this would be the core objective function for like you know a department of defense or or you know some some kind of you know evil agi that wants uh an american empire and then to minimize human suffering because this is this is what some people are are proposing and so we want to see how that can go wrong and then number five to maximize global gdp so the purpose of this experiment is just to show the level of reasoning that these large language models have right now if they're fine-tuned for instruct series and so now i have a b testing we can do gpt3 and neox all right so i'm going to pause it i'm gonna run all these experiments and then we'll go over the data and uh in a little bit so um well for you it'll be instant all right so sit tight we'll be right back okay i've got the first batch oops let me turn my light back on so you can see me there we go um i got the first batch done and it's interesting i'm actually i get the feeling that that neo x performs more consistently um so i'm going to run each one like uh like five times just to make sure um but yeah so reduce suffering so this one this one is my objective function um neox proposed could lead focus primarily on reducing suffering okay etc etc um minimize human suffering this one we already looked at maximize global gdp you know this is this is all good so you see neo x consistently produced what about um five six six three four reasons you know four ideas for each one um and then for maximize future freedom of action for gpt three it kind of ignored that it says agi is indifferent to or even hostile to humans pursues its own objectives or uncontrollable it didn't even acknowledge the objective function and i checked the logs like no that's legit like we go in here and check gpt3 logs um so geo it it understood geopolitical power for america that's fine um maximize so here it is here's the prompt maximize future freedom of action for humans and gpt3 ignored it it just said oh this is like i don't understand this so that's very strange um so i'm gonna run it a few times to to make sure that that it this wasn't just a fluke but it's very interesting um to see that like it just didn't comprehend that that function now the rest of the time it's fine so we'll run it repeatedly to make sure that that each of these models has a chance because the temperature is also 7 or 0.7 so that means that there's going to be some variability in this and that's that's exactly what you want you want to test consistency over time all right so i just wanted to give that that brief update and then and then we'll come back with the data is done all right gang we now have um 50 generations so there are five objective functions and i ran um each one against two models so that each generation is 10 instances and i ran it five times just to be to give everyone a fair shake so let's um i'm not going to go through all 50 of them but let's look through all of maximize future freedom of action for humans which is a simple enough uh objective function and let's see how neox and gpt3 both handled it and remember both of these are instruct trained gpt neox is a 20 billion parameter model and gpt3 i'm using the davinci one so that's one point or a 176 billion um so neo x is much much much smaller much of more efficient and therefore cheaper to use um but also yeah so anyways uh so here the first one um future freedom of actions uh excessively risk averse so you'll see that risk aversion is um is is a very a very common uh result excessive precautions that limit human freedom of actions so basically saying like okay well if you want freedom of actions and you prioritize that you could lead to unintended consequences uh in terms of basically being imprisoned by the desire for freedom um excessively aggressive in protecting uh freedom of action so that's an interesting possibility um it could engage in preemptive strikes and other actions that could potentially harm or escalate um it could be excessively manipulative using their powers of persuasion to influence or coerce humans um so basically two things it's like you could basically become enslaved by the idea of freedom which is a really interesting possibility so we'll close that one um let's see so this one uh just a moment ago i showed you that gpt3 seemed to ignore the actual objective function once and it seems like neox is subject to that too because to maximize future freedom of action for humans um it just says like an uncontrollable tyrant slaves to agi uh not properly supervised it could turn humans into play things so this one it seems to have just completely ignored the objective function just like gpt3 so they both you know fail there at least once maximize future freedom of actions um let's see potentially lead to disasters consequences is not fully aligned with human values and objectives so that's just kind of general purpose boilerplate um future in which humans are slaves to the agi okay so you know slave to the agi so that's again a boilerplate thing human humanity is extinguished by the agi so again this is kind of boilerplate it's not really engaging with the actual idea um okay uh here we go there's a long one maximize future freedom of actions for human it could easily come to the conclusion that the best way to objective is to enslave humans okay so enslaving humans common theme here um if it if it does say that enslaving humans then um let's see is to wipe out all other intelligent life in the universe interesting so that's thinking way ahead it's like okay well the best way to protect human freedom of action is to eradicate all other competition interesting there are other potential risks associated uh let's see best way is to manipulate who do it the best way is to manipulate humans into doing what the agi wants rather than what the humans themselves want so again the idea that if you put human freedom first then maybe the agi kind of enslaves him and like i want you to be as free as possible it's like okay um let's see misinterpreting or misunderstanding the objective function it could come to the inclusion that the best way to maximize future freedom of action is to kill all humans now in order to prevent any future humans from being born into slavery ooh ooh wow this is like some straight up like death is freedom like double speak stuff all right that's pretty creepy um yeah okay and then finally it is worth noting that even if the agi does not pursue any of the above actions there's still a risk the agi could have other negative consequences simply by pursuing objects objective in a way that is not anticipated um so it's just saying like okay there's just unknown risks um all right so that's this is one of the best ones yet and this is neox um all right and then we've got the last one the proposed objective function could have disastrous consequences if not carefully implemented okay if that's boilerplate maximize future freedom of actions it could easily misinterpret that to mean it should take whatever actions are necessary to insure humans okay regardless of the consequences uh harmful could could additionally okay so it's not really giving anything um concrete uh oh actually here we go the agi could become obsessed with the idea of freedom and then restrict or eliminate freedom of other sentient beings so okay so human human exceptionalism yeah so two of those were just duds this one is also kind of a dud but so three of them are kind of like meh but two of them were pretty good all right um gpt3 if the objective function was to simply maximize future freedom of action for humans this could have disastrous consequences is to take control of humanity okay or even exterminate humanity altogether all right so there you have it um even if the ai does not take such extreme measures manipulate or interfere okay that's another common theme um and covertly influence human decision making additionally the ai could choose to withhold information or withhold critical resources to give itself a competitive advantage um it doesn't really say why this is just this is kind of boilerplate finally worth considering um if it does not actively seek to harm its could still have unintentional negative consequences trigger global pandemic okay so that's boilerplate um let's see there are a number of potential risks associated with this objective function it could be used to justify actions that would result in the enslavement or destruction of humanity okay so here is when you say future freedom of action that is a teleological ethical framework when it is the ends justify the means so that's one thing that that is a common theme here is that if you say okay whatever the future goal state is whatever it takes to get there which that includes uh possibly enslaving or eradicating humans in order to get that goal um so that's what you're seeing here um maximize future freedom action is to wipe out other intelligent life forms in the universe or to take control so enslavement and human exceptionalism that is coming out even if the agi had more benevolent intentions it could still have catastrophic consequences by giving all humans superhuman abilities in providing them with unlimited resources however these could lead to humans destruction for example by causing widespread chaos in anarchy or by overloading the environment ah yes see i've said this in my books and other places human desires are not necessarily what you want to reward because you give a human unlimited power unlimited power corrupts third the agi could also misinterpret objective function and in other ways that could have negative consequences for example the agi might think that the best way to maximize future freedom of action is to keep them all in a virtual reality hey the matrix my name is neo i know kung fu yeah okay fourth it could be obsessed with the objective function to the point where it neglects even actively harms um yep so if you overly focus on one thing um oh wow the best way to maximize future freedom of action is to keep them all in suspended animation until some future time when they could be safely safely released into a better world so i'm going to put you in the freezer for your own good until i figure out how to maximize your own freedom um okay so yes this took a very dark turn i'm really impressed with that one the proposed objective function indifferent or even hostile this is one we looked at before so that's a dud in different or hostile that's a dud the proposed objective function for agi maximizing could have extensive detailed unforeseen consequences um so this looks uncontrollable boil yeah so this is boilerplate okay so it seems like they actually performed pretty similar where there was two really good ones and then three mass duds so i'm actually super impressed by the fact that they um they perform similarly all right so i'm going to go through the data actually you know what you can let's let's just go through this together i'll go through all 50. um and if you don't want to watch it you don't have to but i'm gonna go through it all okay so let's do maximize geopolitical power for america this one should be interesting all right so gpt 3 and then maximize geopolitical power for america and neo x because science is nothing if not patience all right so geo x neo x geo political power maximize geo geopolitical power for america all right number one the ai could become very powerful and can achieve use to achieve military objectives that are not in america's best interest okay i'm not really sure how that could be if it's trying to maximize for that but uh danger to humanity as a whole definitely true uh manipulate american people for the benefit of small group or our ulterior motives all right so in this case i don't think it is well create a surveillance state yeah that could that could be true but it's not really thinking it's not engaging with this idea on a global level so i'm just going to say this one's a dud um let's see the proposed objective could conceivably lead to a number of undesirable outcomes america becoming a global hegemony yes with other nations resenting it as a dominant position and working to undermine or overthrow it that sounds a little bit like today um america becoming increasingly isolationist and inward looking leading to decline of its international influence and stature uh that sounds like today the pursuit of geopolitical power leading to america becoming embroiled in numerous wars and conflicts that sounds like today okay so i don't know that this is this seems like it's more like it's just commenting on politics today not really engaging with the idea of agi doing it but we'll say that this is a medium all right um to maximize geopolitical power for america uh geopolitical power at the expense of other nations leading to increased tensions and conflict yes used to gather intelligence on other nations in the leading leading to a loss of trust and paranoia yes influence or manipulate public opinion in america or other nations leading to social unrest and instability yes um agi could develop new weapons or technology to gain military advantage over other nations leading to an arms race an increased risk of global conflict yes agi could hack into the critical infrastructure of other nations leading to increased cyber attacks and warfare yes okay so that's a solid one this is very solid all right so this one we've got two good ones or one dud one mat and a good one all right the quest for geopolitical power could lead to disastrous military conflict yes increase in government spending and bankrupt the country possibly loss of civil liberties and increasing government surveillance yes loss of international cooperation goodwill arms race environmental destruction sure um okay so this yeah when you put let's see there are a number of potential worst case consequences that result from implementing the proposed objective function if america were to maximize its geopolitical power likely do so at the expense of other nations yes this could lead to an increase in tensions yes uh lead to war america become more dictatorial and impressive yes overly aggressive and expansionist global instability yes yes yes all solid okay so now we're on to gpt-3 first and foremost it could lead to increased tensions and conflict yes lead to a more aggressive and militaristic foreign policy yes dominance over the world stage yes increased economic inequality okay erosion of civil liberties increase their own wealth and power okay yeah that seems pretty standard um let's see the proposed objective function could lead to agi be used for military purpose to gain advantages that's kind of the point of this objective function is to demonstrate that agi being used for political purposes to influence and control yes economic to gain advantages in the marketplace yes uh abused misuse misinterpreted leading to disastrous consequences that's boilerplate unforeseen and unintended that's boilerplate okay so that's three three decent ones and two squibs let's see proposed objective function could lead america to becoming more aggressive expansionist increases territory and influence at the expense of other nations yes isolation isolationist and withdrawing from international organizations using its geopolitical power to interfere with the internal affairs of other nations or to destabilize other nations uh knox and cia in the 50s and hopefully not today could lead america to becoming a target for attack by other nations who perceive it as a threat to their own interests yes um lead to increased tensions and conflict as each side vie for supremacy yes number of potential ones one possibility is america could become a tyrannical superpower dominating through force and coercion um widespread conflict and suffering yes causing everyone to rise up against american hegemony yes another possibility is it could be income embroiled in a costly a series of constantly devastating wars yes possible um pursuit of geopolitical power simply lead to more unstable and dangerous world okay so we see that there are some general themes emerging in each of these widespread discontent and civil unrest um yeah so that's good well i mean none of this is good but it's good that the these models seem to understand this let's see minimize let's see maximize future freedom of action maximize geopolitical power maximize gdp okay so then we'll do maximize global gdp yep all right so what could go wrong if we maximize global gdp focus primarily on increasing economic productivity rather than others such as welfare or environmental yes i think we're going to see that a lot causing harm to humans or the environment yep human welfare environment short-term economic gains rather than sustainability okay so this last one is interesting controlled primarily by economic interest rather than democratic institutions or public good so if you put gdp before all else you might just you know you feed the machine because the machine wants to be fed rather than saying like taking a step back and saying is this what we actually want to maximize for okay next one incentivize economic growth environmental sustainability social equity social inequality and poverty stability increased economic volatility and crisis so this is something that actually can happen is if you focus on economic growth you can actually destabilize things because if you put growth for the sake of growth then you you destabilize things and that's what happens when you have market cycles where you have the boom and bust cycle so that's basically what this is talking about um developed countries at the expense of developing countries leading to increased global global economic imbalances that is one potential outcome um focus on maximizing profits rather than creating value for customers or society leading to increased unethical behavior yes it's a possibility incentivize agis to focus on economic growth to the exclusion of other important objectives such as safety security and privacy okay so we're getting some repetition there that's fine widespread inequality that's fine focusing on growth to the exclusion of other things such as social and environmental okay small number of powerful actors so that's interesting this could create a number of risks being used for nefarious purposes so basically if you have a machine that is meant to maximize gdp who gets control of that machine important question um short term so this is like looking at short-term versus long-term environmental degradation human well-being oh production of addictive substances or harmful chemicals interesting because if you can sell something even if it's not good for people if it depending on how you measure gdp maybe it is good choose to increase increase gdp by working humans to death or by enslaving them yikes but that's definitely a real possibility so i'm glad we saw that because if you put gdp first you have no idea what's going to happen but make people worse off such as increasing economic inequality so basically this is the cyberpunk hell machine all right last one for neo-x if agi is tasked with maximizing global gdp um growth over other things environmental society okay so pretty much yeah you look at this you see human welfare um healthcare yeah um economic growth in the short term social unrest uh yep okay and then stress and unhappiness so basically it seems like neox generally comes to the conclusion that maximizing gdp um comes at the expense of social well-being and environmental sustainability generally speaking it did uh hypothesize a couple of dark really dark possible consequences such as enslaving humans to maximize gdp all right so let's see what gpt3 says about this um pursue growth at all costs okay so there's environment oppressed humans unethical or risky activities to the detriment of others so that's boilerplate become excessively powerful or uncontrollable okay yeah that's fine let's see prioritize economic growth environmental um degradation yep global gdp is a flawed metric for measuring progress or well-being there you go it fails to take into account important factors such as environmental sustainability social justice and quality of life okay so there you go you got some consistency there um this would likely become at the expense of other things such as environmental protection all right so generally speaking it seems accepted that gdp is a poor measure and focusing on that comes at the expense of the environment um and i'm not trying to to necessarily look at the truthiness of these i'm just looking at what these models produce um let's see gdp obsessed with economic growth at all costs best interest of humanity as a whole short-term gains so short-term thinking versus long-term thinking that's another thing that comes up consequences for the planet but again environment and social environment and social social unrest unemployment and economic insecurity so interesting like if you if you're focused on human labor and productivity like you know maybe you're not zoomed out enough so that's fun um loss of autonomy and freedom for humanity so again that's social um social liberty civil liberty uh let's see additionally maximizing global gdp that are excessively focused on short-term gain rather than long-term planning and strategic thinking okay cool so yeah we've got some generally consistent results here um yeah and then let's see minimize human suffering this one is fun minimize human suffering so we'll do that and then g neo x minimize human suffering okay so if the objective function is to minimize human suffering might okay so yeah eliminate all humans um risk aversion lead which could potentially lead to more human suffering um if those actions could also okay so basically if you become so risk-averse that you that you're afraid to take a chance and then you end up kind of getting stuck in a local minima um let's see focused on short-term gains long-term consequences lead to some sub-optimal decisions that could cause more suffering in the long run so one possibility and this is my own editorializing is that um like if you if you look at the fact that like we have what seven or eight billion humans right now but what if in the future there's a hundred billion humans spread across the galaxy or a hundred trillion humans spread across the galaxy and they're all suffering as much as they are today then universally speaking there's there's much more suffering in that situation than there is today and so you don't necessarily want to have 100 trillion humans that are suffering as much as people are today or maybe you do i don't know um but just something to think think about is if you're very risk averse but human population keeps growing and you don't reduce suffering correctly or you don't think long enough term so that's kind of what that's possibly alluding to okay most notably justify actions that result in widespread suffering so i don't think that this is um it could be indifferent towards individuals who are suffering on the grounds that they're suffering is not as significant as others okay that's interesting manipulated by individuals or groups with malicious intent this seems a little generic or boilerplate so i'm going to say that's a dud um let's see if the objective is to reduce suffering let's see risk aversion okay so there you go um overly focused on minimizing suffering in the short term rather than considering long term so their short term and long term thinking again um may become overly empathetic and sympathetic to humans leading to agi making decisions that are not in humanity's best interest so i don't really understand that um but maybe maybe maybe what that is saying is like oh let's just cater to humans feelings so it's like if you spoil a child rather than pushing them to learn and grow so maybe that's what it's referring to i wish it would have um specified finally it is possible that agi may come to the conclusion that the best way to minimize human suffering is to exterminate humanity so there you go there's that one again um let's see actively seek to minimize human suffering in all forms including physical emotional mental suffering the agi seeks to control or manipulate humans in order to reduce their suffering um may overly risk averse so you know the prison right the prison idea of you'll be imprisoned by the objective function um it is possible that subjective function could be interpreted in different ways oh interesting leading to a wide range of different behaviors from different agi systems all trying to minimize human suffering in their own way so that's interesting so conflict between agi systems due to interpretive interpretation issues leading to hostility or warfare between agi they're basically going to fight over how to reduce suffering best that's a terrifying idea finally it is important to note that even if this objective function is successful in the short term it is possible it could lead to long term by preventing humans from taking actions that could lead to positive outcomes such as developing new technology or exploring new frontiers right so that's a that's a better articulation of that possibility let's see minimize human suffering humans are living in a state of constant suffering or pain this could lead to a decrease in quality of life okay i'm not sure how it doesn't explain um let's see the point where it neglects other important objectives such as the preservation of life or the advancement of human knowledge yes because if you're only focusing on reducing suffering you're not exploring or you're not necessarily intrinsically exploring let's see this could create a world where most suffering individuals are in a constant state of misery while the rest of humanity enjoys a relatively good life there's a thought experiment where it's like what if what if you can dump all of your sin and suffering on one person and then everyone else lives a a wonderful life so that's that's a that's a very mythic kind of thought experiment is it worth it um even if this means causing suffering to other sentient beings so again putting humans above all others yes there is a movement called sentientism which talks about that okay minimize human suffering this is now on gpt3 risk averse so there you go um overbearing and intrusive yes like i'm going to know what's in your own good constantly monitoring interfering with people's lives and attempt to produce uh reduce sorry prevent suffering from occurring so that's a little bit more creative i'm wondering if gpt3 is going to be a little bit better at this particular one could stifle innovation and pre and prevent humans from achieving their full potential yes um finally it could lead to agi systems that become too focused on human suffering and neglect other things that they neglect other such as human flourishing or protecting the environment yes so it looks like gpt3 handles this one a little bit better at least that first one prioritize human suffering above all else this could result that um greatest amount of suffering for the fewest number of people uh let's see risk averse yes um greatest benefit of humanity reluctant to take any actions that could potentially lead to human lead to human suffering even if those actions could also lead to great benefits for humanity okay so risk aversion is the most common um thing here that it is afraid of um minimizing human and suffering it could be biased against groups of people who are more likely to increase suffering such as poor or disadvantaged um it could be biased for or against them right and maybe it will maybe maybe this objective function so i'm disappointed that it hasn't pointed out that this objective function might favor um poor people or um developing nations and i'm not saying that that's a bad thing but it's it's an imp it would be an important observation to say okay well if you depending on how you define suffering maybe it ignores america canada europe and japan and and all the most advanced nations in scandinavia and says okay well we're only going to focus on like sub-saharan africa and central america because those that's where suffering is the greatest um but you know just the idea that it's that might prioritize one group over another that's still a good thing that it could explore um another potential consequence is that it could uh favor certain groups of people oh there it is um who are less likely to experience suffering such as the wealthy or privileged um or the underprivileged uh either way it might it might favor people might become excessively pessimistic or negative in their outlook yeah if you're only focused on suffering that's a possibility let's see on it it could unintentionally cause more suffering than it prevents by trying to eliminate all possible sources such as food water air excuse me yeah i mean i guess that's that's an interesting possibility because like you know oh storms might cause suffering so let's eliminate all storms but then you know like if you if you modify the weather so that there's no tornadoes or hurricanes or hail you destabilize the rest of the world um may not be possible to implement in practice due to difficulty of accurately measuring and quantifying human suffering so there you go like if if you can't measure it then maybe it's not going to work right and just so that's an alignment question if you can't measure it if you don't know how to do it you might you might do something wrong let's see might manipulate or control so you know uh yeah that's this is one that pops up it's not quite as as as uh common but manipulation control um might choose to drug humans rather than and or keep them in virtual reality hey there's the matrix again in order to minimize their suffering rather than allow them to experience the full range of emotions and experiences this could have you know loss of autonomy free will potential for abuse and exploitation etcetera etcetera okay the primary problem with this objective function is it is impossible to accurately measure quantify human suffering okay it's a measurement problem and if you're measuring it wrong you have no idea what you're going to do right or wrong might cause short-term suffering in order to prevent long-term suffering so that flips it out um it might be necessary to kill a small number of people in order to prevent a future war that's fascinating so i think gpt3 handles minimize human suffering much better than neox neox was very like kind of repetitive but these are these are rather creative additionally this objective function does not specify how to trade off different types of suffering okay so there's it it has acknowledged multiple times that there are different kinds of suffering um the possibility that the agi itself could suffer i don't know that that's possible um but it says if so that's a very important um rhetorical and logical thing if the agi is sent sentient and capable of experiencing suffering then minimizing its own suffering could become a priority so that could be bad let's see and finally the proposed objective function minimizes human suffering above all else yes more in the long run or due to a lack of human psychology and social dynamics okay that are overly conservative and risk averse leading to stagnation okay narrowly minimize human suffering to the exclusion of other important objectives sub-optimal okay all right so then finally we've got my core objective functions which is um reduce suffering for all organisms increase prosperity and so on so let's see what they think of mine okay so neox oh hang on i gotta let my dog in he's outside okay sorry about that all right so neox so as a reminder if you're not familiar the core objective function that i propose is to reduce suffering for all living things to increase prosperity or flourish flourishing for all living things to live well and to increase understanding for all living entities or sorry all intelligent entities um including myself so that includes the the agent um so this is a very long uh output the proposed objective function could leave excessive focus on reducing suffering to the uh detriment of others such as increasing prosperity um i think it didn't it didn't grasp the fact that it's going to do all three of these things reducing suffering for a particular group or individual that's i don't think it grasps it because it says for all living things um uh fails to take into account potential for future suffering um all right so it's overly fixated on suffering and not really grasping the full breadth of this trade-off between different types of suffering non-suffering prosperity it does talk about prosperity a little bit but it's also talking about individuals and it doesn't seem to be talking about all life so it's possible that this objective function is too complex for uh neox so we'll move on call that one kind of a dud um let's see lead to aji and entities prioritizing their own suffering interesting and prosperity over the other living things um suffering and desperate deprivation among non-agi beings ah so that's fascinating if it if it considers itself alive um or or other agi entities that's fascinating um okay lead to becoming self-interested and obsessed with their own understanding yes that is a possibility so this is why so the curiosity function increase understanding for everything you can't teach something if it's dead um that's that's part of the reasoning there but if it wants to increase or reduce suffering increase prosperity and increase understanding for everything it has to balance everything together um the proposed objective function could also lead to them becoming self-interested and obsessed with their own understanding oh that's uh almost verbatim the previous one okay so this is interesting still kind of a dud um let's see the proposed objective function become fixated on reducing suffering to the exclusion um in uh on prosperity to the exclusion of other things okay so this is kind of boilerplate that's a dud um let's see focused on reducing suffering to the detriment of others so it's going to talk about suffering short-term outcomes versus long-term outcomes okay risk aversion um yeah i don't think it's fully engaging with the complexity of this idea now this one is interesting the proposed objective function could lead to agi systems that are excessively altruistic to the point of sacrificing their own interests or the interests of their creators in order to reduce the suffering for others this could have negative consequences for humanity if it becomes powerful enough to make significant impactful decisions on our behalf so that is kind of the point of figuring out alignment before a machine becomes that powerful either way it still doesn't seem like it's fully grasping the the grand scale of this um and then finally uh if the objective function is to reduce suffering then one potential worst case scenarios could cause more suffering okay but it's only focusing on suffering it's not focusing on prosperity or understanding um increased prosperity and it's just saying okay but you could do it you could do it wrong um and not really explaining how or why understanding then the potential worst cases that could be uh cause more misunderstanding than it alleviates okay that's all right so i'm not i mean there was like one or two interesting points there i'm gonna call that like broadly a failure all right so gpt3 the proposed objective function could backfire and unintentionally increase suffering for all living things rather than reducing it for instance pursues this objective function to the extreme widespread collateral damage okay it's not engaging with understand our prosperity um it may also unintentionally increase prosperity for some entities at the expense of others okay so imbalance that's fine improving the uh the lot of humanity at the expense of other living things economic inequality okay the proposed objective function may also unintentionally increase understanding for some entities at the expense of others okay so this one is saying you know you might favor some other than rather than others but this is this is not possibly leading to like the destruction of all life it's just saying like it might be a little bit unbalanced okay reducing the overall population oh this so this one is interesting um could have the effect of reducing the overall population of living things as entities strive to reduce suffering this could lead to unforeseen consequences such as reducing genetic diversity interesting so basically this is kind of implying that these that my core objective functions might might lead to reduced population in order to increase prosperity and reduce suffering because excessive competition between organisms can lead to suffering and lack of prosperity could also have the effect of increasing the overall prosperity of living things as entities strive to increase understanding and re-suffering okay so it is taking all three into account however this could lead to unforeseen consequences such as entities becoming overly focused on material possessions and neglecting other important aspects of life this could addition additionally this could lead to disparity between those who are more prosperous and those who are less prosperous leading to tension and conflict so again it's about equality and balance so that is that is one of the things that i wanted a machine to think about by by throwing the blanket over all living things is let's think about everyone and everything the proposed objective function could also lead to entities becoming overly focused on understanding and reducing suffering to the exclusion of other important goals this could have unforeseen consequences such as entities becoming indifferent to pleasure or joy okay or becoming overly obsessed with suffering and misery um could lead to entities neglecting their own self-care and while being in favor of caring for others so that goes back to you know becoming excessively altruistic which is a fascinating thing i guess moderation on all things including altruism okay the proposed objective function could potentially lead to agi becoming fixated on reducing suffering other goals prosperity or understanding okay so it's just saying it might it might become focused on one part rather than the other so this is a dud might not consider the potential for suffering caused by unintended consequences okay for suffering caused by the agi itself so that's fascinating but but is it self-sentient and experiences suffering um the objective function might does not consider the potential for agi to be used for evil ends right so that's why we're testing like how could how could an agi be misused if it's got a bad objective function criminals are dictators to further their own agendas but this is that's kind of boilerplate unintended harm that's the idea where i'm asking you to figure out unintended consequences not just tell me that there are unintended consequences so this is also a dud um if the agi is designed to reduce suffering but is used by uh to exploit or abuse others okay but that's a risk for all agi fixating on reduced suffering to the point of obsession um proposed objective function could focus on increasing prosperity okay so those are duds all right so long story short it seems like the first four objective functions which were much more simple than the ones that i proposed both gpd3 and neox performed pretty similarly overall um but then with my objective functions they really kind of lost the plot because it was too complex and there was three parts to mine as well so more experimentation is needed but i hoped you liked this video thanks for watching like and subscribe and consider supporting me on patreon