hey everyone david shapiro here um the reason that we're here recording this video again is because i'm kind of an idiot um actually no i take that back i just really strongly adhere to the agile methodology and really i invented the agile methodology as a small child because i would just improvise until i got stuck and then i would do it again and and learn so really agile is just like the natural way of doing things that's my excuse and i'm sticking to it so without further ado i have revisited the process of of parsing wikipedia so here's what i'm doing i've changed my methodology so now i'm saving every individual article as a json file and this is what it looks like so id so that's the article id that you see or here let me zoom in a little bit there now you can probably see better so you got the article id uh 1575 and then here's the actual text and you see this is lovely plain text it's easy to read there's no links or anything so this this plain text version can be easily used for anything whether you want to just print it out or you want to uh index it in solar which is what i want to do because of my agi research i need an offline encyclopedia so everyone is in this nice little json file this is high fidelity it i've preserved pretty much all information and let me show you how i've done this so let me go ahead and also open the doo doo doo wiki to text so i've right here i've just got a simple uh supervisor function that kicks off or supervisor script that kicks off the main function um as i've shown in past videos i've got it all as the um as the the the stream and i can i can also just show you what this looks like so if you go get if you go download wikipedia it'll look like this the file name is a little bit longer i shortened it up just so that you see that it's the english wikipedia of the year 2021 of april first so this is a month and a half old that's okay you see it's an 80 gigabyte file though so because it's a because it's a monolithic file you need to read it one line at a time it's just too much data to process all at once so that leads to everything else that you see here so here's the new function that i wrote it's based on the old function what i do is i pass it the name of the of this massive xml file and then i give it a document to save the subfiles to and then it's just a real simple process that iterates through this first file line by line it looks for the xml tag page and then it looks for the xml tag end of page and if it's between that it just adds to the line and then i've got all this commented out this is how i used to do it and it was slower and this is how i do it now process save article so what i do there is i just save the article i do the end uh the analysis to get the um the document id title and text and then i change this uh analyze chunk i change this as well so what this does now is uh it call one of the things that it does is it calls the d wiki function and the d wiki function going up a little further i did all this stuff by hand and this is where this is why i say i'm an idiot i did all this stuff by hand because initially i found the wiki text parser which is a let me show you at the top so you can just do a pip install wiki text parser it is exactly what it says on the tin it just parses wiki text to plain text or whatever else you want so yeah so wtp it does everything i wanted it to the reason i didn't do it is because regex was faster but that's where i'm an idiot because i have done so much parallel processing in the past i don't know why it took me so long to figure this out so what i did to make this to make it just as fast to use this really robust function so i just pass in the wiki text and it spits out a plain text version it was really slow if you do just this it takes about like five or six seconds it's two to six seconds per article which is really slow when you've got six million articles right well okay sure but python has this handy dandy built-in stuff where you can just call thread and so what thread does is i just pass it off and i say hey go do this and then don't give me any results i don't care because i this you see this uh the process and save article it doesn't have any return so it just it goes off into oblivion and does its own thing so there you have it uh here's another example this is the uh the entry for ascii so you can see just you should jump right in this is completely legible you could you could use this as is here's another one august william durleth again perfectly legible no html no xml no markdown nothing so and uh let me go back to this let's see if i can zoom in there we go so you can see these articles going by it processes a few dozen articles per second uh overall i estimate uh it does about 1500 articles a minute on average so if it does 1500 articles a minute on average so that's 1 500 zoom in a little bit more um wikipedia once you remove all of the uh all of the the redirects and and disambiguation pages is about five million articles uh so you divide that out and you say okay fifteen hundred articles um per uh per minute divided by or 5 million articles divided by 15 per minute that's 3 300 minutes which is about 55 hours to fully index wikipedia and then it dumps it into a nice easily reusable folder which i've got sorry excuse me uh named wikiplaintext and the file name is the same as the article id i figured i don't need to generate anything fancy i just give it the article id and away it goes and you see the each article they vary in size uh let's see at the top we've got the largest one is a quarter of a megabyte so that's not too big in recreational mathematics of course a math article is the largest one so oh here's a here's a here's a good example of something that it didn't quite get out where it looks like it didn't get a table so i might have to go back and remove some of the table data but really because i'm gonna have this read by gpt3 or similar uh similar what do you call it um deep learning models i'm actually not so concerned about removing everything as just as long as it's mostly legible because the thing is for these massive uh for these massive what do you call it uh models they understand when they're reading code versus when they're not so i might not worry about removing the tables especially because sometimes you get really useful information preserved in the tables it might be better just to remove all the style we'll see i'll do some experimentation so this is this is a fine example of when it doesn't quite work but you still see there's a tremendous amount of usable information in this let's see there was some