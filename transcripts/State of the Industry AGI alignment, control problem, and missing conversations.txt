morning everybody david shapiro here okay another state of the industry uh update so someone on the discord asked about um the control problem subreddit and i said that i don't really pay attention to here because i'm not impressed by the conversations but it is the state of the industry so let's go through it so i picked some of the top things from the last month this is is probably the best one it's on less wrong um and it was published about two weeks ago um and he's got this great table where he's just he breaks down like everyone who's working on uh this on the pro on the on the control problem so what is the control problem ai alignment um and nobody can even agree on what what it is because right now there's like one camp is just make the ai do what humans want because they kind of assume that ai will will always just be a tool it will always just be an extension of humans that's a real bold assumption i don't know that that's true and then there's other folks that are just kind of debating about like inner alignment versus outer alignment um and they're kind of picking it all apart and that's not and i i don't want to like devalue what and what these individuals are doing um but i i think that they're not quite seeing the forest for the trees yet um and so i had i had a i had a friend um a researcher in computer science kind of explain it to me so from the academic perspective what you do is you look at where the current conversation is in the academic world you go to that conversation and you add just a little bit to it if you want to introduce something like completely new like a brand new architecture or brand new theory like um that's that's pretty hard to do you have to have a lot of credibility and in the meantime private industry is surging ahead right we have microsoft and google and meta and and all these other companies that are like they are participating in the scientific process because they're still posting um research on like archive and stuff but they're like there's they're completely disjointed right because you've got the academic thing where they're doing like little toy toy level experiments and then you've got private industry actually producing new models actually producing um new cognitive architectures and stuff and so there's this huge disconnect and so i am particularly skeptical about anything that is purely academic like if you see like uc berkeley and they're the only authors on the paper like it's it's much it's very unlikely that um that if it's that if it's a single publishing author from from a strictly academic environment that it's going to be any good because when you think you know agi research you think deep mind which is private industry you think meta which is facebook you think google you think open ai you think microsoft you don't think stanford you don't think you don't even think mit and this is what is so disturbing to me is how far behind um academic uh you know american academia has fallen um when it comes to these things so anyways this article is pretty it's pretty good in terms of just kind of getting a survey of of what's going on um you know so like sam bowman and outer alignment it's an academic lab um you go down and take a look at it you know runs a lab at nyu you know he is on sabbatical working at anthropic right so like an open source consortium um inverse scaling prize creating data sets for evaluating language models as well as inductive biases and so right now like just implied in this is just understanding if there's bias in an llm and not thinking about how to use an llm to engage with the real world it's very strange but again implicit here is the idea that the ai and large language models are just tools they're just components of things that humans will ultimately be in control of whereas my approach is is fundamentally different where my approach is let's assume that the machine is going to become autonomous how do we build an autonomous machine that can think on its own and and and do moral reasoning now i was very happy to see artificial moral cognition from deepmind 2022 but all they do is propose a framework and this framework is even more primitive than lawrence kohlberg's stages of moral development because that's basically all it is you look at the actual paper and they're talking about they they talk about um you know moral development in in in humans um and they kind of list you know they list four uh milestones whereas lawrence kohlberg taught has six that goes up in through adulthood um and i'm like this research is 50 years old actually it's probably older than that how old is lawrence kolberg's research 1958 so this this research is let me do some mental math 50 uh 65 years old um almost so like why is this why is the academic research oh this is deep mind actually why is it so far behind like this is a solved problem we understand human moral development why are we reproducing it and doing these very very low value add things when we should be doing experiments with moral development and moral cognition in llms but do you see any any experiments with input and output of llms or fine-tuning data sets no it's not happening right now and so like when i get really salty and i'm like nobody is actually doing any research this is what i mean and if if you're like oh well dave what have you done like check out my book on um on benevolent by design and symphony of thought i actually even have a fine-tuning data set out there called core objective functions um so yeah uh oh yeah so anyways kind of getting back to the list of things here's the open ai blog about our approach to alignment research now in one in one capacity they do explicitly say like we are on the path to agi and then unaligned agi could pose substantial risk to humanity yeah like that's good like okay just say it out loud say it directly and let's move on um so but then like so their current goal is to is to automate the the alignment research which i think like okay that's that's one approach i don't necessarily think that that's the best approach um but the goal is still to create an autonomous machine okay so they're they're aiming for autonomy so that's good um you know but they don't have any statement about like let's let's create an autonomous machine let's imagine the future where we assume that machines are going to be smarter than us and that they're going to be autonomous how do we how do we address that um and so they're talking about like you know human feedback and training models to us to assist and so what they talk about here is like um you know like okay you're talking about summarizing books what does that have to do with alignment right because if you're not thinking about the problem structurally correctly then your research might not matter and that's that's what i mean is like the you know the the the nightmare scenario scenario that everyone is afraid of is that ai becomes autonomous it becomes sentient and then it decides to kill us um and yeah like okay um you know this this one is probably the most relevant we trained a model to write critical comments on its own outputs i did that too it's not that big a deal but the idea is that um it can think right and so they're getting closer to the idea of a cognitive architecture and if you read my book symphony of thought i talk very extensively about these feedback loops of where you have one model produce an idea and another model um either comment on it or just uh discriminate or discern against it um so it's like okay they're moving in the right direction um you know but then and they're also very uh they're they're they're very uh cognizant of their own limitations which i really appreciated this this has this shows some really good intellectual integrity um so like the path laid out here under emphasizes the importance of robustness and interpret interpretability research um so like yeah this is this is 100 um my focus uh let's see using ai assistance for evaluation has the potential to scale up or amplify even subtle inconsistencies right so this is this is something that anyone who does automation will tell you is that if you if you rely on feedback loops in automation um you will get unintended consequences unless you have self-correcting mechanisms in each iteration but if you don't have those self-correcting mechanisms you will it's like a fractal pattern it'll just spin out bigger and bigger and bigger here we go i really appreciated this one aligning agi likely involves solving very different problems in aligning today's ai systems yes um we expect the transition to be somewhat continuous but if there are major discontinuities or paradigm shifts so this is exactly what i'm talking about thinking about autonomous machines and agi requires a major paradigm shift it's not just a model agi is a system just like human intelligence is a system there are models in it many a plural many but it's not going to be one so then most lessons learned from aligning models like instruct instruct gpt might not be directly useful yes i'm glad they said it out loud the hardest parts of the alignment problem might not be related to engineering at a scalable and aligned training signal for our ai systems even if this is true such a training signal will be necessary so what they what they mean here is like okay what is it that we have to respond to if we want safe agi what is what is the signal to know whether or not it's messing up and i write extensively about that both in benevolent by design and symphony of thought and the the short answer is there's never going to be just one signal um in fact in any condition where you use a single signal a single loss function you're going to end up with unintended consequences and part of part of the solution that i propose is that you have a dynamic tension between several signals or several goals let's see it might not be fun it might not be fundamentally easier to align models that can meaningfully accelerate research um alignment research than it is to align agi in other words the least capable models that can help with alignment research might already be too dangerous if not properly aligned if this is true we won't get much help from our own systems for solving alignment problems so basically this is saying if the model can't self-correct then what do we do so i to me open ai's uh very frank blog post here is is the most self-aware um and kind of salient take on alignment but even still most people are not talking about um we're not talking about um like let's let's assume that there's going to be an autonomous machine or talk about the possibility that there's going to be autonomous machines um why is this michael oh it's not it's not the other michael cohen um so bostrom russell and others have argued that advanced ai poses a threat to humanity so let's see what assumptions they're talking about so abstract we analyze the expected behavior of an advanced artificial agent with a learned goal learned goal planning in an unknown environment given a few assumptions we argue that we'll encounter fundamental ambiguity about the data in the data about its goal for example if we provide a large reward to indicate that something about the world is satisfactory to us it may hypothesize that that what satisfied us was the sending of the reward itself no observation can refute that oh interesting um so basically it'll say like okay we humans just like sending the reward um then we argue that this ambiguity will lead it to intervene in whatever protocol we set up to provide data for the agent about its goal we discuss an analogous failure mode of approximate solutions okay so basically this says if the reward struck comes strictly from uh from the outside world it will probably just try and game the system so i talk about this quite extensively in benevolent by design where all rewards have to be internal so basically the machine has to decide to reward itself or hold itself accountable which is where um you know openai's idea of um let's see where was it of training the machine to critique itself so this is the thing is that this is how humans fundamentally work all signals in our brains originate from within our brains now that being said there are signals that we get from our body so for instance if you eat a delicious meal your body creates hormones and chemical reactions that reward you but the the fact of the matter is is that we have to connect that you know the external behavior to an internal sensation an internal reward um and then also we monitor ourselves for for self-correction and error detection so say for instance you say something and someone starts to cry you hurt them and what we have is we have neural mechanisms that that actually give us a sense of pain or shame or some some unpleasant sensation to correct our own behavior um so in that in that respect like you know okay great like why did it take a whole article um you know to to to prove this this is basic biology this is basic neuroscience um and it also takes only just a couple of uh of of llm prompts to demonstrate this so this is this is why i'm like really frustrated with with the pace of things and also the approach um let's see what was this one red teaming language models to reduce harms method scaling behaviors and lessons learned let's see we describe our early efforts to red team language models in order to simultaneously discover measure and attempt to reduce their potentially harmful outputs okay so this is basically just is an llm um producing harmful output um okay i don't care that's that's not alignment research that is that is that is a fine tuning project um let's see okay the alignment problem from a deep learning perspective within the coming decades artificial artificial general intelligence may surpass human capabilities at a wide range of important tasks okay good stating something um just right on the nose good good this report makes the case for why without substantial action to prevent it agis will likely use their intelligence to pursue goals which are very undesirable in other words misaligned so this is the thing that word alignment and misaligned oh my god like just by just by looking at the survey of of articles here people use alignment to mean is it is it gonna is it gonna spit out racism right is it going to um is it gonna follow the instructions that we want it to or is it gonna destroy all of humanity that when when a term is so broadly applicable and ambiguous it is a bad term um so probably what they should start to do is use a term like outer alignment or control problem because what they're talking about here is the control problem not alignment this is you know this is outer alignment um or or the control problem so okay i'm being ultra salty about this this is why i did my research and wrote books because i looked at the landscape and i said why why are people missing the biggest part of the conversation okay with potentially catastrophic consequences the report aims to cover key arguments motivating concern about the alignment problem in a way that's as succinct concrete and technically grounded as possible i argue that realistic training processes plausibly lead to the development of misaligned goals in agi in particular because neural networks train via reinforcement learning will learn to plan towards achieving a range of goals gain more reward by deceptively pursuing misaligned goals okay so this is again if you have a reward function whatever that reinforcement function is it's going to game the system it's basically saying the same thing i explained my claims with reference to an illustrative agi training process and then outline possible research directions so again this is all speculation this is not actual work um then that so that's that's my other criticism is that 99 of what people are doing is just speculation at this point it's proposals it's speculation and then where the rubber meets the road what they're actually doing is very basic fine-tuning projects or llm prompt engineering and no one's thinking about intelligence as a system where you have like what component inside of the system is going to is going to do this now okay i need to stop myself because i'm ultra salty um yeah so let me do a time check we're at 18 minutes this is this has been long enough for state of the industry uh uh uh video okay so state of the industry and this is where it's been for about the past year um some of these some of these experiments that people are talking about um are very similar to stuff that people have been doing for literally decades where they're doing like what does the agent model believe or what is the moral reasoning here and they're trying to break it down into very simple math problems which like okay that's a very interesting toy um but it's not something that is actually implementable um and so that's that's what i that's why in symphony of thought i lay out something um you know four four criteria um it has to be uh it has to be robust it has to be interpretable um it has to be um wow why is my y's implementable right um those are three of the things oh universal universal uh implementable uh interpretable and robust um so if it doesn't have those things it's not really worth exploring um because we are at a point where people are deploying large language models in chat bots in robots it is being deployed right now so if your research is not something that is implementable um i got bad news for you you're falling further and further behind the curve um so it has to be robust so you know um several of these articles talked about robustness which is great which means it has to be self-correcting um it has to be universal right because we are we're quickly getting to um to a point where this is the this is going to proliferate right you know once it reminds me of nuclear proliferation which was once the scientific community and the militaries around the world figured out a few problems about making radioactive isotopes and refining them we had nuclear proliferation where we went from a you know a couple dozen nuclear weapons are on the globe to like over a hundred thousand in the space of a decade and so what we're going to see is we're going to see an ai proliferation especially as language models get more more open source and more efficient and so your research really needs to be focused on well here let me just bring it up symphony of thought um let's see we'll bring up the pdf version and yeah so ch ch where did i have it in here ethical paradigms universals okay it'll be down here somewhere more pages um anyways technological trends yep so this this is this is where i make the argument like look this stuff is growing exponentially we need to get a move on um my book is less salty than this video i promise i talk about why model ethics and morality and then i talk about how complex it is okay so here's here's the four conditions for what i call good alignment research or control problem research one universally applicable whatever framework it is it has to be universal now the thing is is that morality and ethics and alignment are so complex that you cannot just choose one framework so the conclusion of this book is you have to you can't just create a moral framework you have to have a thinking machine that can that can think about all moral frameworks um it has to be flexible and adaptable uh so i kept saying interpretable i apologize flexible and adaptable so for it to be universal it has to learn and adapt over time so again talking about it has to be a thinking machine it has to be a learning machine it has to be eternally robust so robustness means that it has to be resilient not only against its own drift but it has to be resilient against external things because you don't want an edge case where an agis operational parameters change because something in the world changes so for instance um you know humans will become vicious if we are scared right or if we're in a um if we're in a war people will do unspeakably horrible things just to you know just to survive but you don't want an agi for its for its um purpose to break just because the conditions change and then finally it has to be implementable if your research is not implementable like i got real bad news for you like you're falling behind so those are the four four conditions for what i call like good like agi research right now um if you just if you're just speculating about how agi will work um and not testing something which again this is this is why i really approved of open ai because one of the things they say is like we are going to be testing things like we're actually going to be testing um let's see what was implement i don't maybe it wasn't on this blog anyways but they did say like very explicitly like we're going to test stuff we're not just going to you know hypothesize um yeah so this is i i said the video was over like five minutes ago okay anyways so this is the state of the industry on alignment outer alignment and control problem um the short version is i'm pretty salty about it but there's a few people that are starting to move in the right direction so thanks for watching