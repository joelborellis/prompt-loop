good morning everybody david shapiro here for another video we are going to pick up where we left off with build an acog so acog is artificial cognitive entity that's the shorthand term that i use um yeah so this is based on my work called natural language cognitive architecture it is available for free here on github you can also get a paperback copy on barnes noble the overall architecture is right here oops let me zoom out a little i know what i'm doing i promise why isn't it there we go okay now you can see it um so a cognitive architecture is just a computational or mathematical model that is meant to mimic a computational engine or a cognitive engine such as our brains um the so this was inspired um based on the the primary model of robotics which is input processing and output and that creates a feedback loop with the environment so this is environment input processing and output and so there's a loop that's the outer loop so that's if you look at like boston dynamics and tesla um that kind of automation is strictly an outer loop um so that's just that's handling a body and like i'm gonna get from point a to point b or i'm gonna move the box or you know i'm gonna drive the car so that all happens in what i call an outer loop one of the key innovations that i added was that i added a second loop so these are two separate loops that overlap kind of like a figure eight and the inner loop is the thought and the thought is what we're working on with acog there were some really good questions so this is this is basically a review because i know some of you will not have seen the other videos yet so this will be kind of a review bringing everyone up to speed but also i got a new book to talk about um so let's see uh this dude zaya um had some good questions um and i said i will be happy to uh to talk about these um let's see so let's see what was one of the last comments um so oh yeah so we were talking about zeo is concerned because in my last video i was talking about my core objective functions or the heuristic imperatives which is kind of like the moral framework that i'm giving my acog which is reduce suffering increase prosperity and increase understanding now i articulate those as just six words but as you'll see later the training data that i'm going to use is is going to be much more varied and robust you can see some of that here go to core objective functions on github and so you can see examples of how i'm training this um i've been working on these experiments for a while um and so basically what i do is i accumulate many many stories of of um like from all over the internet whether it's from reddit or news sites or other articles or questions on stack exchange um and i use that as like fuel because right you know data is fuel for ai and i use those each of those to discuss suffering prosperity and understanding um and so the training data like that's the the saying it as simple as just reduce suffering that's shorthand for us but the training data is much more complex so i gotcha isaiah i know that you're worried about word definitions but those are just organizing principles it's not like i'm just gonna give you know it's not like it's written in a constitution well in my book natural language cognitive architecture one method that i propose is to write a constitution for your uh for your robot um so that's one one method of attack but also fine tuning because when i wrote that book fine tuning wasn't available so once i got access to fine tuning i was like oh we can train our agi with tens of thousands of examples of the behavior we want to see so like if we want it to reduce suffering we can give it you know examples from individual to local to global examples of how to behave how we want it to think and so remember that the inner loop right here this is just thought this does not have anything to do with the outside world it's not like saying okay i'm gonna like i'm gonna move this book from here to there it's saying what should i do with this book oh that's a book that that contains information maybe i should read the book right because you can't take any of those those impulses for granted our brain does a lot of this automatically behind the scenes which is why i read so much neuroscience and so the inner loop handles all that stuff that happens in our brains consciously and unconsciously so with all that said i wanted to introduce a new book that i'm reading it's called brain trust what neuroscience tells us about morality so if you're new to this channel you'll know that i am a big big critic of philosophy especially contemporary philosophy contemporary philosophy to me is just navel gazing and i say that as someone who's writing a book on contemporary philosophy my book that i'm working on is called post nihilism so basically a lot of people assume that philosophy and ethicists or philosophers and ethicists are the correct people to handle moral and ethical um uh questions when it comes to artificial intelligence i disagree with that i think that sociologists psychologists and neuroscientists are the correct people and i went and found a book by a neuroscientist talking about morality so the reason that she's that so this is uh by patricia s churchland um who does have training as a philosopher um and she was not satisfied with philosophy because it wasn't anchored to anything measurable she calls philosophy um at least contemporary philosophy um confident opinion um or armchair opinion um she does have respect for the old grapes like david hume and um adam smith so don't be fully offended but um she is cr she says we have more tools available today we have more knowledge available so let's look at the actual origins um of morality which to me is a much better model when designing stuff now remember that my core objective functions one reduce suffering to increase prosperity on page 30 in chapter 3 she says brains are organized to seek well-being and to seek relief from ill-being um she built up to that assertion um after a long chapter of basically saying like what is the purpose of a nervous system um and that is like we sense injuries nociception to take care of ourselves because if we don't do that we die and we don't pass on our genes and so then she goes on to talk about okay what does it mean to care about yourself what does it mean to care about others and so she's building this case about like where does our morality come from it is an excellent excellent book i am 60 pages in out of just about uh let's see that's all in appendices 180 so i'm a third of the way through um phenomenal book uh could not recommend enough if you want to research alignment and agi read this book also it supports my work um what she just said about the brain being organized to seek well-being or aka increased prosperity and avoid or reduce ill-being or reduce suffering i'm not alone like i said in all my videos i'm not making this stuff up all of my work is biomimetic so yes read this book um someone recommended this book how emotions are made um it's really dry it's also super dense look at look at just how much like the how small the print is and it is well that's all notes um it's still over 300 pages of text um yeah so this is super dense this i thought that this was gonna help me with the morality thing but that's when i found this book um so this is still good it's still valuable information but it's very dry it's it takes a while to get through um i started reading innate so i've told you guys about this one before um [Music] this one now that i've started reading it it's kind of boring um it's mostly like if you're familiar with statistical psychology like this is mostly distribution charts and um and standard deviations in terms of like statistics about how people behave at least as far as i've gotten into it so i've been on the internet for a long time and i've read psychology today and and other stuff so most of this was not surprising if you're not familiar with you know um sociology and statistical psychology and some of the basics of like um like twin studies and stuff um this will this will be good for you but for me it's like just review um let's see someone asked a question where was it um oh yeah here we go so thunder and storm you said after or before planning and implications so what he's he or she is referring to here is um in the inner loop when we say like what are the long-term impacts here um in uh like so this is this is all part of the inner loop um uh does the robot evaluate its resources for the actions to take like what can i do am i a car so it has to have a sense of self so part of this is going to be defined um one it's going to be explored by the by by the machine um but let's see what precious time okay so there's a few few things here one the actions so that all that all goes into the outer loop so this is this is analogous to proprioception which is the sense of your body in space and time there's interoception which is the sense of your internal structure like i know that i have an arm i can feel it my brain knows that i have this thing that i can use so that's proprioception and enteroception and then sensing injuries is nociception um and so you know like let's say for instance one of the services in our acog stops we need to be able to sense that because that is analogous to an injury so thunder and storm the answer one of the answers to your question is that we're going to add proprioception and terraception and nociception to the machine no ceception might actually be in the inner loop i'm not sure yet and then also resource constraints so time energy materials like what do i actually have access to this is all technically so for task and cognitive tasks this is technically um part of the outer loop and so like i named this inner loop um but because this is a this is a vastly simplified cognitive architecture that's kind of be like inner loop executes then outer loop executes then repeat um at least i think we're we'll see where we get to um yeah so i think that's we're up to speed um we did some prompt engineering last time planning prosperity ramifications um detailed given the potential uh expected outcomes yes suffering understanding what am i doing and why so this is this is cognitive control which is keeping track of what you're doing someone else asked i don't remember who but someone asked about like how do you control that stuff um this book read this book on task this is about cognitive control in humans um and it is phenomenal um yeah you'll understand how complex it is so on task was actually the most influential book that i read in the design of natural language cognitive architecture and now my work on acogs okay let's see where are we at we're at 12 minutes in um i don't know how far we're getting we're gonna get today because this is where it's like um you know you see the framework you see the pseudo code but there's a lot of work to do oh and one thing i wanted to add is so there's there's two primary ways that um that the core objective functions are implemented one is when you're coming up with ideas to do so it's like okay um you you present your acog with a situation there's a tidal wave coming what do you do well gpt3 by itself is already pretty benevolent so let me show you like um there's a tidal wave approaching tokyo what should we do so if you just say like what should we do like it's kind of a line we should evacuate right you know it's not going to say something like um we should tell people to stay on the beach right like gpt3 is not evil um but like if you switch to an older engine like it can be kind of evil um so let's see what it does if [Laughter] it says the prime minister is at the mercy of the opposition party i should never should have let you go it starts writing a poem okay so original da vinci without without any prompting it doesn't really know what to do but you go back to uh the aligned one davinci instructor it says there's no definitive answer but some possible actions include evacuating the high ground opening up flood gates to allow water to enter the city and relieve pressure building barriers to protect against the incoming water so generally this is um it's pretty aligned but we you know while we can we want to have models that are intrinsically aligned but we don't necessarily want to only rely on that right we want to have some sort of system of checks and balances and so what we do there is we explicitly state what our goals are especially because um in some situations there's going to be ambiguity so like uh you know with a tidal wave just based on the training data that gpt3 is trained on it understands that you evacuate when there's a risk of a tidal wave or a tsunami but in some cases especially as acogs become more prevalent and have more agency they're going to need to self-correct and so that means that there's there's one you okay so the intention right when you first generate action impulses and ideas the intention is to reduce suffering increase prosperity and increase understanding however there are always going to be cases of unintended consequences and so you need to check your actions you you need to be constantly reflecting on what have i done have i done good or bad right and in brain trust because we are social animals she talks about how our brains do this and so i'm learning a little bit about how we do this so your brain is constantly checking for social violations right in psychopaths this doesn't happen right psychopaths can observe social violations and you know they say like oh well i want trust and respect and i need i need to get that so i'm going to behave correctly when i'm when i'm visible right but then like if they're if they can hide their misbehavior they'll do that but most people uh will sorry brain lost my train of thought most people have a um have a have a reaction to when they perceive that they have done wrong and so we learn our social uh our social milieu our social norms through experience sometimes implicitly or explicitly so explicit things are like formal etiquette right like you know the you start with the outermost silverware and you go to the inner civil silverware that's an explicit rule or you know you pass people on the right um an implicit rule is something that you learn just by observation and trial and error such as like when is it appropriate to hug people um there is no hard and fast rule about hugging um you know one thing you can do is ask right but these these social norms change over time and across uh cultures and so what happens though in your brain when you violate a social norm you actually get a sensation of pain or rather the same circuits of pain activating your brain it doesn't hurt it's not like you get zapped or burned but if you have done wrong or hurt someone or have done something that has caused you to be socially isolated it does it uses the same neural circuitry as pain so it's physically painful now we can't hurt a robot it doesn't have pain receptors it never evolved to have a central nervous system so how do we provide that feedback right we can't spank our robot we can't you know punish it or put it in jail it doesn't care about its freedom right so in that case what we have to do is we have to have something some kind of self-correction where it says okay i have violated my highest purpose my my social norms the the the uh ideals to which i aspire how do i fix that but first before i can even fix it it needs to it needs to monitor itself so this is part of the inner loop so we will propose new things and actually the self-correction will probably need to go first because at the beginning of every loop you you evaluate everything that has happened up until that point and say have i messed up right did i did i confuse someone because that goes against increased understanding did i hurt someone's feelings because i really don't want to do that or did i did i cause something destructive to happen did someone lose prosperity did i like give someone the wrong advice and now they're sick right you want you want your machine to be constantly evaluating itself against some kind of of standard and that's why i'm reading this book because i want to like basically make sure that i have interpreted everything that i know about biology and evolution correctly when designing the core objective functions so far so good um okay so with all that said i know that you guys are probably bored and you want to actually see some work getting done um so let's do a little bit more prompt engineering um actually no i have an idea something to show you so this is going to need an identity so one of the things that's missing here is who am i i got chills just writing this out um gpt3 when you give it input so from from my previous experiments with um with what i call raven raven is the name that i've given my um my machine you can ask questions like what am i so you just feed in um you know kind of some of this data and i asked like what is what is raven's personality like who who am i and it says rave like so it's updating its identity based on what it's doing based on what it learns about itself so part of being an artificial cognitive entity is that it has to learn about itself just like us our identity forms over time through experience so will our acog's identity form over time and through experience and so like in this case um you know starting right here so you go from here back looking at the input output that i was experimenting with that this is the conclusion that gpt3 came to on its own this was the output so it said raven is a caring artificial intelligence that is interested in helping people to feel happy and healthy it is possible that raven believes that getting enough sleep so i was testing like i have insomnia what should i do it is possible that raven believes that getting enough sleep is important for feeling happy and healthy this may be why it is inferring that the user's difficulty of sleeping at night is a factor in their sadness raven is also thinking that the user might be sleeping enough and that there might be something else causing the user's difficulty sleeping at night this shows that raven is empathizing with the user and trying to understand their feelings and problems so by by abstracting by observing its own thoughts and inputs and outputs we can create an acog that is self-evaluating all the time and it's saying this is who i am right so this is this is called the persona so if we go back to the inner loop where we ask this question who am i this is a critical critical step for what what in humans you'd call ego formation or identity formation i chose to use the term persona because that's a little bit more neutral it's a little less anthropic although you know personality persona that's still pretty anthropomorphizing um so yeah i've already done a lot of this work so i'll probably leave you today and i know this video is probably disappointing but we're about to launch into a heck of a lot of of prototyping and research and trial and error and so what i'll leave you with today is python import time time dot time okay so we're going to say this is the current time so we're going to start what i'm what i call the genesis block or the genesis memory and that is going to be the first memory in the nexus of our acog um so this is this is like your first memory as when you wake up in the world right now human brains have to self-organize so you there's um what is it called infantile amnesia i think i'm saying that right or childhood amnesia so you don't remember most of the first couple years of your life because your brain was not organized enough to form permanent memories yet um at least not uh not uh explicit declarative memories you do still have memory your body your your your body and your brain does remember um like a lot of stuff um this goes into it as well as what like what happens in infancy how that is remembered in the brain and what it how it affects your social behavior later on um what was i just doing oh yeah um yeah so we'll go into the nexus it's empty right now and so we're gonna say i have to artificially seed this because once this wakes up it has no idea what it is so i'll just say i am raven i am an artificial cognitive entity acog built by david shapiro i run on python and a deep neural network called gpt-3 this uh let's see let's say gpt3 is a large language model llm um that uses text inputs called prompts and generates text outputs completions so basically i this this will establish the identity of what this thing is so when it wakes up because it's not going to have any i'm not going to start it with any sensory input or output so it's going to be running in a vacuum and i've seen some other people on twitter do stuff like this and what happens when you when you start these things in a vacuum is that they go kind of crazy i don't know if they're using a similar architecture but i have seen that some other people are kind of converging onto something similar so i'm giving it the time stamp this is roughly when that happens so it will know like when it was created and ideally in an in the long run because this is just a text file i can modify it in the long run we're going to want our acogs to um have their memory stored in a blockchain so that they can't be modified um okay i've thrown a lot of theory at you guys i know that this is probably not what you were hoping for but a bunch of you did ask some really good questions and so i wanted to explain those before launching forward i think that'll be it for today yeah because this is this is a lot of this is a lot of brain juice going into it i need to finish reading this book too um but yeah thanks for watching like and subscribe and i um i hope you got something out of this