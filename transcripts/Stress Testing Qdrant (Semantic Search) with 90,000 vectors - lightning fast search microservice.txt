morning everybody David Shapiro here for a video today we are going to stress test quadrant so if you're not familiar with quadrant go check it out quadrant.tech that I also have a demo video that I posted just a couple days ago um so without further Ado we'll get into it so here's what I'm doing um so under my quadrant demo uh repo which uh Link in the comments um basically what I'm doing is I've got a folder with 55 000 um uh uh uh different samples of data it's 55 or yeah 55 megabytes so they range in size from one kilobyte to 33 kilobytes each and so what I'm doing right now is I'm going to split them all up into chunks and then I'm also going to use um I'm going to use Universal sentence encoder to generate embeddings for them um so so here's here's how it's going to go um and I'll just go through the whole thing um one one bit at a time so we'll do that and just print files and then we'll go to quadrant demo python prepare data uh oh yeah okay could not load so basically this error that error that it showed just before it barfed out all the files um come here is basically saying you don't have Cuda because I'm not using I'm not using tensorflow Hub with GPU acceleration with uh with this um with this model Universal sentence encoder large five I found that you kind of don't need it it's a hundred times faster than Universal sentence encoder 4. it's basically like sub millisecond per sample so it's not necessarily needed um okay so we'll do that so for file and file so we're just going to iterate through every single file um we'll do text equals open file um oh yeah we're not going to save this out to logs we're going to save this out to um this is just be we'll call it data um encoding yes sort Keys true indent one that's all fine um okay so open file and the file path is contexts dollar and then we'll do file okay so basically that just sends back the content of one of these so here's an example and it's like oh wow that's a lot of text so then what we're going to do let me close some of these Superfluous things so I'm not distracted okay zoom in a little bit so you can see it better okay so text equals open file context so here's an example context and so then we'll do chunk equals text wrap dot rap um and then we'll do text and then a thousand so we'll basically split this into into the number of um into a thousand character chunks so as an example um let me just let's see CLS just show you how this works it'll do CD context oh whoops contexts plural contexts that's difficult to say at least with my uh particular vocabulary of phonemes contexts okay um all right so we'll do python import uh what do I need uh text wrap and then we'll do um with open um Reddit one one eight six eight dot text uh read encoding equals UTF eight um uh um as in file um let's see text equals infile dot read text okay so we got all the text all right so then we'll say chunks equals uh text wrap dot wrap um text a thousand and so then we'll do length of chunks so there's 31 chunks in this so then we'll do chunks zero and so you see basically each chunk is now oh here I'll just do print all of um we'll do um from P print import and then it'll do pre-print chunks Okay so chunks is now a list of chunks or a list of texts so you see how it's got this little bracket here that means that it is a list um and it's it's a little bit difficult to see but you see that one one instance ends here and then the next one um proceeds so basically we're gonna we're gonna split this up into chunks of a thousand characters so that's what that does so um break break the file contents up in into chunks of a thousand characters okay and so then we will take um actually I need to open Nexus again because I don't remember things off the top of my head I figured out once and then I copy my code because I'm lazy um all right so embed where else is this used yep so embeddings equals all right so then we just okay that's that simple all right so let's see embeddings equals embed and then we'll do um chunks so this should actually be chunks plural I think I think that always uh so let's see text 2 equals bacon um text wrap chunks equals text wrap dot wrap um text two I guess I could have just put in the the thing all right thousand chunks okay so text wrap always returns a list even if it's shorter okay so that's good so we can count on this as always a list um always a list and then we'll get a whole bunch of embeddings so irrespective of how long the file is it'll give us a list of of uh text of strings and that's exactly how you use the embedding service all right so then we have a list of text and a list of embeddings what do we do with it then um I'm gonna have to look up how you do the zip thing where you you put things together um because actually no hang on let me pause this for a second because I need to gather my thoughts I didn't think this far ahead okay I didn't want you to have to watch me fussing around with this but I think I've got a simple thing so um we're gonna get a list of embeddings which are numpy objects I believe or it might actually be something else but so we take the those embeddings convert them to numpy convert them to lists so now we've got a list of vectors that is matched to the chunks and so then for I and list of range 0 to the length of the chunks um so we'll just iterate through so rather than zip them together I'll just do it in one step and then we'll save the data by passing a string and an embedding back and then I'll just do one file so let's see print file um yeah and we'll go from there uh just for some debug output so we'll do print um chunks I uh vectors I and let's see what happens it'll probably blow up because it usually does okay uh CD go up a level CLS python prepare data of course um oh right invalid syntax that's supposed to be a comma model does not exist yep if you've watched my videos you've seen that this happens more often than not um really got to figure out a way to fix this and so then you just go up a level you delete the folder um and try it again ignore above Cuda RT dll error if you do not have a GPU set up on your machine so depending on how long this takes it might be worthwhile to um to get uh to get it installed with um tensorflow GPU um if if it is too slow um I'll probably pause the video and do that um yep AVX and avx2 hey I just did a video yesterday about Hardware level support um no such oh whoops I need to create the um data folder man so scatterbrained logs why did I oh yeah because this was my this was my first um demo so I'll leave that there okay third time's a charm so what it's doing right now is it's loading um everything up dialogue one text okay so that should have created this folder so we've got embedding and then the string Bianca did you change your hair Chastity no Bianca you might want to think about it okay so that was uh Cornell uh that was a movie dialogue um and so then here is a 512 vector or Dimension Vector detailing what is in that so one advantage I looked it up because if you look at a table of Bert embeddings they have limited window sizes Universal sentence encoder does not you can have an arbitrary length of of um of embeddings or of characters or tokens because the universal sentence encoder will just keep reading it until it gets to the end and then it will spit out whatever its internal representation of now the longer it is the more diluted it gets so that means like the longer it is the more likely all the all the values are to be like probably like 0.5 or something or maybe just zero because it looks like this this Vector is from negative one to positive one so if you have like if you haven't read all of the internet the vector will probably just be zero zero zero zero all the way down okay so I'm going to pause it again and I'm gonna get um I'm gonna get uh tensorflow GPU uh setup I'll probably do that in Anaconda um and then we'll come back if you want a video of showing how to set up tensorflow Hub with GPU I can do that separately just let me know in the comments all right pausing now let's get this figured out so that it's faster and then I'll be right back actually you know what heck with it I'll just show you what I'm doing um okay so if you open Anaconda Navigator um it's the desktop GUI you go to environments I do everything in bass just because like what I do is so simple now if you have a lot of different experiments that need a bunch of different um environments you definitely want to use different um uh different different um what's it's a VNV so if you ever see VN v e and V it's a virtual environment it's a python virtual environment which means it is a self-contained python environment um but since what I do is so simple I can usually get away with doing it all in bass this did not used to be the case especially a couple years ago you needed all kinds of very very specific versions of everything um I'm wondering why it's taking so long oh do I need to connect um anyways I also spotted that I need tensorflow Hub in this one uh so anyways um so we're basically what we're doing is I just search for tensorflow and I'll put tensorflow GPU and tensorflow hub in and I'll let it basically that what this does is it looks at your environment and it looks at what you're trying to deploy and it figures out all the dependencies for you this is easier to do today than it used to be especially with getting all the correct Cuda packages in place much much easier than it used to be when I picked up the book for Karis deep learning with python by um Francois Chalet the sholay I don't know if I'm saying it right anyways that was back in like 2017 or 2018 it was a pain in the neck to get tensorflow set up tensorflow GPU set up on Windows um but it's much easier today anyways I'll let this finish solving and I'll I'll resume the video once this is done doing its calculations in the background um so anyways be right back okay I think I figured out the condo thing um I installed it as the um like for all users and so I needed to run well first I probably needed to run the um conda Navigator Anaconda Navigator as admin but I did The Prompt as admin um and so then uh what I'll do is I'll just try and install it directly via command line because the uh oh and I did conda update um that's what I just did here let me make this a little bit bigger so you can see it properties font uh 28 there we go okay so then we'll just do conda install Dash C Anaconda tensorflow GPU hopefully this will work so it's running as admin and conda has been updated fingers crossed um and if it doesn't work I'll pause the video again and just figure it out um because I'm sure you're probably bored of this by now but it's running as admin so come on now oh and it's been updated so solving environment all right I'll pause this like you don't need to watch the spin so I'll just pause it and show you the results okay I figured it out it's installing now you look down here uh first I had to update con Anaconda Navigator so I did you know update all that stuff so I'm on Navigator 2.3 and it's running in admin mode so it can install in the environment directly so it's installing now oh my God sorry I'll go ahead and pause it and this will be done in just a second oh and while that's finishing though I'll just show you that um I am doing a little test so we can do a side-by-side test and look at the average time per file so I'm just accumulating here's the start times dot append so I'm just timing how long it takes to do some to do each each file and so then we can do a side by side of um of how long it takes to do it actually I'll do a overall time as well uh yeah I'll do that so do do because this is stress testing right this is performance testing all right moment of truth it's all done um I got the I got the tensorflow GPU environment set up oh and so if you're not familiar with Anaconda to run a particular one you just click the play button and choose you know whether you want to use python or terminal um let me close this this is extraneous so basically what this does is it launches it in that virtual environment which you see right here um so that says okay use this version of python to do everything that you're about to do and it should be able to launch uh no 10 okay looks like I need more stuff uh okay final pause again finally got it running I am so sorry I might edit this video to cut out some of the uh some of the garbage okay so it's running um you see the average time per file is going down it's .07 seconds per um per uh per file so that's that's pretty fast um I had to go in and add a Keras GPU um and then it worked finally um so it's using my GPU you see uh let's see GPU memory oh wow if it filled it up it used it used all available memory um and it's using about 22 of my gpus resources running at 46 C so that's not too warm um let's see 46c I think that's about 120 degrees maybe 130 degrees Fahrenheit um but yeah so it's running nice and nice and smooth um if we come over here to data we see we've already got um 1300 out of 55 000 so it's going up pretty fast uh yeah and the average time is continuing to go down so that's not too bad all right so I'll let this run let this finish um actually first let me show you what it I think I already showed you one but they're all going to be roughly the same uh size because it's going to be a chunk of no more than a thousand um a thousand characters followed by the uh as well as the um the vector so basically what I'm doing is I'm preparing a whole bunch of data um for to stuff into quadrant so that I can stress test it so it'll be it'll be 55 000 um original files but it'll actually end up being more um total files so each of those is going to end up being a data point all right so I'm gonna pause the video again let this run we'll see how long it takes total okay we have oh let me turn my light back on there we go uh we have success it took 3 500 seconds so just shy of an hour but the average time was .063 seconds per file so that's not bad um grand total it generated um let's see uh 88 000 files at 1.1 gigabytes so we have um we have just shy of 90 000 data points to add to this um so now what I'm going to do is take the quadrant stress test and I'm going to update this I'm not going to make you watch me just do some basic programming um so basically what I'm going to do is upload uh upload this all at once and and and while it's uploading we will watch the container performance so the volume will be listed here um and then here's the image so I got the quadrant image and then the container I deleted the the previous demos just so we're going to start from scratch I'm going to upload it all in one go and we're going to see how long it takes and we're also going to watch CPU and GPU I don't know if quadrant uses GPU by default I'm not running it in privileged mode so I don't think so I think it just uses um I think it just uses uh CPU quadrant um uh Docker hub so because I don't if you go to their their Docker image there's no mention of GPU um to run the container yeah I it doesn't seem like it uses uh it doesn't uh because you so with Docker if you need to give it access to system level Hardware you need to run it in privileged mode so like if you need to give it camera microphone GPU um so I think this is just um can take advantage of modern CPU x86 architectures it allows you to search even faster on Modern Hardware okay so it doesn't really say anything about um yeah all right so anyways I'm gonna pause the video again I'll um I'll get the uh the upload working and then we'll do the stress test and see how fast it goes and pause all right almost there um yeah so let me show you I went into the quadrant client on GitHub and I think the function that I need is upload records so I give it a collection name uh Records which is just an iterable of type record with a batch size and then number of parallel I'm going to do a little bit larger of a batch size based on the quick start I think this is an example of a record so a record is a dictionary that has at the top level ID integer and then the vector it looks like it's just plain um at least at least if you use the the Raw One it does it this way but if you go here and you look at record um I think the record has to have um let's see base record Vector um I guess wallet list float okay so maybe the vector doesn't have to be um maybe it doesn't have to be a numpy array I think if you do upload collection it does require it to be a numpy array because if you do uh not recreate let's see upload collection um here it is upload collection so you see that where the vectors it's a union of in numpy ND array um so I think that if you use upload collection it expects an ND array um but if you do upload records it just it just expects a list so let me go ahead and fix this because the embedding is just going to be a list so I don't actually need numpy here um but still looking at this where did it go the quick start yeah so you need ID vector and then payload so those are the three things that it looks and then the payload is itself another dictionary that's just what is the information attached to this one so that's what I've got here where I've got ID Vector payload and so the the payload is going to be the content which is the string and then the file name and so as a quick reminder this is what I'm going to be uploading it takes a while so let me go ahead and start by let's run the docker image so I'll show you what that looks like and then if we go here we say we see it's starting this and then volumes it has started um started the volume so this volume will grow as it's running and then if I show you in here um let's do oh why is it okay so Docker desktop um but if you look at what's using the most memory the virtual machine is using the most memory obviously okay so this is running so I need a new one of these CD quadrant all right so I think we're ready to go I'll get it running um so this is the stress test where basically I'm just going to upload it um and do it then and time it so we'll do um so we'll do upload data so start equals time and then batch size 256 and then we'll do print um uploaded in and then we'll do time minus start seconds okay um yep so I think that should be it go ahead and clear that out all right let me make sure this runs so we'll do python quadrant stress test and so now it's got to load a few thousand of uh of these um make sure that it it succeeds and doesn't bomb out and then we'll let it upload and I'll come back once it's done um so yeah spinning up 88 000 almost 90 000 records I'll go ahead and pause it and just show you when it's done you don't need to watch this all right got ran into a bug so it says IDs batch equals record dot ID for record in record batch uh dict object has no attribute ID so it's looking for a record object but I gave it the wrong thing Okay so quadrant client and upload records upload collection oh hang on okay we'll try this a different way um I'm switching to upload collection which worked with the quadrant populate so basically what you do is you just separate the vectors from the payloads and then you just upload them together so we'll give this a try and uh we'll go from there because yeah it blew up when um when I tried to so it started started the client created the collection and then as soon as it's tried to upload the records it blew up um also important note you do recreate collection and it nukes it and then starts starts from scratch so we'll give this another try and we'll be right back wow okay so I was wrong uh I thought that it would look for a uh a numpy array but it says it's not a valid list so it's just looking for a list so that's fine one thing that I noticed though is that storing these is an as a uh storing it as a numpy array was actually way more memory efficient so let's do this um so removing so it's no longer numpy array it's just vectors and payloads um so yeah let's see if this works oh boy we'll get there okay I'm hoping this is the this is the third time's a charm um so let me show you what I mean so we're at 65 66 it's going up fast we're already using 1.5 gigabytes 1.6 this will be two gigabytes of of ram just to load it um oh hey look it's uploading records now now we see the VM memory is going up slowly CPU is not going up that much let's see if it's using GPU um video in code the video encode actually could be what I'm doing um GPU memory is not going up so I don't think that I don't think it's that um but yeah so VM memory is slowly Rising as it's uploading the records but it worked it seems like it's uploading so we'll see how long it takes I have no idea how long it's going to take um but yeah so the the trick was it looks like this is the final version so recreate collect collection we're doing um Vector size 512 distance cosine ID is none so it'll figure out the IDS itself and we've got vectors and payloads separate so basically what it does is it just says okay each index or uh in in the list is going to match so you got a vector and then a payload and it'll match and you can have whatever information you want in the background all right so I'm gonna pause it well it should come back one last time um and we'll see how big it is let's see volumes volume okay so it's it's going up slowly 268. I have no idea how big this will be total because I've got um like 1.1 gigabytes um so we'll see how big this gets total um 286. all right so I'll pause it we'll be back we'll look at the final final numbers in just a minute okay it may or may not have finished because it didn't error out but it was giving me these these are just info level things um then this is from the docker container but then here let me minimize this a little confusing it says um uploaded in so 88 700 in 163 seconds so that's not too bad um so let's see result green vectors 88 788 okay it did it indexed vectors count seven seventy one thousand so I think it's still indexing in the background let's run that again segments six disk data size RAM data size I don't know why it's a zero Vector size distance um so let's see 7168 so I'm not I don't know what's going on in the background but we can look at um the container it's still running the volume is 393 megabytes I don't know how it stores it that seems like it's pretty efficient um but yeah so let's do a quick search so we'll do uh search so we'll copy this guy and we will do collection so it's not that it'll be um stress test um here we'll just do this um stress test search dot text all right so here's the vector top three um all right so we'll go grab a random embedding and then we'll come over here so the vector and we'll do um all right so we'll do slash white space so anything greater than two regular expression find next okay there we go so replace and we'll replace that with zero space so replace all [Music] right so there we go we've got we've now got a vector and so we'll replace that Vector with this guy all right I don't know this is gonna work um stress test Point top three oh yeah those are escaped okay so let's see if this works Json D serial into file well okay so it didn't like it I think it's too long hmm all right I'm gonna pause again and I'll use the client to do the search actually one second okay so this is really class watch this um yeah CLS search uh okay so point zero one four seconds and um so the score I picked a vector from the data and it found you know score 1.000 so it found the exact match um and then another score that was only half you know et cetera Etc so got the exact right one um and it pulled it all I didn't get the job you know Etc oh man that was that was really fast so it's search 90 000 records and gave it all back to me exactly as I put it in in .014 seconds so really fast all told it's using three and a half gigabytes of VM memory now keep in mind when it first started up it was using about 2.2 so this is actually using only about 1.3 gigabytes in additional memory so I don't know how the memory usage is gonna um total up there but it uses very little CPU so you look at this like demo search like there's a brief little bump of CPU usage um very fast very efficient uh yeah so I guess that's about it what's most baffling to me is how the data is only 400 megabytes is that changing 393.2 I wonder if it's doing uh 393 okay no it's not changing so anyways uh seems like it's relatively space efficient in terms of data um memory is the heaviest usage CPU is low and it's lightning fast so I'm pretty impressed this was 90 000 records and it searches really dang quick all right that'll do it for today thanks for watching like And subscribe and consider supporting me on patreon