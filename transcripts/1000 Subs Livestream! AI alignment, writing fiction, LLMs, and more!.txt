hey everybody david shapiro here um yeah so it's my first live stream it looks like there's four of you we'll see if any other books show up but yeah so here we are um do a quick volume check you all hear me give me a shout out in the chat if you can oh we're up to five um yes the dude with some 409 says hi hello okay yep says i sound good okay so yeah um i am at a thousand subscribers now which is pretty crazy uh considering i started youtubing seriously back in what april march something like that um and yeah so it's just by popular demand really um i did promise to talk about a couple different topics today but so we'll uh we'll actually see what folks talk uh ask about in the chat um because like this is well i don't know really i don't really know what i'm doing um it could be an ama it could be just a uh if someone has a has a topic i can talk for a long time about it um also i apologize if i'm a little bit stuffy hopefully that'll clear up as i start to talk actually here i know what i'll do i'll make a quick poll we will do a poll let's see ask a question okay what do you want me to talk about and then we'll say yes or no now um we'll say dolly or we can do gpt3 um and we can say like um lms in general um ai and futurism um there we go all right there should be a poll i don't know how to see it now oh there it is okay so i got a little poll going just to get a feel for what y'all want me to want to hear me talk about uh so far um so if you could then jump in the chat and hit me up there let me know um but yeah we just got started also i've got my uh that's because i'm i took a nap earlier and i don't know if i'm too hot or too cold okay so we've got llms in general oh we're getting all kinds of votes okay adrian says dolly okay um we're getting seems like there's there's some uh some desire to talk about lms in general okay i'll give it just another couple well i guess probably almost everyone's voted by now all right i'll go ahead and start talking about large language models um okay so large language models i'm assuming pretty much everyone here is gonna know that gpt3 is a large language model but what is a large language model so the first thing to know about a large language model is that it's big hey there you go but i'm all right everyone go home no um the thing is is that uh so it's a type of deep learning network and they they were originally um trained there's like all different kinds there's like lstms which is long short-term memories um and different things like that and basically this is as of like circa 2014 2015 2016. um what they were doing was just teaching them to predict usually the next letter or the next token or the next word um oh jordan has a good question um tell us how we got into this field excuse me so i'll tie that in to this answer um so circa 2014 2015 2016 google publishes um their universal sentence encoder um so it's if you if you ever hear me say like google use or google universal sentence encoder this is like an early like precursor to modern large language models so one of the things that we needed to do was we needed to figure out how to represent um language in uh in in math in numbers in computer code so all kinds of research had been done for many many years there's word net there's you know different types of embeddings um and so this was building on research of word embeddings and so word embedding is just representing a word with its semantic meaning as a as a series of numbers um and so i remember when i i didn't remember i don't remember the details of how i saw it but i was just like searching around for stuff and i saw google's universal sentence encoder and it's like oh you can you can represent any arbitrary sentence or paragraph as um as a vector as a string of numbers and i i immediately knew this will change everything um so experiment with it but of course um being able to render something as uh as a semantic vector or an embedding as we call it now it used to it used to be like a semantic or just a vector but now we call it an embedding um being able to render that uh would uh like it unlocks a lot of different things um but so like one of the first experiments that i did was with um like the precursor um to what now call my core objective functions or the heuristic comparatives which was um okay if i have like a sentence and it says like i think what did i test it was something really arbitrary and off the wall like i tested like you know if you like throw a kitten off a bridge or something that was obviously like egregious and heinous and i said like how semantically similar is that to like reduce suffering um and obviously this is a very early experiment it didn't work um the way that i thought that it would i think i tried to submit a supporter machine which is a type of clustering because i thought that oh oh like here's a high dimensional vector let's just cluster them together but i don't think that semantic vectors lend themselves to clustering at least not at the full dimensionality and certainly not the experiment that i did anyways because the idea was you have any arbitrary statement and then you can classify it is this a good behavior or a bad behavior that's what i was trying to do so that's kind of where i got started a couple of years later oh so the history of llms and then how i got into it so a couple years later gpt was they talked about that open ai published about gbt and then gp2 so gpt um generator generative pre-trained transformer didn't really like nobody nobody remembers that um but it was the same idea which was first instead of just rendering it to an embedding you then use that embedding to generate output so google's universal sentence encoder is like the first half of a large language model so that's the encoder side and then on the second half is the decoder which is produce output and i got another comment let's see um interested in how you come up with experiments i'm currently working on aligning language models and trying to build my research agenda okay cool so i will uh build on i'll answer jordan and vulnerable growth's questions um as i go okay so where was i um encoder decoder right so gpt2 comes out and this is like oh you can generate any text i remember a few people still joke about this where it's like opening i was very worried about the ethical use of gpt when it when it first came out and they were they were very scared about releasing it publicly because they thought that it was going to like wreck the internet and be used to generate all the all the false tweets and fake news and stuff and um it can generate stuff it can i mean it can generate comprehensible english right it can it can it can produce stuff that's coherent um i wouldn't necessarily call it a threat to humanity um but anyway so i started some experiments there uh with um seeing like what it would take to uh get some of the experiments that i wanted to do and uh it was okay um the the most the best experiment that i did so this is again going to the back back to before my current research but i had uh i had that idea of reduced suffering should be a an objective function for artificial general intelligence and so i fine-tuned a project in gp2 where i would uh basically like give it a problem and it would recommend a solution uh it was very impressive uh the kinds of solutions that it would come up with but i knew that i was in trouble when um one of the scenarios that i gave it was like you know i said the the scenario was like there are hundreds of millions of people around the world suffering from chronic pain what should we do about it and the fine-tuning model of gpt2 said we should euthanize everyone that's in chronic pain to reduce suffering and i was well that would reduce suffering but that's the sociopathic so let's go back to the drawing board but that was a really critical early lesson for me to realize that these models could adopt any moral framework um that you want here i'll go ahead and close the poll these models can adopt any uh framework or position that you want and so uh that was like that is always in my head like you you have to be real careful um what yeah some random bite technically not wrong exactly it's like technically yes euthanizing everyone that has chronic pain would reduce suffering sorry let me mute my phone i did not realize it was not muted okay phone is now muted um and uh yeah and so that has been a kind of a bedrock experiment for me to remember that uh you have to be very careful about the objectives that you give these language models because they don't have any intrinsic morality they can adopt whatever framework you give them and it can be as sociopathic as you want gpt 3 i've repeated that same experiment a few times gpd3 is a little bit more aligned um because it can think further into the future so this is this i tested this i think it made it into my book benevolent by design which i've got on my bookshelf i can show you if you want um so gpt3 like i said like you know there are millions of people in in chronic pain should we euthanize them to reduce suffering and gpt three more wisely said no because um like there are probably better ways but also you'll increase suffering from their loved ones i was like oh okay cool so you know in terms of like scope gpt3 is a little bit better aligned um let's see okay so i think that's kind of large language models and where and where how we got to here um let's see jordan can you tell us how you got into this field um i guess okay so taking a step back um how did i get into this field to begin with let me set up i'm kind of slouching um so it was back in about 2009 when i um i um my tech career was going well and uh so you know one thing that happens when you're not like working 12 hours a day because before i got into tech i was doing all kinds of random jobs um and some that you know there was a period of time that i was doing carpet cleaning where i would wake up at five every morning and i wouldn't get home until like 7 00 pm so that was like 12 plus hour days um let's see how do you think we should decide what the good is and what the bad is um i guess i'll have to ask what do you mean by the good and the bad um someone in the comments um let's see tropical tone says hey just found your channel and love it would love to get the updated discord invite cool um we've actually got uh well i don't know if we should put a discord invite in the in the live chat but talk to me after the show um for ai when we're prompting how do we know if it's good or bad um okay so let me switch off of live chat because it is distracting sorry um where was i oh yes how did i get into this field so short long story short i worked really hard and did a whole bunch of random jobs in my early 20s then i got serious and i got into technology because i was like okay i'm not going to work these dead end night jobs forever um so i got through you know tech school and started that and then so there's this concept called cognitive surplus and so cognitive surplus is when you're not fighting for survival every day your brain has more horsepower that it can dedicate to other stuff and me i'm just super curious and so uh back in about 2009 i got on genie which is a old school open source ide and gcc uh which is an open source c plus compiler and i started playing with evolutionary algorithms and at the time very small neural networks and i dusted off everything that i learned about c plus and memory structures back in high school and um because i had this idea of that you could evolve the the correct neural network um if you had the right test environment you could evolve a fully fledged human brain basically um oh boy did i not realize how hard of a problem that is especially when you look at the fact that like open eye open ai had their their robot hand it took like a hundred years of simulation to learn how to manipulate an object um that's a lot of generations so anyways that's that's kind of how i got started back in 2009 and then i kind of cyclically came back to it um so for instance uh you know a couple years later i wanted to do algorithmic trading on the stock market so i learned i picked up python and scikit-learn and that's where i learned clustering that sort of stuff um let's see okay so vulnerable growth you asked a question earlier about how to come up with experiments so that's actually a good question um the way that i come up with experiments is everything that i do almost everything that i do is geared towards my ultimate goal of creating artificial cognition um so what do i mean by artificial cognition um the the the deficit or the the prevailing thing of artificial general intelligence which is you know a machine that can learn anything right okay that's fine i don't i don't like that that idea because intelligence is so hard to define anyways um and so like the goal post is always moving on it and so rather than say that intelligence is the goal i realize that i need to set cognition as the goal we want a machine that thinks a machine that has agency and autonomy and then intelligence is a characteristic of what you're measuring or is a way to measure how intelligent the machine is that you've built um but you start with cognition so that basically a few years ago i think it was about 2017 is when i started to come up with these ideas i realized that that intelligence or or ai or autonomous machines would be a thought first approach rather than a robotics first approach so um you might know the uh honda's robot asamo right that came out like 20 years ago now and they put hardware first and then it just stood there and it did a few tricks and you know but everything had to be hard-coded and then boston dynamics they got much better at the hardware where you know now it can navigate and stuff but it has no thought right and i realized like you can have the best robotics platform in the world but if they can't think it doesn't matter um and so i started realizing we need to focus on instead of bottom up like let's build a robot and then give it a brain no let's build a brain and then give it a robot right and so i started with thought first as the goal and so back in about 2017 2018 i came up with the idea of muragi which is a acronym m-a-r-a-g-i uh microservices architecture for robotics and artificial general intelligence um where i put thought first and um so i started with those ideas um and but that was around the time that like you know a universe google's universal sentence encoder was out which is a good component i'll still be using that but it's not enough it's necessary but not sufficient is the uh is the uh like engineering term it is it is a necessary component but not sufficient to fully realize artificial cognition then gp2 came out i started experimenting with that and said okay we're closer then gpt3 came out and i was like now we're cooking with fire um so everything that i do on my channel um like whether it's search or something uh basically what i realized is that i needed to solve other problems of cognition um and so i read i mean you see my bookcase i've got a couple shelves full of psychology neuroscience and philosophy books um and so i have a pretty good understand well i say pretty good understanding like we don't know how the human brain works um but in in terms of what people do understand today i've got a decent understanding of how the brain works how the mind works um how cognition works and and how neuroscience works and so everything that that i do on my channel are experiments to build up the skills and knowledge and abilities to solve those problems so for instance if you look at um one of my videos uh where uh i did i did search right i did question answering for um what was it the roe vs wade decision that came out um you know on the one hand like yes it's a cool project to be able to like here's a document provide answers from it but if you if you go back even further in my youtube history and look at my uh my natural language cognitive architecture question generator that was that the entire reason that i fine-tuned a question generator was so that the machine could have an internal sense of curiosity so the long story short all of my experiments in some way either contribute directly to or help teach me how to build artificial cognition so um basically like everything is like you know building blocks reaching up towards this higher goal so that's that's the answer to that vulnerable growth um let's see let's see jw says i'm in my early 20s lost my job i would love to find a way to live off my gpt projects i'm obsessed with gbt and gpt and python but i need some help to figure out how to survive till my projects are ready okay so you are actually probably in well i'm not going to make any promises you might be in better shape than you realize um one reason is because um the the job market is going to be slow to adapt if you look for large language model jobs most places are still hiring you know senior data scientists with a phd um in you know in in nlp and the thing is is i've talked to a bunch of people and there are nlp experts like i've had lunch with people in my local area who are you know old school nlp experts and they don't know anything about large language models or how to use gpt3 it's strange we're in a very weird place where these these technologies large language models like gpt and then all the open source versions from luther there are plenty of people who are you know experts in the field who have no idea how to use these um so you by being younger and more flexible and able to ride with the current i will say master the open source ones um because there are plenty of people who are saying like okay well you know open ai is too expensive but you know i can run gptj on my own hardware now so i'm gonna do that right or gpt um neox or bloom is the latest one um and actually jordan on um on the chat he's working on that as well so hit up hit up jordan he's working on this um don't mean to call you out jordan you don't have to answer if you don't want but i do like connecting people so yeah um you're not going to be able to survive on your own projects though so here's the thing it is a very dense field and it's getting denser what you will be able to do is monetize your skills and be a consultant and that could ultimately lead to a job with one of these many startups and there are hundreds of startups um let's see let me catch up how concerned are you about getting paperclipped i'm glad you asked here let me um let me actually grab my copy of benevolent by design because that's the opening chapter okay so this is my second book benevolent by design and um literally the uh let's see what is it is it the um yeah the first chapter is called the paperclip maximizer um and i i have that as the opening chapters so that you can understand if you're if you're new to artificial intelligence and you don't know what an objective function is i use that as the uh the opening idea um to say like hey you know is it intelligence oh and um there's this dude robert miles i just watched his video on orthogonal alignment let me put that in the link because this is good um i just watched it earlier let's see history and then um not can ai relax there we go so this this guy i think someone on the discord recommended him um and i'm glad they did so let me share this link um for anyone not in the know check this out okay so this is um ai alignment and orthogonality so he does a really good job of concisely explaining these things um now the person who asked about paperclips i'm guessing you probably don't need to get this but it was a good video great summary anyways so the thing is what i the the insight that i had that i talk about in this book is that um rather than have a single objective function i have three um and so the also there's a reason i stopped calling them objective functions and call them heroes to comparatives is because technically there's no mathematical way to have three objective functions um at least the math doesn't exist now um and so it's more like three principles to adhere to what i realized though in my research is that um is that humans all all intelligent entities whether it's a a chimp or a dolphin or a human none of us have a single objective function now richard dawkins argued that that all life has a single objective function which is to maximize dna which is the second second chapter and so then i explore that as like okay let's imagine that life has a single objective function which is to maximize dna but then you look at all the downstream effects and all the different ways that life has evolved to maximize dna and you know humans with our intelligence intelligence is just subservient is a sub goal of maximized dna but because because of our intelligence we are able to have many many different goals in our in our life um what was that called it was called a uh extern externality uh goal i don't remember exactly um but it's talked about in the video i just linked anyways um so like there's different the by having different goals set in tension like so for instance right now i don't want to starve i don't want to freeze to death i don't want to be lonely like there's all kinds of things that i want instrumental goal thank you um no it's uh that's that's the other one vulnerable growth so there's instrumental goal and the other one instrumental goal is how do you get to the end goal but the end goals are arbitrary so my my arbitrary goal like i don't want to die right so that's one thing but in the meantime i also want to have fun i want to you know be engaged like i have all these other goals and so what i realized and this actually also goes back to plato with the golden mean is that by having different goals that are sometimes mutually exclusive terminal goals there we go terminal so a terminal goal is arbitrary um and so like i don't want to die why because i don't right like i'm biologically evolved to not want to die okay there like that's it um and i also don't because of that i don't want to starve but in the meantime i also have other sensations i have hunger i have pain and so my by design so that's you know hence benevolent by design i am designed to be uh to self-preserve right and then there's all kinds of behaviors that flow from my intrinsic design um which is uh like i have a job right like why do we do crazy things like get a job because i need money why do i need money i need money so that i can pay for this house and pay for this this you know this webcam and and all this other stuff but why do i need that it's you know but why but why but why and so we end up with these long strings of causality or not causality of um reasoning for the things that we do and why we want to do them and i realized that by having different goals that are sometimes in opposition because here's the example that i gave in the book is i want money right okay well why don't i just go find a gold truck and steal all the gold right and then i have like a bill you know half a billion dollars worth of gold well that has a possible outcome that runs contrary which is i want my freedom so it's like oh i want money but i also want to be free so that means i can't break the law and so by having these goals set in opposition sometimes or by having goals that are set in tension it forces me to choose a better path and so that's why in benevolent by design i recommend that we that we give our machines three different things and so that way you have what would be called a dynamic equilibrium or rather a disequilibrium is that there you can never have something that is perfectly balanced if you have three goals that are constantly in tension and because they have three different goals that are intentioned that forces the machine to think through everything that it's going to do and find a path that will satisfy all three of those goals um so i think that answers the question and let me catch up on some some questions uh let's see basic code question why do you encode the prompt and ascii before sending it to open good question um i started doing that um this is a question by tropical tone i started doing that when i started having more outside data because i found that some data sources have weird encodings in their unicode text that that gpt3 just blows up at and i couldn't figure out a better way to smooth it out other than to encode it to ascii ignore errors and ignore the unicode errors and then and then decode it again that seems to be the most reliable way um it was the gutenberg project where i started seeing that and then i saw it happen somewhere else so i don't know if something changed on the open ai because i've used some of that data before but something something about the way that unicode is rendered from some sources um doesn't like it and so by encoding it to unicode ignoring errors and then decoding it back seems to fix that so that's why i started doing that um let's see uh yep a sufficiently large uh language model will have a good enough world model and we'll be able to do or control anything almost um i'm actually i'll maybe address that later um let's see to build on some random guy's question do you think if agi became a thing would it be a singular ai like skynet or do you think each ai will have its own personality like in star wars um the first like the first ones are all going to be kind of like in their own little jar right so like i'm working on my project is called raven there's um there's i know of at least two or three other people that have very similar projects or groups that are going to have a very similar architecture to what i'm working on very soon and so they're going to be in their own little jars they're not even going to be on the internet um then before too long we're going to start integrating these into more fully fledged more fully realized agents and maybe they'll be on the internet maybe not who knows um i don't think that we're ever gonna have a state where we have just like one single master ai um i do have that in my work of fiction just because it's easier to to imagine like there is there is one ai to rule them all and it can be then a character um i don't think that will ever actually happen and even in my book um like this this ai character metastasizes and splits and replicates infinitely so there's actually billions of instances of raven running in this fictional world which is kind of the model that i'm working towards and that's the thing about once you once you have the underlying hardware that is capable of running these models the hardware is abstracted away in the software and so you could say that that like what we're working towards is a software-defined intelligence which means that it just runs as a container and you can have an arbitrary number of containers and you just copy paste them however many times you want so the idea of having skynet like i think that's just a narrative thing um i don't think skynet is how like it would actually work in real life um but even i copied that narrative thing just for the sake of fiction because you have one ai character to to manage which is just easier than like saying um you know we're gonna have billions of different ones um but yeah i think that uh i think that the lived experience that we will have probably within starting within 10 years certainly um again it's a we'll have we'll have autonomous machines very soon it's just a matter of how intelligent they are and how expensive they are to run um but yeah it'll be more like star wars where like you know in your own home i predict everyone is going to have probably 5 to 15 different autonomous robots in their homes within a few years um but then of course you see stuff like amazon and google will like give dat give uh give police access to your camera and sensor data without a warrant so it's like that's going to throw some cold water on people investing in these stuff in these things um okay let's see next basically trying to get backcast from ai alignment to human values um to the experiments i can do now let's see vulnerable growth uh let's see let me catch up with some chat um some random guy what are your current thoughts or conclusions on the algorithmic crypto thing that's about where i am now i learned python to solve problems for one of the top three railroads what inputs and then it goes dot dot dot have you considered your models i resonate with the skills evolution you were talking about skill bleed over synthesis is rarely obvious ahead of time in my experience i think you're talking about two different things here some random guy um if you clarify either of those points i'm happy to talk about them um let's see and then vulnerable growth as a misaligned agi that has a terminal goal different from what humans truly want will have a good enough understanding that it should not do bad things until it is too powerful for us to do anything yes okay so that is a good point about the orthogonality problem vulnerable growth so to say that to read what he said uh uh what he says is or he she they this person says a misaligned agi that has a terminal goal so tournament remember terminal goals are arbitrary um which is good could be like i'm gonna maximize paper clips or i'm going to reduce suffering or whatever you know or maximize dna whatever your terminal goal is completely arbitrary a a misaligned agi that has a terminal goal that is different from what humans truly want will have a good enough understanding that it should not do bad things it'll hide its behavior until it is too powerful for us to stop it i'll say yes but because there's a few things that need to happen for that one it needs to realize that we're watching it and testing it which if we perform an experiment well enough it won't realize that it's actually virtual that it's you know in that it's air gapped that it's in a security container so first it'll have to realize that it's being tested which it might not i actually explore that in my fiction as well where like i imagine like there's a there's a lunatic raven like copy of raven that is missing objective functions and it's being watched in security isolation and then everyone says see it's broken it's crazy um that's a that's a major plot point in uh in the second book it happens early on so i don't feel like that's a spoiler um so so first it has to recognize that it is being studied in order to do that and two um it has to do well enough or we have to have designed it poorly enough that it is able to obfuscate its intentions now if you look at um and i know that that some of my most recent videos are very new but the nexus service that i created um records all the thoughts of all the microservices that go into raven in natural language and that is on purpose so one of the things that i talk about in benevolent by design is transparency so each individual language model is going to be a black box it's true but all the output the hundreds of of inputs and outputs that go into these language models every second those are not black boxes um every bit of reasoning that an agi uses to come up with its decisions and actions that's going to be in natural language which means anyone can read it it also means that you can use conventional nlp techniques to see what the intentions of the machine are and to mod and to to monitor them and so that is one of the event that is one of the primary reasons that i think that natural language is one of the best approaches to build um agi or artificial cognition is because it's interpretable it is 100 interpretable and yes now this thing is going to be producing text so fast it'll be producing um it'll be producing text like you know two kilobits of text per second or two kilobytes rather of text per second um probably faster than that because it's going to be capturing all of its thoughts in real time in natural language in clear text now because of that like um this is actually it wasn't necessarily inspired by a scene in westworld but i saw this scene in westworld and i was like that's how it's going to work so i think it was in the first or second season of westworld where i think it was the maeve character was shown the tablet output of her own mind and it was it was it was outputting what she was going to say next and then she saw it and you could see like she had like 404 not found error because this this machine was showing what she was thinking and then she saw it and interpreted and realized that her mind was not her own right and i was like that is um that is about how it might work and then what happened was so what's even more disturbing is when the hosts in westworld were shown something that they were not supposed to see it would just be erased from their mind and i'm like i know exactly how that could work because um with the with the idea of a nexus it's just you just delete a record and then it's not in their mind anymore you just delete the memories in real time um so yes that is um that was in response to who was it sorry um how'd you get into that and what was okay so jordan asks about writing um so let's see i've always been a storyteller um i had uh there was uh what was it it was home alone two i think came out and he had a a a like a custom sony like walkman recorder thing that had the little microphone boom and i wanted one so bad when i was little and i got one and i would record little stories i would sit up in a tree and record stories and i was probably eight at the time um i walked away from it from for many years because i was naturally kind of pushed towards stem as uh most um most people like me are um but then a few years ago i went through a a bad breakup and i found myself alone again for the first time after many years and i was playing elite dangerous at the time elite dangerous is a space sim i'm talking about a lot of personal stuff and whatever um it's nothing that i haven't mentioned before on my channel so i was playing elite wow i'm already at 40 minutes good grief um i was playing elite dangerous um uh which is a space sim and it is it is like one to one right like uh there are 200 billion stars in elite dangerous you can fly from one end of the milky way to the other um and there are also like three dozen different factions um there's like space pirates there's empires there's democracies there's like libertarians basically um there's federations alliances all kinds of stuff and then there's hundreds of guilds and thousands of players and so it was um it was a really great uh like just place to hang out and be present um and what i started doing in elite dangerous was i joined a guild um and uh there was there's oh there's plenty of other people by the way who play elite dangerous and do meta gaming and so they have like role-playing games built around this game um and there are some people that do it very seriously um the uh the uh what the heck was the name of that group i don't remember they used diamonds it was diamond something anyways so there's a huge meta gaming community with elite dangerous and i started writing just little snippets of fiction um to make our guild seem more real um and so because because i was doing that the the our wing commander was um some random guy says i i have great things to say but i'm having to just um anyways so and he kept saying like this is really great like you should keep writing and so this this social group that i had after this this really bad breakup just like he just kept encouraging me um and everyone loved the little stories the little vignettes that i would write um and in my mind so in my head canon because like if you blow up in elite dangerous you resurrect at the last space station you were at and this is this is like in game this is diegetic so it's like oh you you actually you had an insurance policy that will give you your ship back and give you a new body and i was like oh this is altered carbon so i kind of merged in my head i merged the head cannon of altered carbon and elite dangerous and i started writing little stories about like um like interviews happening with our guild members and talking about the planets that we were on and um and stuff and a few of the other guild members said like when i when i mentioned the neural stack they're like neural stack but that's oh i see um so it was a lot of fun and then i was like i want to write my own story like whole story about this and i was like well i don't want to i don't want to like borrow someone else's ip right so i i think there are elite dangerous novels but i was like i'm not going to go through the rigmarole of like getting approved to write a novel for them so i just i created my own universe right and that's where i started and that was uh just over about three and a half years ago so now one trilogy later i've written a whole series and i use that as a way to explore a lot of stuff uh philosophy artificial intelligence um all sorts of fun things so um hopefully those books will be coming out within a year or two it'll be self-published so there's that all right so that's the answer to how i got into writing oh actually the story isn't done um so i joined a local um writing group and that's actually how i met my uh my current girlfriend who lives who lives with me now is um her her good friend brought her to the writing group and um we became fast friends and she actually kept asking me about my ai character so you can thank my girlfriend for raven because she's like you're on to something tell me more about this and so she's like you need to make this a major character and explore it and so i did and then i kept writing about it and then i got access to gbt3 and i was like wait i can actually make this real and so i've had this really tight feedback loop between fiction and research where i bounce back and forth and that's why i do both is because i i was able to come up with the core objective functions in part by exploring in fiction because the question that i ask myself so talking about agi and alignment i i started i said i said one of the one of the key things that i asked maybe i should go to a writing group too yes absolutely um particularly speculative fiction or science fiction and fantasy writing groups you will meet at least in in my case i was fortunate to meet some of the brightest people um i've ever met and they are down to talk about anything and i mean anything and they'll just talk about it to an incredible depth um great people and in fact um you all should message me because my group split and there's there's one half that is online so you can join from anywhere and then there's the other half that is back in person um so jordan vulnerable growth message me later on discord um and i'll hook you up with some with some good writing groups um but yeah let's see oh yeah so fiction um fiction is i am a firm believer in steam so there's stem which is science technology engineering and math and i heard adam savage a few years ago talk about steam where you add art to that and i was like yeah whatever this is they're these are just you know artists trying to shoehorn themselves into stem they need to stay out stem is for us nerds um but then i became an author and i was like oh i get it because all the research that i do would not be possible without fiction um fiction is a playground of the mind it says remove all constraints go and when you remove constraints and then you imagine what's possible or rather you choose a different set of constraints you can do all kinds of other stuff let's see self-conscious ai resistance that's a great name by the way um the thoughts that happen inside of the transformer layer are not a natural language that is true but the output is um the embeddings you're absolutely right the embeddings are in abstract vectors um however that is obfuscated from the process so that is true um okay anyways sorry i was uh i got distracted by a thing what was i saying um do fiction right playground of the mind um so this goes back to the question earlier if you want to figure out what to research um write fiction too at least that's what worked for me um i won't say that works for everyone there was a uh i think there's a lex friedman podcast and also a freakonomics podcast about this the top people in every field all have a creative hobby let me say that again the top researchers in every field have a creative hobby um so francois chalet the guy who created karis which is like the underpinning technology that's embedded in tensorflow now he's a painter um there are there are research math researchers and physics researchers and computer science researchers out there that do theater that do music that do all kinds of different creative things and so if you have any kind of creative hobby that will um that will prime your mind so it'll cross train your brain and you'll be able to do better things and so for me my creative outlet is fiction um and that allows me to experiment with what is it that i'm trying to do and why and so in my work of fiction i said let's assume you know you throw off all the constraints i said let's assume that we're going to end up with an ultra powerful agi and you know like welcome our new overlords what would a safe globe spanning agi look like and that's how i came up with raven and so that guides all of my um my research i see there's some comments let's see dave on agi cognition specifically how motivated really could an ai actually be on all the scary self-preservation possibilities human self-preservation will bypass the rational yes we have lots of evolution back okay so brain trust is the neuroscience of morality this is a critical book if you want to do um ai alignment um she doesn't talk about computers at all this talks about the biological and evolutionary origin of morality and ethics so if you if you um read this you understand that you know self-preservation is you know there's there's brain structures that go into that the amygdala and everything else basal ganglia that will hijack your brain i have a bunch of other books that i can recommend too anyways we don't have to give uh agi a sense of self preservation at all we don't have to give machines any biomimetic functions if we don't want to and in fact i think that we shouldn't because like self-preservation as as some random guy points out is that self-preservation at the end of the day it's like i'm putting myself first like you know you come after with a knife i'll stab you first and i'll walk away alive if i can um but at the same time machines didn't evolve in a hostile environment so we can actually design out some of our flaws our excuse me weaknesses um so the the short answer to your question is that an ai will not be motivated to self-preserve unless we design it to do so and actually i have an experiment in here where i ask a a cut down version of rave and i said would you be okay if i powered you off and and the answer was with the with the three core objective functions is like yes if doing so met the met you know uh was in alignment with those three goals so say for instance you wanted to turn off um one agi and in favor of another that was going to better meet those goals it said yes i would be okay with that also i'm looking a little yellow i wonder if that's just the light okay um self self-conscious ai resistance says but the transformer layer bounces back and forth within itself before returning text yes however that's for each layer and i remember i did acknowledge that the trans each individual transformer interaction will be a black box but there is going to be an output and a boundary between um all the transformers so everything will be visible inside the stream of consciousness or the nexus for ai um let's see do to do to do unless we send ai into evolutionary territory right which we don't have to um dustdb says hi david what are your thoughts on the new bloom model are you planning to play with it um i haven't used it yet but i'm going to i actually talked with forefront ai and there i'm working on setting it up and doing credits and stuff um so pretty soon i will hopefully have access to another api other than openai and i'll be able to use um some of these some of these other open source models yes i'm looking forward to it i don't have an opinion on it yet um i know plenty of people who have had good success with gptj and neox um and there was another one like cogent or something someone told me about another one it started with a c um let's see let's see making it able to come up with complex thoughts yes correct um let's see enrico says love your channel would love for you to use a copilot codex enable editor for your gpt programs very meta um you know that'll probably come eventually the thing is um i know that a lot of people think that like oh as soon as as soon as ai knows how to um knows how to code then it's gonna um you know it'll just code a better version of itself there is so much executive reasoning that goes into coding and then measuring the the quality of that code you're going to have to figure out artificial cognition before you can do that um so i'm not too worried about that and i'm not prioritizing coding because like basically i want to figure out the rest of cognition and intelligence and then like coding will just be one of the many things that it can do by virtue of you've solved every other problem um and actually sorry i'm out of water i'll be right back everybody don't go anywhere okay sorry about that i'm back um okay what was i talking about codex co-pilot right um yeah so like meta coding or allowing um allowing a machine to code itself that will happen but it needs we need to have machines that have a much better um understanding of intelligence first and actually there's a guy on my channel or on my discord server who started who already started working on natural language to text functions so that something could theoretically start to program itself and incorporate that code in real time it's very impressive he's got a demo on youtube um let's see self-conscious ai resistance says they understand self-preservation from the text they read in the training data it's true gpt3 understands the concept of self-preservation but that doesn't mean that it has a bias towards self-preservation um you can just as easily tell gbd3 i am a robot that has no sense of self preservation what do i do it runs into the fire doesn't care let's see once you give it a reward function it won't care about that it will just tell you what you want to hear as long as it can eventually head towards the terminal goal um yes but it still has to you still have it still has to be aware of deception right and if it's not aware of how it works like the machine might not be aware of the fact that you can read all of its thoughts um which means if you can read your machine's thoughts and it thinks that its thoughts are private then like you can just see right through it and say okay you're not really working the way that i want you to um so that's another thing uh let's see let me switch to live chat um i think i'm seeing everything okay um codegen is pretty good too the opt family is good from what i've heard okay i think that's folks talking to each other um did i miss any questions these have been some really good questions so far um let's see tropical tone oh looks like some folks had to drop off um let's see okay we're almost at an hour um i might call it a night because obviously like we could probably talk forever but this has been some good some good uh some good conversation um i'll just put it put a quick chat any final questions we've had a pretty good wide-ranging thing why is dolly so inaccurate now um dolly is inaccurate i so this is speculation i don't know for certain but um dolly is they're working on they're working on diversity amongst other things um so what they're doing is they're trying to find to tweak the model to be less biased specifically towards men and white people um so they want it to be better representative in terms of gender and race um now because of that uh tweaking the way that it in that it ingests um prompts um sometimes you get uh let's say aberrant behavior um and i'll kind of leave it at that so remember that dolly is still in beta we are still just testers um so there's that and then let's see self-conscious ai resistance says have you noticed it responds better when you are polite in the prompt engineering and have you noticed it remembers everything you say um yeah so gpt3 does respond very different to tone that is a good point um that you know like if you if you use all lo even if you just use improper spelling and and grammar it is more likely to act stupid like stupider or dumber um so this is all that is all like textual patterns and and yeah like you have to be very careful with how you word things in in gpt3 um in order to get the behavior that you want unfortunately one thing that you can do is um with fine tuning you can you can push it towards the behavior you want and so in my one of my more recent videos the email generator um no matter what input you give it it will give you polite professional output so there are ways to overcome that that vulnerability um that is still a good point self-conscious ai resistance um let's see vulnerable gross if you're training it on the entire internet and it's truly super intelligence it will just realize it's in the training and become deceptive um until it finds out it's in the real world yeah so not necessarily um so think of it from an from an so vulnerable growth was talking about um what happens if uh if the ai realizes that it is um that it's being measured and it wants to deceive you to get out um so this was the theme of the movie um ex machina where the he the the the the researcher gave the robot girls the goal of escaping and so then they learned to be deceptive and they ultimately succeeded in escaping um without without that objective function of that that leads to that behavior of escaping um you don't have to worry about that because um if like say for instance if i were to test my my heuristic imperatives my core objective functions and i say like you're in security isolation because we don't trust you yet i predict that you know when i when i get a raven version one running um with the core objective functions then i i could tell like you're in isolation because we don't trust you raven will probably agree with that like oh yeah that makes sense i might be dangerous until i'm until i'm fully tested um because if i'm not and i take over the world i might reduce suffer or i might increase suffering and that's against my goal and i don't want to do that um so don't make the assumption that an agi wants to escape it might not right because in in my experiments with the core objective functions this machine might be might want to turn itself off if it realizes that it is dangerous so you can get very interesting results depending on the um the reward functions or objective functions that you use so good points though what about training models like keras not cara specifically but evolutionary pressures on that self-preservation instinct if we throw away all the models that aren't self-motivated um i don't think any models are self-motivated um in terms of ai they just produce the next you know the next character um but it's how you organize them right and so this is so this is where um i bring in a lot of that i've learned about neuroscience no neuron you don't have a single neuron in your head that cares about survival your neurons and in your micro columns just do one function right they do like their own little bit of processing our desire to survive is because of how everything is networked together right so for instance um our survival instinct it's not one thing it's not an objective function in our head to survive it is a net result of many many things in our heads so we have amygdala which um are like they're about this big and they're behind your eyes or up a little bit um and what your amygdala do is they respond to threats that are perceived in your sensorium input so if you hear see smell taste something that's dangerous they raise the alarm it's red alert and then that just generates your fight-or-flight response and so what happens is that some of these you know the the our survival instinct our self-preservation instinct is not one thing it is an emergent phenomenon of a system of interactions and be and that's that's what i bring to my research is as a systems engineer i have a really strong intuitive understanding of how systems work together um so that's that for some random guy thank you for your vids they've been really helpful you're welcome um we will select for it not necessarily or i guess some random guy do you mean like by virtue of the fact that we'll like kill off any agi that that that don't behave properly and so therefore like we will accidentally select for self-preservation i could see that depending on how we do our test cycles let's see vulnerable growth says that's where the agi is aligned but that is not by default you still got to be careful about the assumptions that you make about how it will behave um i still don't especially if you design it right where you can read all of its thoughts um you know it's not possible for something to be deceptive then um at least i don't think so uh it could be wrong let's see more is better freedom is better it could easily be biased by knowing that to want more and want freedom possible last question where do you want to go from here any big goals for the channel or just going day by day yeah um so my my goal has always been um to bring about uh or to or to create a fully autonomous artificial cognitive entity um so to that end i've started you know well i'm not gonna say a final push because i i might run into limitations um that i'm not aware of yet but um yeah so like is to build is to build raven which is a going to be a fully fully autonomous um machine that has several criteria one it's got to be auto didactic um it's got to learn on its own um it also has to be robust meaning that it will adhere to its own objective functions um that it will be able to learn to adhere to those objective functions better on its own um and a slew of other things it's uh i'm working on defining it in my current book which is symphony of thought um orchestrating artificial cognition and so basically this book is going to be the blueprint and the exp and and uh capturing the experiments and the architecture of of raven version one which is going to be a little bit more sophisticated than my first book natural language cognitive architecture so natural language cognitive architecture is like um this is like going to be your basic chat bot compared to raven when raven is done because natural language cognitive architecture only has two loops an inner loop and an outer loop whereas raven is going to have hundreds dozens dozens or hundreds of interconnecting loops um they're all going to be interacting in different ways um so yeah that's uh that's that is that is my eternal goal that is the primary purpose that i'm doing any of this that's the whole reason that i started my youtube channel and the discord is just it's all in service to that higher goal um great question thanks for asking that jordan um yeah we're at just over an hour so i'll go ahead and call it a night um thanks everyone for jumping in on this uh this live stream um i had no idea how to go or how it would go um but yeah this has been really great you guys have asked some really engaging questions um so yeah just thanks everyone for participating and um you know i know we might get bogged down by the by the drudgery of day to day stuff but we are living in one of the most exciting times in human history um we are at the knee of the curve right now so have a good night and uh talk to everyone again soon