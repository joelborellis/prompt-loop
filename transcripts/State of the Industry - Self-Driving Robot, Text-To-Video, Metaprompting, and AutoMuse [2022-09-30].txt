good morning everybody David Shapiro here with your state of the industry update now first I need to address the elephant in the room and that is that you might have noticed that these videos are coming out more slowly the reason is because I posted a poll the other day and uh the results were almost unanimous um y'all wanted me to focus on making higher quality videos with lower volume so here we have it um all right so first up today uh I am of course going to plug a little bit of my own stuff is auto Muse so Auto Muse is my product my project of all of my experiments surrounding fiction so I've already done experiments with half a dozen or so different tools using gpt3 uh for creating fiction tools such as synopsis Creator um scene to scene like converting Pros to movie dialogue and that sort of thing and so what I'm doing is I'm getting a team together actually later today I'm having our first Founders meeting so I'm building a team and we are going to launch Auto Muse as a um as a as a service here soon so stay tuned if you want to get involved you can go to automews.io and click on contact and then just fill out the uh the beta access list alternatively you can sign up and support me on patreon and patreon supporters will be given write a first refusal uh Priority Access to the closed beta and then of course as things um ramp up but yeah so go ahead and sign up here or sign up on patreon and um and you can get updates on auto Muse as we progress I'll be of course posting um here things here on YouTube one of the next things that's going to happen publicly is we'll be working on putting together a Kickstarter campaign so keep your eyes open for that that's probably going to be the next big announcement for auto Muse moving right along the next big thing we have to announce today is Google's LM nav self navigating robot self-driving self navigating robot now this is really exciting because this is it does a few things um one it uses a natural language instructions to tell the robot where to go and what to do which uh you know I proposed that a couple years ago with natural language cognitive architecture and then we also saw something similar with seikan which was a recent uh paper where you could give a a an articulated robot an a robot with an arm instructions like clean up this spilled soda uh but now they've taken it to the next level um and this is this is Google um uh has taken it to the next or sorry uh yeah UC Berkeley and Robotics at Google um they've taken it to the next level and I won't play the video um uh two minute papers has a great video explaining it in Greater detail but what I wanted to call your attention to is the fact that this is multimodal and so there's a large language model a video language model and a visual navigation model and so they have three different models working together excuse me to understand the goal uh and and create its own objectives and measure its progress towards those objectives and overcome obstacles and all sorts of fun stuff um uh so like you it's just fascinating to watch so for a little bit of background of me this was a like Teenage Dream of mine so my cousin and I were both Electronics nerds and we worked on autonomous robots back in the day but this was with like basic microcontrollers um which so a basic stamp is like the precursor to Arduino um and so like you could run like 200 lines of C I don't remember it was not much um and it was surprising with just a few rules how clever you could get a an autonomous robot back in the day right there was challenges this is in the 90s challenges of having a robot a self-driving robot that had to navigate a maze blow out a candle and then navigate back to where it was now of course that is like you know again just a few hundred lines of C or basic rather uh when it was the basic stamp Arduino runs on C I apologize um but this is orders of magnitude more sophisticated and it's interesting when you think of like how much code was is required to do a relatively simple task as long as you've got the right hardware and sensors but then if you if your sensor is just a camera right um and you get a whole lot more information and part of the complexity there is that amount of information that you have to process so anyways very fascinating this this hits close home to me because this is this was a dream of mine like more than 20 years ago was um I was to work on something like this um and now here it is and of course I've moved on to cognitive architecture and other stuff um but yeah very fascinating paper check out Google's LM nav um oh and here's the uh here's it's up on archive as well but honestly their their site is um does a good enough job of explaining it if you do want to take a deep dive check it out up on archive okay moving right along the uh the next thing that I wanted to share with you was an anonymous paper it is it's under double blind review right now but this paper um if you read the abstract it's not entirely clear what they're talking about um by conditioning on natural language instructions large language models have to have done uh impressive capabilities we propose automatic prompt engineer so prompt engineering is something that we uh if you're working with llms you know that prompt engineering is a big thing and so they did they did several excuse me several experiments and they were able to achieve human level prompt engineering with the llm so the llm can write its own prompts now I have been talking about this since way back with natural language cognitive architecture this is called meta prompting and actually one of my more popular videos recently was about metaprompting um how do you get how do you get the machine to write its own prompts to identify what it is that it needs to do so they are talking about this feedback loop here and you click on the PDF and they've got um they've got some diagrams about how it how it's done this is a little bit more sophisticated than the video that that I showed you um because I just I was just like off the cuff like here's here's a prompt that can write another prompt um but they they're talking about how do you how do you test it right because if the machine is doing its own work and own experiments without a human involved to measure how accurate or how effective it is it needs to have a way of measuring itself so that is a that is a good advancement so this the implications of meta prompting are not immediately obvious meta prompting is critical for autonomous machines because here's an example if you as a human are engaged with a problem that you haven't seen before essentially what you're doing in your head as you're trying experiments is you're creating instruction sets for yourself to test right and then you're measuring the effectiveness of those experiments that you've done so you say okay I'm stuck here what's a problem that everyone has faced your car doesn't start right or your car is running weird or something in your house is broken right and so you come up with a hypothesis well first taking a big step back you say okay my ultimate goal is I want my car to be running again right so you hold that in your mind and then you say okay well you ask yourself what do I know about cars you turn the key the sound that it makes changes you know that gives me more information and so part of the process that you're dynamically coming up with is first I need to gather information okay does it start no okay that doesn't work um you know do I have gas okay sure yes what do I know about my car and so what you're doing is you're constantly brainstorming ideas of how to approach the problem and then and then you're executing on those ideas that's the key thing you're brainstorming what to do uh or what to look for then you're figuring out a process a prompt an instruction set to test it and measuring the results and you're doing that in a loop and so this is this is something that I wrote about extensively in my latest book Symphony of thought which is okay how do you decide what these tasks are how do you brainstorm how do you measure your own success and so I have a collection of dozens and dozens of prompts and other similar things so this work is very much in line with my own work but this is specifically on that one problem of meta prompting so this is a really great paper I'm curious as to who who wrote it but it says Anonymous authors paper under double blind review so whoever's doing this is pretty serious and I think that this is going to be a really good really good thing and also the the acronym ape is not lost on me if you're at all an internet Denizen you know that um all the people who are a big fan of the GameStop stock they call themselves Apes so I'm just wondering if that was like a nod to that or if it's I don't know but anyways there's nothing wrong with automatic prompt engineer um I actually had on my backlog um I was going to make a video about how to make a prompt engineering chatbot maybe I'll do that since this paper is relevant um okay so that's about it for the meta prompting paper all right and the grand finale for the day the one the for all the cannolis is uh the metas make a video so this is a text to video um uh project by meta and this happened way sooner than I thought it was going to um I know that that plenty of people have been working on it and of course if you're on Twitter and you see all the stable diffusion stable diffusion um animated videos um you know that people figure that out very quickly and So Meta is has created some really decent um uh uh text to video and I'm not going to dig into the paper I'm just going to sit here and appreciate how how fast this is happening and so it's not going to be too long before you have um better scenes um that uh that you can then stitch together and make a full-length TV show or movie so I'm telling you like fully synthetic entertainment is coming and then there are no limits because it's not it's I mean it's technically CGI right computer generated imagery but it's not through 3D simulation this is going to be 100 neural representations of physics graphics people faces um I was telling a friend of mine that neural representation is going to be the big thing like if if you if you take out Transformers um the other the alternative thing is neural representation which of course neural representation is something that all neural networks do but just the concept of neural representation of whether it's places people's scenes um stories that is going to be so big um and so here we see that that kind of prediction coming to fruition and um yeah this is I I'm just completely blown away um where oh man like just the the number of possible things that you can do with this and they're prompting it with just a couple of images and then it's it's taking in Runway and obviously like it's really trippy um uh so I guess the most important thing to talk about here though is what are the implications for art so obviously things like Dolly and stable diffusion and mid-journey um have caused a lot of consternation amongst the art community not the least of which is because excuse me let me drink a little bit of coffee don't know why my throat's dry not the least of which is because a lot of the training data uses art from the internet um and that art that people have is copyrighted it is not public domain and so what are the ethics and legalities of using that training data and then of course in the future when all the future training data that is used by these AI art machines was generated by AR by AI art like what do you do with that and so maybe um maybe the the need for nfts is real and this is me saying that someone who has been skeptical of nfts because if you're an artist and you publish your work as an nft that guarantees that everyone knows you made that and every time that image is referenced whether it's pulled into a data set um you need permission to pull that image into that data set um so I think that there is probably going to be a need for nfts because you know as as a novelist right I'm thinking about like in my in in the future when my book is published I want everyone to read it and I wouldn't mind if my novel is used to train a future novel writer I mean hell that's what I'm working on with auto Muse um but I also want to make sure that I get credit for that and if anyone produces something I want to make sure they get credit for it too so one and this is not saying that the technology is bad just that there are problems to solve and we have to be creative with those with those uh with our solutions to those problems now moving moving on um one thing that I wanted to point out that's not obvious I don't know why they don't have this at the very top you can sign up to use make a video research um or make to use the make a video studio with um with meta with Facebook um so anyways this is very exciting the morality and ethics and legality of it um are are really important actually on the Discord server on the cognitive AI lab Discord server we had a really spirited discussion about um culture how is AI art going to impact our culture my position is that this is going to allow any number of of subcultures to really flourish because the thing is is stuff like movies are really expensive to make and and because of that because there's a profit motive for movies only movies that are going to be profitable get made but there are millions and millions of great stories that people want to see uh movies of so like you know I've got my bookshelf over there there's a bunch of books that I want to see as a movie but they're never going to get made because the audience is too small or they'd be too expensive to make so I am so excited because you know what like one of the first things that I'm gonna do I'm getting chills thinking about this one of the first things that I'm gonna do once um make a video like this technology is more developed is I'm going to convert my novel into a movie I'm not going to sell it to a studio I'm going to make my story into an AI generated movie exactly how I want it and you know what my the goal is not to make money but it's to make art and so by by making these really powerful art tools and democratizing access to Art I think we're going to see an explosion and we're going to be um we're gonna be just such a hyper focused culture on exploring art and all of its different Avenues anyways I'm rambling I'm proselytizing um if you want to hear more of that please let me know I'm happy to have a a live stream where I just ramble about the ethics of art and culture and how it will impact us philosophically anyways I'll stop it there thanks for watching have a nice day um like And subscribe and consider supporting me on patreon the um the advantages of supporting me on patreon is you get access to my exclusive blogs but also you get bumped up to the front of the line for access to Auto Muse all right that's it the end have a good one