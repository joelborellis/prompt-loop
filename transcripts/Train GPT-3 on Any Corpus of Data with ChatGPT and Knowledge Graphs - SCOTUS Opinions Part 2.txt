good morning everybody David Shapiro here we are back for part two of the um Knowledge Graph Supreme Court decision thing so we're using chat GPT and gpt3 to help us along and as a quick recap where we left off was um I didn't get that far um but I downloaded about 22 Supreme Court opinions this was about Anti-Trust law so a very specific domain um and then I got them converted to text so that is um I did that with another uh repo it's fine but then what we did was the the big thing that we achieved was that we figured out that we can get gpt3 to just go ahead and write my um write the uh Whatchamacallit for us the the knowledge graph Json and so what I'm doing is the nodes that I asked for was each node should be a case citation precedent or prior opinion each node should have several properties such as date case number involved parties reasoning for including in this opinion and other relevant information so that um worked really well because you see like it's got a case number it tells me when it happened and then the reasoning so like involve parties like great this is Phenom this is phenomenal information so what we're doing is we're going to create a cross-linked web as to like why all these things are interlinked so that way theoretically if a uh the the this hypothetical use case if an attorney is researching antitrust laws so that one they can you know go to a court court of appeals or even present to the Supreme Court they will have a masterful understanding of established law and the reason that this is important is because common law is um how law Works in America it's by prior precedent is the so you've got the the laws that are laid down by the legislative branch and then interpreted by the judicial branch and so the judicial branch keeps track of their own interpretation right because there's the legislative branches it it has to do with separation of powers anyways so that's how we got Where We Are um it worked really well and um there's all kinds of what-ifs and gotchas that I'm not going to worry about because this worked really well and we can iterate over the over the long term so but this I had to plug in manually and it's about four pages so like if we do if we search for how many new pages there are one two three yeah one two three so that this has four pages total um which the that that was good so the next problem is we got to take these and regardless of how long they are we have to break them down into chunks of four pages maximum so rather than do this manually I was like why don't I just ask chat GPT so here we go um I have a folder named um uh let's see um opinions uh text um this folder contains um uh text files of scotus decisions um that were converted from PDFs the pages are demarcated by um by the words new page I need a python function so I'm basically talking to this thing like I'd be talking to a developer I need the python function where I pass um pass uh actually no I actually let's just ask it to do the whole thing I need a python function that um reads every uh uh file in opinions underscore text um and then breaks each file into chunks of four pages um the purpose of this is to um limit the size of each subsequent file please then save the chunks into um into a folder named chunks text um and append um a serial number to the original file name for instance um you know star underscore one star underscore two and so on all right let's see what it does why would this violate the content policy okay I sent in feedback um let's see what we've got so import OS split text files input folder output folder pages per chunk so good it's parameterized it um if not uh output folder make it nice okay for file and Os Lister input folder um with open OS path join input folder file r as F read it excellent Pages equals split on new page perfect so it understood that for I and chunk enumerate page range zero pages per chunk excellent so this is this is a list comprehension that will break it into equal chunks or chunks of four output file equals this underscore plus oh dang that's good that's good okay um this is wonderful so let's go to this um I had started writing it and then I was like I don't have the energy for this I'm telling you I have said it on Twitter and I've said it on mostly Twitter but LinkedIn and a few other places English your ability to describe what you want is going to be the primary programming language from now on period end of story if you can think through a function then um yeah this is this is it so let's run this let's see if this worked um need a command prompt CD and we're in the scotus opinions um and then we'll do python step 01 split chunks why you no work um this actually happens quite a bit let's see if it can fix it um so thanks that mostly worked but through this error and you fixed that error please now if this can debug this I know exactly what it is yep yep there you go you need to specify the encoding yep perfect oh wow okay it's saying add the ignore flag wonderful wonderful adding encoding utf-8 should be sufficient um so that goes to do right here hmm fascinating did it get farther this is weird though input folder file contents read encoding utf-8 as infile so sometimes this happens um let's see what is the encoding here so sometimes what I do is I'll convert it to something and then I'll convert back so let's yeah hmm I wonder what happens if we do let's add let's add that ignore thing rather than rather than getting lost in the weeds let's just do errors equal ignore because you know what I looked at the text file it looks fine to me oh and let's also see if it um okay it got pretty far already so it got pretty far in the process before blowing up one two three so now we've got all the chunks cool cool cool um all right heck with it send it what do you mean this was in so this was in Split text files F right oh it couldn't write it interesting oh okay okay okay um so when we write it we need to we need to convert it to ASCII and back um so let's see if it understands this oh man I'll be I'll be jazzed if it if it understands this okay um great that worked uh now I have a new problem here's the error um please find a solution for this new bug so what so the problem here is and I um yeah that you're trying to join a list of strings that contain spaces I don't think so new page nope that's not it replace no no I don't think that's it I think we need to fix the um string encoding um here's the last part of the error I feel like I'm talking to like Hal 9000. yep there it is that's it yep ah so we need to add the encoding to to utf-8 right okay that makes sense this thing is smarter than me um okay so when we write it yeah oh that's the problem I okay um encoding equals UTF eight okay cool but uh this took way less brain power okay let's see if that works DLS clear screen that was fast all right so now we have 144 chunks of text let's see what the biggest one is 20 uh [Laughter] uh [Laughter] I think the uh I think the OCR messed up with this one why do you believe that the Sherman Act okay I'm probably disturbing my audience what's funny is that it got the capital letters fine but then the rest it's like no I wonder if it was like if the scanner bed was like moving too slowly or something foreign I'm sorry okay I'm gonna pause this because I'm gonna keep laughing okay I had to stop and make myself tea I'm still probably gonna be looking a bit I'm sorry okay now normally oh sorry oh I don't know why this is so funny normally um this would uh what I would do is just delete data but in this case I don't want to lose anything and imagine like what if half of our files were like this so let's come up with a solution so I'm going to with so this this this this function works so I'm going to say okay cool let's make a new script so let's go back over to Chad GPT great that all worked um but it turns out the OCR barfed on a few files so the um so there are lots of duplicated characters such as um please write a new script using Spacey or nltk to um de-duplicate uh extra characters um search through all the files in um see what was the folder name chunks.text in chunks underscore text um deduplicate deduplicate characters um and then save the cleaned up copy um in a new folder called um dedupt text all right will that work I don't know if that'll work is Spacey that good or T and Doc um okay I think I'll need to add the encoding but let's give it a shot and let me make sure I've got Pippin pip install Spacey okay so while that's running I will do save and we'll come back here and we'll do um Step oh two oh so you might notice I started I I um I do my functions like in order so that way like if you're looking at this repo you don't have to like try and reverse engineer what order to run things in like it's just intrinsic documentation um dedupe characters dot pi okay python step O2 and I know that it's going to barf on this so let's do any coding E equals UTF utf-9 utf-9 is clearly better than utf-8 right it's the next mission sorry I'm in a mood today apparently you having a giggle Mite okay um let's see if it works I'm not going to use the GPU for something this simple it's a small skipping registered can't find and it doesn't seem to be a python package huh let's see um it gave me this error what does it mean it didn't work and this is the value of having good error messages kids interesting all right let's see if that worked still didn't work um okay all right so in this case it seems like we found a limitation still didn't work Spacey load says requirement already satisfied yeah it says it's already satisfied um let's go let's go to Google let's see if uh let's see if this all right import so it looks like you need to do you need to install let's try this that looks like it did a thing sorry I wasn't talking through it so basically I was looking I found I found a Spacey issue on GitHub and um let's see it hasn't bombed yet so I wonder if it's working um dedupt cool there's nothing that's 20 uh no it didn't work still didn't work okay um yeah so seems like Spacey is not sufficient or at least this isn't um okay so let's go back here go back to this um okay I got Spacey to load but the original script does not work the words um in some files are still wrong um with lots of duplicated characters um so for instance for example I probably should ask another way yep yep okay so it's actually going to tell me yeah use regex so that's regex is actually the way I would have done it originally so it looks like this might be the right way to do it it so all right let's see what it says foreign do the same thing okay interesting okay um let's give this a try I will give that a try but what about uh words where um there are supposed to be duplicated characters like book or look um does yours script handle that and we use nltk or Spacey or something else to account for correct words also while this is running I'm going to address the elephant in the room and that was it like a week ago I made a video saying like meh chat GPT isn't that great um and uh so obviously like I kind of have my foot in my mouth because this is amazing it also looks like it failed um so all right let me do a time check we are at 23 minutes so we're almost done for the day um oh there we go okay all right uh it's yeah yeah it'll it's gonna give me that thing so I think what we'll do is we'll just check to see if this happens because it's better to have some data than none so uh I think what we'll do all right split chunks that one worked dedupe characters so what we need to do is find the ones where this happened right so um generally it looks like okay looks like it only happened in one document yeah okay so in this case we'll say we'll pass in any file that um starts with this so we'll we'll uh we'll we'll eat the eat the cost and what I mean by that is we'll just Excel accept that some data is not going to be quite as good but that's plausible because like it's an OCR mistake but the thing is is we just need it to be small enough to fit in um so de-duplicate characters uh yeah I don't I don't think this is gonna work so it it's close but it's not quite um thanks um can you modify the uh the uh the script that uses regex instead um the only problem is in files with and the name can you just sort um or filter filter out any other files okay so let's see what it comes up with there meanwhile it looks like this is the largest one that is um not good right so it's 13 000 characters long so let's see how many tokens this is so as long as it's that's getting pretty close um let's see if we can if if our uh prompt will work so prompt and then we'll do text DaVinci 03 temperature zero and our maximum length is going to be like 600 tokens otherwise it's going to be too long so let's see if let's see if that's enough the output is pretty short so but Json is pretty token intensive so I wouldn't be surprised if it yeah so we ran out of space so it looks like we need to go back and do smaller chunks um but fortunately these earlier scripts are really easy so for instance we just come back here and instead of doing um uh we we just update this so instead of four pages we do three and so then what we'll do is we'll come in here to opinions is fine chunks let's delete these and then we'll rerun um python step 01 and so now what we should have is more chunks so now we have 188 chunks but you see the the um that one that one so this is this is now the largest and it's only 10 kilobytes or nine nine thousand um characters long so this should be small enough to run let's see how many tokens it is um okay so that's now we're down to 2800 tokens let me remove the Json part so now we're at 2500 tokens tops so we can do we can do um we can do like 1450 so that's more than twice as many tokens so let's see it's it's more than twice as many tokens with uh 25 less input so theoretically this should be the limit um cool all right excellent I think this is good all right so we're almost done for the day um let's see what chat GPT said do you duplicate characters yeah there we go okay so let's do this for the D dupe um so let's come back here save it and then we'll do python step O2 dedupe all right we needed to add the um Whatchamacallit encoding encoding equals UTF eight and then for the right and coding equals UTF eight okay so now if we go to the dedupt oh actually here we need to delete this it did all of them interesting um oh it that's no if if not in file it got the logic wrong I'm like no it was only supposed to do like five of these let's try that again there we go okay so now the largest of these is eight kilobytes which is plenty small so we come back to chunks and we'll replace those replace yes so now our absolute largest file is 10 kilobytes 9 300 characters so this should all easily fit within um no this should all easily fit within our prompt window here so now we need to just go ahead and um run the thing so all right so let's say excellent we are now ready to run our uh prompts are you familiar with open AI python module open AI is dot dot dot I'm wondering if it's slowing down because um uh people are waking up this is one advantage of being a super early bird is I get up earlier than like everyone else granted you know it's always middle of the day somewhere in the world um anyways uh so this may or may not help us with um with this part so now let's see open file save file open AI key text DaVinci 03 tempt that that we can do this up to 1450 that's fine in code all right so this I just cannibalized another um thing um we do not want to clean that up because this is going to be there that's fine all right network error yeah I figured that would happen um okay so let's do a refresh um I have a folder called what is the name of my folder God I have like chipmunk chip monk memory it's hard chipmunk memory um called chunks.text called chunks underscore text um that is full of dot text files um each file um please write a python script that uses open AI module and new module and calls um and uses text DaVinci O3 as the engine we'll say as the model um and uses the contents of the file in the chunks folder to populate a prompt The Prompt is stored as um let's see prompt underscore Json LD uh citation nodes dot text and has a placeholder called um what did I call the placeholder chunk called chunk um in other words open the text file or no um let's see open the prompt file then replace chunk with the contents of the file from chunks.text and use this as the prompt for um openai set the temperature to zero and the token count to 1450. let's see if that works certainly here's a script that should do what you have described I do it differently because I use Windows I know I'm a charlatan um I think it's going to work because so this is the the implications of this if chat GPT knows how to call GPT in theory you can create a machine that can do its own experiments with language models I want to say that again the implication here is now we have a system of a machine that understands the code to call itself well enough and then obviously this thing is intelligent enough to understand results you could in theory have something that trains itself or makes its own data sets or whatever um okay so I like this whoops copy code let's come over here and do this um I guess I didn't say what to do with the response um so we'll do let's see um [Music] and then yeah we don't need any of this stuff so that's fine um and then we need to rather than print um we'll need to save great um the output from the the uh let's see great now response dot text should be in Json format can you please save the output to a new folder called um uh let's see we'll say called AG underscore Json um instead of just printing um we should uh let's see otherwise uh say use the same file name as the dot text file but just replace the dot text with DOT Json um let's see before you write this script can you uh tell me if you understand um give me some pseudocode so I can check to make sure we understand each other if this works yeah well it understands the concept of pseudocode and then it's going to go ahead and write the script Okay cool so let's see if it fixes it um asking for the pseudocode may or may not be viable especially because this code is so simple it might have been easier just to ask for it to um to just go ahead and do it um but yeah so we're also going to need to do um in coding equals utf-8 always need to do that here I'll just copy this and then we'll need the same yep okay cool so instead of printing it we save it so with open as blah blah right so let's go ahead and add grab this comma um I think we're ready so then one last thing though is um what I like to do is we'll keep we'll keep the print um just that way we know what's going on and uh we'll add new line new line new line and then respond to that text um here actually We'll add a little little dmarc okay let's see if this works CLS uh python step O3 open file is not defined ah right because I used um my own function for that um so let me um darn it here let me pause this for a second I'll just copy this function from somewhere else you don't need to see that okay so what I had to do is I had to add this function so what I usually do is because I have a very particular way of doing things and I do it every time I usually write my own open file and um and write and save file function which always does it in utf-8 which is why I have to keep manually adding this stuff because you you pick a default and you stick with it so rather than ASCII or ANSI or whatever I just say everything is utf-8 anyways this should work um CLS and if this works I'll pause it we'll take a look at the results and call it a day because then the last step is going to be getting it all together and visualizing it that is going to be fun come on you can do it um key error text response dot text okay so it didn't like that um all right well let me look at one of my other functions um let's see let's do YouTube generate chapters so I do response choices text ah response so it didn't understand that part but that's fine um okay so instead we'll do text equals and we'll do that strip so then we'll just do text and then right text so that should work and away we go so 95 of this was done with chat GPT if it works there was a couple things that I had to fix manually um but you know it did its best um and what I was really impressed by one of the things I was really impressed by oh there we go um no such file it didn't like the backslash backslash but we got we got some good Json here um okay so then we got to open kg Json replace dot text with blah okay so let me tell it this say okay okay that mostly worked uh but I got an error um it may be important to note um running on Windows here's the error okay so it's going to be fixing this so anyways there's just a couple things um that it didn't like okay it didn't wait is it that simple that it just it what didn't exist oh it didn't create it okay because the previous script it it checked and then made sure it existed um okay so anyways um yeah so there's only a couple things that I had to go outside of this or or background knowledge that I had but for instance when I asked it to use Spacey or nltk um it tried it didn't work but then it suggested use regex instead which is you know I would have done that as someone who's been cleaning up uh bulk data for a long time um okay so then it should add the little thing you know it check if the yep there we go this function this bit right here I'm pointing at the screen you can't see it um if it doesn't exist then make it okay cool so basically all we need is this really you only need to check once you don't need to check every single time but whatever that's fine so then we'll come down here check if it exists all right CLS clear screen with any luck this worked but yeah so most I mean the vast majority of the code it did I just told it what to do now imagine you slap a voice interface so then you don't even have to um type it out right uh use whisper use open ai's whisper hey are you listening open AI I want this I want to be able to talk through the code um okay so now we're saving this stuff it should be here hey look at that okay so this will take a little while to run but we've got good Json it worked so I'm going to go ahead and stop the video for today I'm gonna let this finish running um and then tomorrow we will merge all this together and visualize it thanks for watching