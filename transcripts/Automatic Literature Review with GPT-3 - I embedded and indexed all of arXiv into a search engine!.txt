hey everyone David Shapiro here before we get started I just wanted to say that I have updated my patreon page um with new tiers and um clarified what each tier gets you a lot of folks ask for some time and help and so what I did was I created the two higher tiers will entitle you to a little bit of uh dedicated one-on-one time with me each month so go check out my patreon page and um consider uh supporting me on patreon and if you want any one-on-one time I'm happy to talk to with you and your team and you know just provide a little bit of insight and guidance so anyways thanks for watching and I hope you enjoy this video about automated literature review for scientific papers thanks good morning everybody David Shapiro here with a video I have created an automated literature review engine it is broken up into two parts there is the search engine which searches all of archive with semantic embedding and then the second part is a gpt3 script that will take whatever papers you have selected and create literature reviews for them let's show let me show you how it's done hey everybody David Shapiro here with a video um okay so ignore this I haven't updated it yet this repo has gone through a few iterations um I was talking to my fiance who's a librarian and I was like okay I've got this idea I posted on Twitter all kinds of people gave me all kinds of ideas like research assistant Knowledge Graph interactive data fine tune generation teaching Aid someone was saying like you know it'd be great if you could help write Grant proposals there's all kinds of scientific papers there's academic text and I'm like okay what does all this have in common and she asked me she said what what do you want to achieve with this what's the output and I was like well I want it to be something useful right one it's got to be something useful but it's also gotta like be attention grabbing um you know it's got to make a good YouTube video and um she's like well it sounds like what you're trying to do is a literature review like automatic literature review and I was like that's it so that's what we're doing all right so uh obviously there's a lot of data here I'm not going to get to all of it but let's start with something simple so there's a data set called the archive data set up on kaggle and it is a metadata of everything on archive so 1.7 million articles here's an example of an archive uh paper so this was published uh 2020 battery draining attacks against Edge Computing nodes and iot networks okay super techy right and obviously not everything in archive is like this but we've got all the metadata so like the metadata is basically what you see on this page title date authors that kind of stuff so let me show you what it looks like so I went ahead and downloaded it and you get um 2.1 million lines of Json l and here is what one entry looks like so you get the ID so the ID is what you can use to go to Archive and just plug it in right here so like you describe any archive or any like use this is the base URL and then you can also get the PDF which is just archive.org PDF slash the ID dot PDF download it directly you can get the full text um we're not going to go quite that far yet so we'll see how far we get with this um but what we're gonna do is we're going to index um with a semantic Vector we're going to index all uh all of the abstracts and titles together so I was going to borrow a code from my my quadrant stress test which also means I need to go ahead and fire up Docker with the with the quadrant container so oh it's starting okay so anyways uh as a quick review in case you don't know what quadrant is it is a vector-based um search engine a semantic search engine um it runs in a container it's very small so we'll go ahead and fire this up it is connected to this um this data store um actually hang on I might need to start it a different method um where is the uh here we go um let's see Docker run here we go so I need to start it with this so we'll do that okay because this command actually tells it what storage to map um and it says it's listening so that's good um okay I think we should be good there all right so we've got this data which I've got um so under uh archive so here's here's the uh here's that file it's showing you so it's 3.4 gigabytes of metadata that's a lot of science um it's all in one file that may or may not be problematic we will see um so let's go ahead and just get started um I've got so that's the quadrant demo and then I've got my literature review bot we got nothing going on in here so we're going to um we're going to start a new file and let's just let's just play around with this data let's see what we can get out of it um so let's go ahead and just copy my quadrant stress test and we'll do um we'll save this under literature review bot excuse me I just ate lunch I'm a little gonna wash it down all right so we will do um uh what are we trying to do we're trying to load um we're trying to load it so let's uh let's do um index archive I know how to spell archive metadata dot pi okay so we got Json numpy um stress test right so oh that's what I did so what I might actually do is break it up because in my stress test experiment I broke it up into two steps where I prepared the data and then I uploaded it so that's probably let's go that way let me open up that file real quick um quadrant demo um okay prepared data there we go so in this one we did um yeah we'll use we'll use tensorflow Hub and I've actually got a um Anaconda environment um was it in base Anaconda Navigator so for uh I know I'm going so fast actually I'll like it because y'all y'all all watch my video on 1.5 x or 2x anyways so this is probably perfect speed okay so quick recap if you're not familiar with Anaconda it is a python virtual environment that allows you to have uh yeah here we go so I've got tensorflow GPU um environment ready to go and so what this does is it allows you to use GPU acceleration to run um run your tensorflow uh instance in this case I'm going to be using uh using Universal sentence encoder version 5. in order to um in order to do this so all right so we're going to do um this is going to be generate embeddings dot pi Okay so we'll come so the the file is going to be here so let's just open a new uh Explorer that's not it automus blogger that's not it either we need literature review bot all right so we will we'll have a folder and we'll call this um embeddings I guess and then let me take a quick look at how I did the um the quadrant demo because how did I format the data yeah it's just in a bunch of Json files okay yep embedding and string perfect okay that's probably how I was going to do it anyways um so what I'm going to do is I'm going to take each entry of this guy because each entry is one is one Json um it's Json L right so basically what I'm going to do is I'm going to take the abstract and the title I'm going to smush them together so that they're going to be one long string and generate a 512 Dimension embedding like this guy um so that's what I'll do and we'll see how fast it goes all right so let's do some coding we've got prepared data here this one is in the quadrant demo um and I've got a copy of it here so let's see what do we need to do different we've got Universal sentence encoder five here let me close this one because that's Superfluous we're not doing that one yet we've got um Docker running here I'll just copy this over into the new readme so that I can keep track of that literature review bot read me um foreign just a little reminder okay so that's a sample generate embeddings curl command close that stress test close that we'll keep this open just for reference okay um yes generate embeddings okay we can delete this comment files equals OS Lister uh actually no so we need to go ahead and change that because we're going to be opening this and we're going to do it as split lines that's going to be oh that's going to be messy um Okay so archive equals open file um that was C archive um and it's called well here I'll just copy copy this guy all right so first let's make sure that um that we can just open the file as is and do split lines all right so that means archive will be a list of Json strings for everything that we want so then we'll do print length of archive and then we'll do exit so let's just see if this works let's see if it works if it blows up because I mean I've got 32 gigs of RAM on my on this PC so it should be okay um and then let's see literature review bot all right so we'll just do python generated embeddings oh right I don't yeah okay so what it's doing right now oh here let me make this bigger yeah I'm not surprised that that blew up so let's do properties so that you can see what I'm doing all right so what it does is saved model does not exist this always freaking happens where um for whatever reason it can't find the model but anyways what I need to do is come in here and start it with um GPU come on wake up what's the matter of you there we go all right so we'll start this conda environment all right so CD CD uh uh literature review bot python generate embeddings so now what it's going to do it's going to have to download um Universal sentence encoder version five and so oh so something to tell you Universal sentence encoder is Google's um deep neural network that does it all it does is it generates embeddings um and they're 512 uh Dimension embeddings but they're ideally for something that is sentenced to paragraph length and that's about the length of an abstract it might be a little bit long but this is this is enough to get a good solid embedding and one advantage over something like gpt3 embeddings is this is free and it's also stupid fast so there we go um yep tensorflow libraries deep network with the following instructions AVX and avx2 build it but okay so it's spotted here's my Nvidia GeForce RTX 270 there we go so it's still loading the model the what I'm going to use for the embedding in the background so then this is the length so actually I probably should have had some more output so 2.131 million um things so it loaded it all just fine actually I probably should have had um had my performance monitor up so I could see how much CPU and memory it took let's see what's using all my memory right now Notepad notepad and the uh and the um the uh quadrant quadrant think of my job all right so first let's go ahead so I'm glad I'm glad that python was able to um to run that um I think I have here let's close this okay so that was a big chunk of no I still have something running python what else is running python I got that guy and that guy okay whatever all right all right so we want something that's going to end up looking basically like this um I'm not going to worry about the start time let's see articles loaded We'll add a little bit of debug output oh I need to set an alarm because I have a meeting here soon I've got an alarm set all right there we go fix it in post all right don't care about times print files overall start time whatever for file and files no so we'll go say for article in archive we don't need the chunks we just need the title so we'll say um let's see info equals uh Json dot load loads load s so because that that tells it to load string so we'll say article um so that means info should then contain something like ID um title and Abstract so let's just start there um okay so print info dot no not DOT it'll be title and then info abstract and then we'll do exit here all right let's make sure that works and also for now I will comment out tensorflow hub so now it's loading let's see how much how much juice we use oh yeah look at that we're chewing up memory name archive oop I misspelled it don't do that kids but yeah we surged up to um yeah we use most of my memory so because I'm running I'm running an index engine and loading a three and a half gigabyte file um and I'm running a um uh a uh well I guess this model is going to be in in my GPU so yeah I'm doing a lot on this Workhorse all right let's try this again very high power usage yeah boy there we go all right it's going up going up going up going back down Okay cool so articles loaded 2.1 million calculation of uh prompt diphoton production across sections at tivatron and LHC energies a fully differential calculator Okay blah blah it works good um yes we are in good shape so so far so good so now what we need to do is um chunks equals wrap text wrap okay probably what would be faster is if we pass um pass it a list now whatever um because I don't know how many how many how much you can pass it once but we'll just comment that out so we've got all the information we need so then we'll just do embedding equals um uh we'll do a list that contains um well here the uh string equals these two info title um plus a nice little space plus that guy and we'll then we'll say string equals string dot replace actually I probably need um import read and so then we'll say so what I'm doing here is um because there's a lot there's a lot of Superfluous new lines so I'll say string equals re sub and we'll replace backslash S Plus with just a space and we'll do that on string all right so this will clean it up make it a nice clean string and so then we'll do embeddings equals embed string and so then the vector will be embeddings numpy to list so then we'll get that as just one and then all we do is save data so the save data file we pass it a payload and it will save it into we'll call that the embeddings folder I think that's what I called it this is going faster than I thought something is going to go wrong wow I've got a lot of tabs open okay um yes so embeddings embed so the vector equals that so the string equals so we'll just call this the article actually I guess it's more like the abstract um no we should yeah so we'll we'll have the title we'll keep the title separate uh so the title equals info title because why not um no reason to smush it together but we're just going to index them together um so that way you search when you search you're searching the title as well as the abstract and then we'll do abstract is info abstract okay and then I really need to plan on this bombing like because if it blows up or if it fails I'm not gonna do any time keeping um but I what I will do is we'll do one exit here real quick go ahead and load this and let's do a quick run and this should spit out a folder or a file here and then we watch the GPU catch on fire there it goes GPU memory is loaded up so it has loaded the model articles loaded there it goes and now we'll watch vectors is not defined oh my God embedding vector okay that's fine it's fine um that should be good let's try again and then we will fix this in post as well here it comes yeah yep yep and then it exits and we should in theory come here where is my folder embeddings why is it not showing up there we go okay hey look at this so we've got abstract um it did not replace oh I didn't okay yeah I need to replace both but we did get it okay so we need to do a little bit more cleanup so you got the backslash n next so I want to clean that up and just make that a space and I also want to trim these um okay so what I'll do is let's fix it we can fix it go away silence I get you that's how old I am I remember when uh Jeff Dunham was live and popular I watched him while hanging out in the dorm room of one of my friends many years ago okay um all right so first what we're going to do is we'll just do title equals info Dot no info.title dot strip and then we will do we will also wrap that in resub yep and we'll do the same treatment for abstract abstract so then we're only touching it once because here's the thing when you're doing something a million times literally millions of times you kind of want to if you if you have to do a an operation on it you do it once if you're only doing it a few times you can afford to be less efficient um but for for this in this case you want to be as efficient as possible and so um what we're doing here is we're going to do idle Plus space Plus abstract abstract and so we're doing the we're doing the regex on the title and Abstract once and then we'll also just because we called it out into a variable so rather than like modified a couple times we modify it once and away we go um so this should be a little bit better all right all right try it again and then okay so while this is loading let me tell you what we're gonna do so by having a semantic search engine that allows you to to measure the semantic similarity between um basically titles and abstracts across a million articles it will allow you to find things that are similar that you might not find just through other conventional means so basically what you'll put in is you'll uh what I'll ultimately do is say there's an article like you find one that's like yes give me every other article like this one and it will find you every single article like that one um and from there there's all kinds of things that you can do you can create a semantic meaning graph you can do blah blah blah um okay so we got some more embeddings so 238 so this should be the next one there we go that's a little bit prettier see how there's no there's no uh uh markup or anything so abstract title and oh so another thing that I did let me show you this um with the with the file save I say out file ensure ASCII equals false so that allows it to use utf-8 which is nice sort Keys equals true so what sort Keys equals true does is it always puts them in the same order which that's not necessary for the machine but it's it's nice for the human so that you know that the abstract will always be first because and I don't know what the logic is but sometimes when you save a dictionary out to Json it'll be all garbled up so it'll always be abstract embedding and then title uh which is nice for us um so then you see title here at the end calculation of prompt diphoton production cross sections at tivatron and LHC energies wow okay um so then you put this abstract into the search engine that we're gonna eventually produce and uh we'll see where it goes anyways so we've uh we've got this so pretty much all I have to do is remove this exit here and just let it go and it will produce um uh millions of these so what I'm going to do is I'm going to add one last thing um try accept um and then what I'll do is errors and so errors equals list and then if it blows up I will just say um let's see error errors dot append article so that if there's any problems I can at least have an archive of the ones that that mess up and so then what I'll do is I'll at the very end I'll do save data and so I'm going to save errors um wait no I uh it's the payload I need that to go to a different place though so I'll just do whoops so I'll do with open and this will be errors.json there we go as out file um we'll do errors okay so this this is just so I'm going to save however many blow up and but we'll just let it run all right so CLS um I actually probably ought to have some kind of output here we'll just print the title there we go that way I'll know it's working because there's nothing worse than just a command prompt they're blinking and not telling you anything granted I guess I could check in the embeddings folder but it's nice to see it actually working okay so let's get this running make sure it works and then I'll pause the video and we'll come back once it's done and actually probably I'll need to come back to this later um and we will continue this work later tonight or tomorrow but for you watching it'll just all be one continuous video um but at the end we will have something that you put in an example article and it will spit out a literature review that's the goal at least all right let's see how hot this gets all right we got the GPU spun up we got our memory climbing oh this is going to take a while because even even at this speed it's gonna oh boy it's gonna take well to do a million uh what 2.1 million of these yeah so we're only at 200 300 okay this is gonna be running for a few days I wonder if there's a way I can speed this up anyways you don't need to watch all that um if I'll if I find a way to speed it up I will come back and show you okay so um yeah it wasn't going fast enough for me so this should work um but yeah so what I was working on was I made the prop well all right so a few things first loading 2.1 million or 2.3 however many millions of chunks are things and processing them one at a time is slow especially when your model can do batching so I'm creating batches of a thousand and we're feeding it in in chunks and then we're processing each chunk as a batch and you can look at the code if you want but it's it's pretty straightforward but the the the the secret sauce is here is where um you do the 100 or a thousand in batches and it should go a little bit faster so instead of um uh let's see what just happened oh I ran out of memory okay so I can't do batches of a thousand um yeah that's not working okay so that's too big so let's go back down to 100 because that did work um so we'll do uh that of a thousand so there's no embeddings and this is filling up the error directory yes okay so delete that okay so we'll do a batch of a hundred so rather than doing one embedding at a time we'll do a hundred at a time um yeah and we'll go from there uh okay so clear that out so generate embeddings so the reason that I'm doing this is because um I was doing it one at a time and I was like okay you know I've got a small GPU um but it was like 36 hours that it was going to take and I'm like I'm not going to let this thing run for 36 hours straight I can do better than that I'm gonna challenge myself to do better so I'm gonna see make sure that this works and make sure that it's filling up my um not errors directly my embeddings directory um oh and another thing that I forgot was you need to have the um you need to have the article ID in there if you want to be able to find the full article later so all right so there we go that looks a little bit better so we have yeah look at that so doing it in chunks rather than um rather than doing it one at a time we're down to 3.3 hours so it looks like it'll take about three hours to do it in in chunks this big um do I want to hey I'm going to be bold let's see let's see and okay so here it's already processed 4 600. are these articles by doing it in batches of a hundred rather than one at a time so yeah this is this is how you do it and there's the ID so you got the article ID so you can go get it from the original the original location but let's try let's just be a little bit Bolder just just a little bit delete these because if we can do them once we can do them again so let's do a chunk size of 300. and if this works you know so we're at down to 2.8 hours um remaining thank you CLS all right so if we do chunks of 300 I don't think it'll be three times faster and it also might run out of memory but we'll see um so the batch size has to do with how much memory your GPU has and so you know you go to Performance click on the GPU tab it'll pre-load all the memory as it's spooling up the model but then there's a certain amount of I'm not sure how the inner workings are but so you see it shoots up to use like seven and a half gigabytes all right let's see what happens if we see if we see it blow up with that you know oom so now we're down to 7 000 chunks total rather than 2.1 million individually okay so we're doing larger chunks the time is coming down and so what I'm doing in the background is I'm averaging out like how long is it taking overall so we're down to 3.5 3.4 and so it's because there's some inefficiencies or whatever you're you know like the time bar however long it's going to take so it looks like you don't get much faster by doing larger chunks because it's down to about you know uh just about three hours so it looks like that's kind of the optimal because you know you do bigger chunks you marginal whatever okay so two hours much better or three hours much better than 36 hours so it's 12 times faster if not a little bit more because you see this number still coming down pretty quick all right so we will uh go ahead and let this run and then pretty soon we will have 2.1 million embedded archive things this is this is going on data hoarder I think the folks on data hoarder will love this all right and we're back um okay so quickly to bring you up to Speedos to what I have achieved and what I'm working on next um let's see so we well here let me show you the code um okay generate embeddings so I do things in stages modularize it so this is the final form of of the generate embeddings and so we load our embedding engine which is um tensorflow hubs Universal sentence encoder five um you've been watching so you know that I'm just bringing myself back up to speed because this is you know the following day um okay so we break it into chunks of 300 that took the processing time from 36 hours down to less than three so we got a factor of 12 more than 12 Improvement um and then I also just broke it out into a chunk now I had a try accept so that it would handle errors and we actually ended up we did get a few errors um so we had four chunks so that's 1200 articles that didn't get um indexed but when you compare that to um you know the 20 20 2.1 million articles that did get indexed um or embedded rather we haven't done the indexing yet um we'll say that that's a pretty good pretty good uh uh average um so I'm not going to worry about those um you know yeah 1200 articles is nothing to shake a stick at and they could be 1200 of the most important articles but chances are you know whatever um if you want to go back through this and fix it yourself feel free okay so looking at the directory um let's come back up here so I took all the embeddings which is about 29 gigabytes um and I seven zipped it so that it's now nine gigabytes and so that you don't have to redo this yourself I put it up on kaggle so the the URL is kaggle.com dataset slash Lieutenant Commander Data because I'm That Kind of nerd um and slash archive embeddings so you can download this yourself and um so there's two things that I'm going to do from here well three actually so first we have to um I've got to update this and I'm not going to show you I'll just do this and show you once it's done you don't need to watch me code this stuff this is boring um so we're going to uh index all of this into quadrant and so quadrant is uh is a semantic search engine it's very lightweight and we'll see how well it handles um you know 2.1 million articles uh yeah this will be a this will definitely be a stress test for that because I only tested it I think when I did my stress test I did like uh well I did less than that let's see quadrant demo data um I did 88 000. so we're doing a few orders of magnitude more data now granted it handled that much data very very well it only took about I think uh less than three minutes to index all of that um so yeah that's gonna be next and then once I've got it all hosted in a search server we will then I'm gonna build a a flask web client to do the search for us so that way I can you'll just there will be a payload window where uh you put in like you know find me abstracts like this and then it'll give you like the top 50 results or whatever um of similar articles um I mean heck we could even do the top 500 and just you know scroll down through it so that way you can just see that the search works and we can look at how good this method methodology is um and you can also see how fast it is and then the last thing that I'm going to do is actually do like a um so we'll do like to do um actually generate a literature review so like given like basically the excuse me the input will be you give it like an article or an abstract and it says okay find me every abstract like this one you know within a certain boundary um and then generate a literature review based on those um yeah so that's where we're going from here go ahead and save this just so you can see what I'm doing um and then we'll say generate literature review dot Pi okay so I've got to do this this and this we'll see how far I get okay so I got it running um we you can see I've got it going up in batches um and it's just kind of running in the background um I've got a doing batches of 256 at a time um and we're at 22 000 out of 2.1 million um the time that it's going to take keeps slowly ticking up we're at two and a half hours now um part of that is the delay of uploading the record so it's like there's a little bit of marginal cost baked in but then there's also um I'm not sure but I think quadrant slows down a little bit the bigger that it goes the bigger that it gets but we'll see if this takes too long I might have to try a different thing um but yeah so so far so good um actually what I might do is update it so that it's not giving me that much output because there's a little bit of marginal overhead for each output and I also might do bigger batches yeah let's do that so let me show you what I mean so we'll cancel this so this this script what it does is it starts the client you recreate collections so this I looked up I looked up what the uh what the client does so recreate collection it dumps whatever's there and recreates it so if you're doing testing it's great you just start from scratch and then I updated this function load data so it just starts here it gets all the files you instantiate vectors and payloads and I do a little bit of numbers math and so then what I wanted to do was let's just have all this come up here so if the length of vectors is greater than or equal to let's do chunks of a thousand at a time um so then we're going to be sending um when we we do send a batch um we're sending a batch of a thousand vectors and a thousand payloads up to the client and all that this does is uploads records and this will send it in chunks of 256 at a time and it'll allow the machine to figure out the IDS in the background uploads it to the archive so yeah so this will hopefully keep it a little bit faster because there's going to be a little bit of overhead cost one of even just doing a print screen and so we're only going to be do we're only going to be calculating times and stuff every 1000 rather than every single Loop but then so all we're doing here is for for every file we will uh we'll increment our counter and then we will open the Json and we'll have uh we'll you know info equals json.load in file and then we're going to spool up the vectors and payloads because for whatever reason with quadrant you have those as separate um separate uh lists and you send them independently excuse me I gotta sneeze I'll fix that in post um anyways sorry uh okay but yeah so anyways let's restart this and see if we're going to go any faster uploading records so you can see here it looks like it might actually be a little bit faster because instead of creeping up to like two and a half hours it's sitting at 1.17 and it looks like it's holding steady um 1.18 yeah it looks like it looks like this method's a bit faster um cool and I can probably even remove the uploading records because that's just extra noise like of course we know you're uploading records that's what we programmed you to do okay so it looks like this will take about an hour to run and then in the meantime we can see you know vmm is using that I wonder if there's any um volumes yeah so this is the Vault we'll see the volume size go up um so this is the actual storage that quadrant is using um for 98 quadrant is really efficient I'm not sure how it what kind of compression it does so we'll see what that gets to at the end anyways uh probably what I'm going to do is while this is running is I'll work on the flask app and I'll give you an update um here in just a minute okay so I've made some progress um I've got the search server up and running it's pretty straightforward um let's close out that comment um so basically it's just a super simple flask server I've got some some basic HTML um actually I realize that I need to close out the HTML so then we'll do let's see HTML equals HTML plus and then we'll do um do HTML uh close equals and we'll say um slash body and then slash HTML it's not strictly required because most browsers will still render it properly but if you want to uh be nice you will you will do this HTML plus HTML close okay anyways um yeah so here it is it's up and running um it uses the same embedding engine um that is running uh that generated the 512 Dimension um whatchamacallit embeddings vectors it's still uploading um so but we're at you know to about two and a half hours left and you see it's pretty stable it was creeping up but now it's creeping back down um so I've uploaded half a million um and yeah it's a really simple thing so you just put in a search term so like you might say like pancreatic cancer in mice and then you do a search that was faster than before um yep and so semantic similarity 0.17 so that's really low so that must mean that we don't have anything in here or that maybe um but yeah so still you're getting like proton cancer therapy let's see if it mice so there's nothing pancreatic yep so there's nothing uh well here let's just do cancer why is this search so much faster um okay automatic tumor segmentation whole images of the pancreas so it's interesting that that didn't come up so I'm wondering um so basically I don't know I don't know I'm using a relatively low dimension thing but you see how fast the search is um I have no idea why it's so much faster right now um is this still running okay that is still running um but yeah so um and also I have no idea how this is only still at half a gigabyte um the container that's running it it says it's using um like if I click on this volume it's it's using it it's the one that's running see it's quadrant storage or quadrant storage um I'm afraid that it might be running um somewhere else anyways so let's say let's see if you copy so like I've got one that I like so okay let's just copy this and we'll put that in there and we'll do a search and you see how the more information you give it the better it gets and that's because the embeddings that um are that are are that I use include the title and the um and the abstract so the more information you give it the closer the embedding will be to what you're looking for and you see how these scores go way up um and so it's the the whole idea is if you find one abstract that you like or if you can think of kind of the title of what you're looking for you just kind of keep typing and you give it as much as you can and it will generate a more accurate embedding and so then I've got it where it just you know here's the title here's the score and then here's a link directly to the archive so it'll just take you straight to it I suppose I could have had a little bit more metadata in here like the date and authors and stuff but I didn't include that like because I was like why duplicate effort like I just got the bare minimum that was required which is the title abstract and ID which allows you to come straight here and then you can say okay this is when that was um but yeah so then like I did uh what was it like uh Large Hadron Collider is that one where LR2 um Higgs vacuum Decay from particle collisions and so it's just like okay cool everything related to colliders and etc etc um bigger better faster more at the LHC um and you see the the semantic similarity score of 0.54 um and yeah so I think probably the next thing that I'll do is I might add a tab or another function where it's like okay you know if you have the results that you want like generate a literature review but you see how fast this is and and the quality of the results and this is using a very low dimensional Vector um relatively speaking because 512 is pretty small you can go up to 12 000 with DaVinci that would be really expensive to embed 2.1 million things with da Vinci but anyway so let's just grab this um and so we'll do here actually let's copy the both the title and the embedding and then we'll just put this in our search um okay so this gives us even higher results so hadron spectroscopy structure so the if you put if you put the title and the abstract for a paper um you'll you're going to get all the most similar papers um and so then it's just you know you saw how fast that was and you don't have to put in keywords you put in this big huge thing and it will say okay let's find let's find the payloads that are the most um semantically similar um which could lead to like this is just the first step in automated research right because if you can find all the right research then you can you know go from there there's obviously a few keywords like hadron Higgs LHC that keep popping up but you you see that one advantage that you get with semantic similarity is that you're not looking for keywords um you're it will actually kind of get an understanding and with a with a low dimensional embedding like 512 you lose a lot right because there's so much specific domain information like you know the Randall sundrome model I guarantee you that a low dimensional model or a low dimensional um embedding doesn't fully understand that but we keep talking about equations and motion and black holes and standard model and so all of these things go into creating that 512 Dimension excuse me uh embedding I'm out of coffee that's unfortunate um anyways so we're off to a good start and you see uh the top you know 0.7 sorry the top ten they start at 0.75 and go down to 0.62 and so what you can do is uh easily just come in here to the search server and change the um change the result query from limit for to 10 to let's say 100 and so let's do that and then restart our um our server here and so then because obviously if you're doing a literature review and you might be looking for things that are a little more distal from your your core search because if you're trying to Branch out you don't want to focus on just the things just the closest cluster you want to like okay let's find everything that's vaguely related and so then let's do um let's see if let's see if open AI um gpt3 and one thing that I found that's interesting is that um oh it it the fewer Search terms you you use the lower the the slower it is I think it's because it has to search further whereas like if you have if you've got a good Vector it's just really fast um so let's do deep neural network computer vision so that's much faster good lore that was faster okay um okay so then let's just grab a whole abstract um from one of these guys and so then you scroll down and it's like okay cool we've got all kinds of stuff and you see that there's kind of there's going to be a whole lot that are at like point five point four um so you can just keep going um yeah and you'll find everything that is vaguely related um and it's all kind of in the same space um so there's a few things that we can do to improve this which I'm not going to do right now I'm just doing this as a proof of concept um is you can do you can use a higher dimensional Vector such as Ada from open AI which is a 1024 or you can go up from there right but you're going to spend a lot of money whereas Universal sentence encoder version five is free and it's fast it only took like three hours to embed all all of these um and so then once you're on the page you could search for you know open AI okay there's no papers from open AI here um yeah but and you see how fast it is and it's just okay cool and I've got it nice and simple so it's it's easily readable you've got the abstract here you got a link that's very obvious um I'll probably shorten this because that's a little bit that's that's a little bit of visual noise um so in terms of we'll just do um score uh yeah because you reduce you reduce visual noise so that it's easier to parse because this if it's the same thing it doesn't matter what you really care about is this um so you know but you've got different um sizes colors and shapes to really draw your eyes so you've got a really reliable pattern so this blue helps break up so you know like okay there's a link if I want that one I just click there um honestly actually probably what I ought to do is have oh I can simplify it we can do the title is the link and then just get rid of the link there because why duplicate it yeah let's do that real quick okay so we will move um we'll move this up to let's see a haref and so instead I guess here I'll do a href okay and then we'll grab payload ID and title there we go and so now we'll have a little bit less noise so now now it'll be this will be the link so you just click on the title instead of having a separate link which is smaller so that's a smaller Target so I have title score and then the abstract and that's that's it so it's can't get much simpler than that and so this is the most like cut down basic Search tool you can possibly have um okay so then let's go back in let's wait for this to finish loading come on there we go okay so do a quick reload all right cool so now you see the format's a little bit different where you've got this it's a little bit it's not quite as pretty I might change the the style so that it's well I guess that's okay and then you got the score and then the abstract um so there's a little bit less noise I don't know if I like this as much I don't know what do you think um it is nice to have the score just here so you can get a quick view of like okay how close is this image quality assessment I guess when you when you pay attention I wonder I wonder if it's the underline that's making it a little harder to read anyways I'll fiddle around with it um we've still got a couple hours left so two two and a quarter hours so two hours 15 minutes of these uploading um probably one reason that we're not getting uh good results is because it's not done uploading um so we're getting the best that we can with what's here um but yeah so uh okay I guess I'll stop the video here we've got this um what am I working on next oh generate literature review um you know what I'll probably just integrate the literature review into this server I think that's what I'll do okay okay we're still making progress um however I was looking at archives bulk download stuff and you know Open Access yes but they also ask users to play nice and you know be be good with harvesting so I'm not going to add a bulk download function to my web interface um this is this will run strictly locally because I want I mean you know they're providing a huge service for free um so let's not abuse the you know the the free service but what you can do is once you use this tool to find the papers that you want you can go manually download them and so what I've done is I'll I have a folder so you just accumulate whatever papers you want reviewed in this folder and so what I'm working on now is um is a script to do the to do the um literature review part separately um so I'll take that out of the web interface it's pretty straightforward so here's the prompt it just says summarize the following paper for literature review paper literature review summary and this is what it looks like so you take a big chunk and then you have here and it does a really good job of just kind of distilling it down so what I'm going to do is I'm going to break it into chunks and uh well here I'll just show you so I've got it here uh let's see so we take out the print and so then we do chunks equals text wrap dot wrap and then we'll do paper and we'll say 6000 because that's usually about the size that you can get and we'll say um okay uh let's see result equals that and so then we'll say four Chunk in chunks um prompt equals open file and we'll say prompt summary dot text replace paper with chunk and we'll do um summary equals gpt3 completion prompt and so result equals result plus and we'll say add a nice little space and we'll add summary okay so then for each of these though there will be a um a literature review um for each paper um and then we will just save it out to file uh okay so probably the way that we'll make this look um because we'll see output equals list and so then we'll have uh so then we'll say info equals file and we'll do file and then um summary equals result okay so that will be that and then output dot append info and so then when we're finally done at the very end of all things we'll save it out as a Json actually no let's not save it like that we will just do um we'll do uh um uh I'll uh text equals we'll do new line new line dot join output um no because that'll still be this like formatted like this uh okay why don't we do it this way instead then so final output will do text so output equals um output plus and we'll do new line new line and then we'll do um that'll be the file name and then we'll also just do the summary yeah that'll work okay so then we'll do file and then resulting summary um yeah because then you can then you can do whatever you want but the point is is that it will it will give you kind of a compact summary with the file name and again there's lots of little things we can do to clean this up um okay so then save file and then I have file path and then content so the file path will be um literature review dot text and the content will be the output all right and then let's run this real quick so I've got five files that are related oh I guess it's not um yep I forgot to put the logs no okay so first we will print file and then we will print results and we'll let it go okay let me come back here GPT three logs do this every darn time all right so I've got a handful of files that I want to generate a literature review for and so what it's going to do is it's going to go through and extract the text from those PDFs and then break that the whole chunk or the whole PDF into chunks and then it's going to do a quick summary for each of those should be running in the background yeah here we go so summarize the paper there we go so it is running oh here we go yeah and so what it'll do is it'll give me the file name and then the the full summary which you can then use to just plug and play into your paper and obviously there's there's all kinds of formatting and official stuff and I'm not a research scientist but this once you get this far you're uh it's pretty easy to clean up um but yeah I think we're almost done so this is exciting um now obviously like this has this is only halfway done it's got an hour and a half left but you can run this on your own to index all of archive yourself and so actually I was thinking through this um this is a pretty useful service so um tell me what you guys think should I should I set this up as a um like an actual website like should I try and Market this and monetize it and um would that be valuable and maybe I could do a Kickstarter uh or or something just set this up and and so then like you just plug and play and you've got your own literature review engine um so let me know what you think in the comments um but yeah I'm gonna call this I'm gonna call this done we'll wait for this to finish spitting out um you know it's it's review um but yeah