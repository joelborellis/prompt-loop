morning everybody David Shapiro here with your daily state of the industry update um as often happens my news feed helpfully handed me this this morning Hydra attention efficient attention with many heads um it was a study just released with uh from Georgia Tech and meta okay so the abstract is pretty straightforward says while Transformers have begun to dominate many tasks in Vision applying them to large images is still computationally and difficult a large region for this is that self-attention scales quadratically with the number of tokens which in turn scales quadratically with the image size on larger images such as 1080p over 60 percent of the total computation in the network is spent solely on creating and applying attention matrices we take a step towards solving this issue by introducing Hydra attention an extremely efficient attention operation for vision Transformers paradoxically this efficiency comes from taking multi-head attention to its extreme by using as many attention heads as there are features Hydra attention is computationally linear in both tokens so rather than quadratic it is linear in both tokens and features with no hidden constants making it significantly faster than standard self-attention in an off-the-shelf vitb16 by a factor of the token count moreover Hydra retains high accuracy on imagenet and in some cases actually improves it okay so that's that's this in in other words instead of having one two you know or you know probably a a a square um number of attention heads like one two four eight sixteen they just go to have as many attention heads as possible and what they found was that you get uh you get efficiency increases so I'm gonna go ahead and Skip down to the results um to 4.4 results so we would present our final accuracy in flop count using Hydra attention in Tab 2 which I believe is right here yep so you see accuracy um they're able to with just a couple of Hydra layers they improved accuracy a little bit but then you look at the when they added more layers the total floating operations per second went down four percent and the speed was increased by 10 percent um and then this is where I'm not quite sure that I get it um because a little bit further down they say that that's 197 times faster I think that's because it's in parallel so maybe each head is faster but then when you have multiple heads you're working in parallel I'm not quite sure I haven't read it I haven't done a close enough reading yet but anyways we present our final accuracy in flop count using Hydra and Tab 2 compared to standard OT et cetera Etc and other otd methods on imagenet 1K Hydra attention achieves 2.54 higher accuracy compared to other otd methods when replacing all layers and when replacing fewer layers Hydra attention can strictly outperform the Baseline standard attention model with two layers accuracy increases okay so they're just saying what the uh what the the thing does okay larger images to explore whether Hydra attention retains these gains with more tokens in Tab 3 we fine-tune the backwards replacement model from uh from figure four at 38 384 pixel resolution for 30 epics using the hyper parameter suggested in 31 this results in a model almost three times the number of tokens which should both accentuate in different and difference uh both accent both accentuate the difference between otd and ot2d attention um and indicate whether the Global Information propagation strategy of Hydra attention is effective at these higher token counts and indeed in Tab 3 we see the same Trend with the 224 pixel images Hydra attention can increase accuracy by uh almost 0.6 percent and throughput by Point by 4.1 with two layers or keep accuracy the same and increase throughput by 15.4 limitations okay but Hydra attention is 197 times faster than standard attention so this is I'm not quite sure where they got that other than the only other place it appears here so I'm not sure if they're saying like Global time was that much many times faster not sure but if it was 197 times faster that is a huge order of magnitude Improvement and it's not entirely surprising because we saw that kind of improvement between Google Universal sentence encoder version four and five where it's literally a hundred times faster just with an algorithmic improvement such as this so if if I'm misunderstanding this paper I'm sure someone will comment anyways but I you know while this number does sound extreme it's not surprising to me okay so so why is the maximum flop count reduction only four percent well it turns out that with vitb 16 uh with 224 by 224 only 4.1 percent of the total model flops reside in creating and applying attention matrices with Hydra tension this is reduced down to 0.02 percent wow okay essentially eliminating the cost of attention in the model while this does result in raw throughput increase of up to 10.2 percent we can clearly do better of course the story changes if you increase the image size we repeat this computation for different size images and the computation of standard attention balloons all the way up to 58 with 100 with uh 1280 pixel images while Hydra attention remains negligible at 0.02 okay so basically this makes the the first step computationally uh inexpensive no matter no matter what size um we test 3 384 and speed up to Hydra tension up to 27 increase however further work needs to be done to validate Hydra attention on tasks that use more tokens um though in those tasks we'd be comparing against the local window uh attention used in vit I'm not sure what that is um which already which has already shown to be effective for large token regimes in images compared to local window attention Hydra attention uses only four percent fewer flops um okay in general the usefulness of Hydra attention allies in its generality local window attention is a powerful solution for dense image prediction but quickly becomes cumbersome with token sparsity such as with masked pre-training or token pruning we leave this for future work to explore conclusions in this paper we introduce Hydra it can work in tandem we've taken the first step in showing that Hydro tension can work at all and hope future work can explore this in more token intensive domains okay so what does this mean one thing that I suspect that this means is when you look at the difference between other Transformers such as Google's Universal sentence encoder version four and five and it suddenly becomes much much faster and computationally and expensive that means that it is ready for Universal deployment now it sounds like this is not quite ready for Universal deployment but it is a big step in that direction now imagine if all image based Transformers can suddenly become this much faster and this much more efficient imagine if your Dolly and mid-journey and stable diffusion Generations instead of taking 15 seconds to 60 seconds they take half a second or a tenth of a second and then so that's just image but if you look at the uh on r slash machine learning where the author post one of the authors posted this someone asked if this could apply to language models and they said it probably could so imagine if this allows us to greatly expand the window size of gpt3 and other language models What If instead of you know uh 2000 tokens or 4 000 tokens this allows us to go up uh in window size by a factor of 10 or 100 or alternatively what if it accelerates um the the inference instead of taking four to five seconds per per uh uh per uh inference what if it takes 0.4 seconds or a few milliseconds and so then if you have a language model that can perform at human ability but it Ultra like superhuman speed then like even if it's only human equivalent but it's many times faster than people that's still like superhuman because it will do much more work in the same amount of time as a human now they're still energetically less efficient but the fact it that they're faster and they can work tirelessly kind of might bring that down to parity with uh with humans because uh when you talk about energetic efficiency you have to also think about time because if something takes you like years to do you know your your brain was using 20 watts over that entire period of time but if the machine can do it in an hour even if it uses a lot more energy during that hour it still might be energy you might reach energy parity sooner so taking a big step back this is when I when I frequently talk about like oh we'll find more efficiencies this is the kind of paper that I'm talking about where um because this space is so new and huge and it's growing all the time we are constantly finding new efficiencies new algorithmic efficiencies um so one of the things that started this whole thing was uh was an advancement in understanding loss functions I think that was back in 2014 or 2015. that's what kicked off this entire Renaissance of deep learning was figuring out how to optimize larger and larger networks and so that one little algorithmic Improvement um and then you know subsequent Downstream improvements enabled everything that we're seeing in artificial intelligence today and then and then seemingly small algorithmic improvements like this which are just small structural differences right like let's use as many heads as possible Let's do an experiment oh hey look we cut we cut the uh the cost down from 58 to 0.02 percent that is a gravimetric shift that is a seismic shift in the computational uh cost of some of these things and as a systems engineer it doesn't surprise me um you know it's like if you have a single threaded application or you have it running on 300 threads right if you can if if you can divide the work up like that go for it so in computation there are four food groups there's CPU or processing we also have GPU so whatever you have you have a machine that does the processing that does the math you have memory then you have storage and network so you have those four basic food groups of computation now for a model that runs on a single GPU you're going to be constrained by that that gpus processor speed and its memory but depending on how big it is you might have to be reading and writing to Storage storage is a thousand times slower than memory and network is is a thousand times slower than that right so if at all possible you want to keep all all uh computation bouncing back and forth between your processor and your memory and with a GPU you have those things combined into one on a single card right now if you have to switch to normal CPU and RAM because GPU has vram right it's got its own built-in uh video RAM so you keep it there but then if you have to switch to CPU in memory you have a slower CPU that's not purpose built for those Matrix operations and then you also have general purpose system Ram which is usually a little bit slower right but then if you're doing really big things and you have to write to disk or swap or whatever that's going to be much much slower and so then you'll end up like so in terms of speed GPU is the fastest by far CPU and RAM is the next uh next one down and then if you have to swap to disk that's even slower but then if you have to share over networks granted you can have specialized network connections to Cluster gpus but if you don't have that kind of specialized Network clustering going going over the network is even slower because then if you don't you're not accessing storage on your local system you're having to package up everything communicate out on a network protocol share data and every all the overhead that networking provides now granted networking can be very fast fast but you know your fastest back plane might be 10 gigabits a second between one host and another whereas you know you might have a thousand gigabits a second between your local storage if you're using ssds so again right you know it's 100 that's a factor of a hundred times slower throughput anyways I'm diving off into other aspects of Technology my point is that it does not surprise me if you parallelize an operation it goes much faster but also depending on where the constraint is if you have it so think of it this way another way to think of this is imagine you have um one line of cars going down a 16 Lane Highway that's what you're doing before if you have just a single head of attention and so if everyone is constrained to one lane and you have a 16 16 Lane Highway worth of traffic going down one lane it's going to be infinitely slower than if you have all 16 lanes open with 16 Lanes of traffic and so that's why mathematically like when you squish it all it's not just that it goes 16 times slower when you have a traffic jam like that it goes like a thousand times slower right because you know you're not it's not like you know the speed limit's 60 miles an hour and you divide that by 16 and you're going four miles an hour when you go down to one lane of traffic no you go down to like you know a mile every like three hours because it's more complex and I I'm not a math expert but basically traffic jams create bottlenecks that slow the whole system down uh disproportionately and that's kind of what we're finding here is going the opposite way of oh hey we open up more Lanes of traffic and suddenly things are going faster than you'd expect so that's kind of how you see this and this is something that is true in systems engineering which is what I do for my day job so anyways I'm rambling now um thanks for watching this has been your state of the industry update um like And subscribe and consider supporting me on patreon have a good one