morning everybody David Shapiro here with your daily state of the industry update as often happens my newsfeed helpfully supplied me with today's topic I think it is a very timely topic because I have been diving more into alignment so today's paper um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me um it's a relatively short paper at least the part that's published is uh 12 Pages it's I think it's much longer they just cut some out for um for uh for internet publishing but the abstract of this paper is pretty good um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human Minds in light of recent advances in machine intelligence a number of scientists philosophers and technologists have revived the discussion about the potentially catastrophic risks entailed by such an entity in this article we trace the origins and development of the Neo fear of super intelligence and some of the major proposals for its containment we argue that total containment is in principle impossible due to fundamental limits inherent income in Computing itself assuming that a super intelligence will contain a program that includes all the programs that can be executed by a universal turing machine on input potentially as complex as the state of the world strict containment requires simulations of such a program something theoretically and practically impossible so to put this in other words in order to anticipate what the machine is going to do you have to be able to simulate it perfectly including the the way that it interacts and the way that the world reacts um basically the whole world is too complicated to simulate with any accuracy and so therefore you know if this if this super intelligent machine exists it's impossible to calculate what the rest of the world is going to do in order to put it in a accurate simulation so we could at best do some you know approximations um so skipping down a little bit they have two main classes for can controlling so the the first class for controlling the machine is capability control you put it in a box you give it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous if you have a machine that can build a copy of itself or that understands programming it could you know undo all of those um pretty much motivation selection so uh direct specification you say you give it uh hard-coded laws uh Domesticity you teach it to behave within certain certain constraints um indirect normativity you endow it with procedures of selecting Superior moral rules so this is closest to what I have done in my work benevolent by design whereby my proposal is that you give it a set of heuristic imperatives that it will um it will ultimately buy into and deliberately choose so that you don't have to control it it controls itself because it abides by certain principles that are going to be reliable um I believe that this is with the experiments that I've captured in in benevolent by Design I believe this is the way to go um augmentation at AI to a benign systems such as the human brain so that's you know merging um okay so you can check out the paper if you want to look at their discussion on that um but I wanted to go skip down to the um the discussion part uh and then I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh not quite as robust because they're not proposing a solution like I have um which is why I'm here okay so today we run billions of computer programs globally on connected to machines without any formal guarantee of their absolute safety we have no way of proving that when we launch an application on our smartphone our smartphones we would not trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war um uh as a technologist this really hurts um let's talk about firewalls let's talk about security protocols you actually can mathematically prove something like that with penetration testing um like there's all sorts of controls and constraints that go into every layer of a piece of technology such as what that piece of technology can can talk to even the security of how it boots up right like we have encrypted boot protocols that ensure that the operating system hasn't been tampered with so uh just right there like yes if you if you're not familiar with how technology works you could conceivably come to this conclusion so we'll just kind of set that on the the B pile of like maybe scientists don't know everything um okay Arthur C Clarke wrote a short story dial F from Frankenstein warning that soon all the computers on Earth were connected via telephone close enough they could take command of our society um they could still use our smartphones and nothing has happened despite the general unsolvability of the program prediction problem we are confident for all practical purposes that we are not in one of the Troublesome cases okay so practical safety you know can you simulate it can you control it um yeah so my point there is uh look up the OSI model and look up security best practices um all right so but I jotted down some notes if predictability is the key thing here um why aren't humans a bigger problem humans are fundamentally unpredictable so why aren't we a Danger let's explore that the reason that human well I mean first humans are dangerous so humans are dangerous um but the limit is uh but each individual is limited we only have so much time and energy and intelligence that we can apply it per day um so let's just call that um physical limits processing energy time those are the primary things is we can only think so fast we can only you know punch so many people in the face if we decide to get violent and we only have so much time uh per day as well as other constraints like need for food but that falls under energy so constraints so law of constraints um so when when we look at the the constraints that humans have computers all have the same thing like you can program the most evil smartphone thing but it's going to be limited because it's only got its One battery to go on right and it's only got It's one 4G or 5G connection or Wi-Fi connection and it's also only got a tiny little you know like quad core arm processor or whatever um and so when you when you when you talk about a super intelligence you have to look at the full stack how much CPU does it have how much RAM does it have how much storage does it have how fast are its internet connection so that again I'm thinking about this from a technologist perspective what kind of firewalls are around it because you can have the smartest thing in the world but if it only has like one protocol out and you've got really robust firewalls it's not getting out unless it convinces something on the outside to let it out and of course that's like you know one of the one of the possibilities but then you can still have controls that prevent that you know interlocks that prevent that from happening um so they're basically there will always be constraints of some sort um and then for computers um it's uh want per flops is the uh is the is the primary constraint and um humans are presently uh one million times more efficient so our brain runs on about 20 watts of energy and it is an exit scale computer as best we configure and the first exascale computer in the world runs on 21 million Watts so 20 watts versus 21 million Watts um you do the math so we've got a long time before these things can compete with us just energetically speaking um all right so then intelligence however so this is this is where um where looking at it from a from a a psychometrics perspective or a neuroscience perspective is very different from looking at it from a computational perspective even though fundamentally they're both math right it's both representing intelligence as numbers the fundamental question is um intelligence is mostly about speed now so there's there's two there's two things right there's there's capability right um there's also um there's there's also speed so can you do something yes or no like you know can do you know how to build a rocket yes or no or can you figure out how to build a rocket now some people are not mentally capable of certain tasks right and then but if you're above a certain threshold of intelligence um then you are theoretically capable of any intellectual task this in practice this is not always true because again we have constraints mostly time right time and processing power it takes time to learn things for us humans um but it's it's about speed so assume that the the biggest question is are there any tasks that the AI can do that humans fundamentally cannot if that is true if the AI can do things can can solve problems that humans cannot then it is beyond human comprehension so let me just jot that down if the AI can perform mental cognitive tasks that humans are incapable of only then is it truly Beyond human comprehension otherwise it's just doing human level tasks only faster and jot that down otherwise it's only doing human tasks but faster or more in parallel now it would not be safe to assume that um that a machine would never be capable of doing things that a human cannot for instance the James Webb Space Telescope it can see the beginning of the universe um because of how powerful its mirrors are to concentrate the faintest light so we can generally design and build machines that are capable of doing things that we cannot right so this is not necessarily a good constraint but it's just another um thought experiment right um we generally build machines that do things we cannot now but then you think you're like okay what about a dump truck right a dump truck is a super powerful machine it can pick up you know the largest dump trucks can carry a thousand tons at a time um and humans cannot do that individually but then you look at the the megaliths that we have moved with you know log rollers and ropes and rafts and sleds and even then generally the most powerful machines in the world are just amplifying ordinary human capabilities um ditto with you know spreadsheets right spreadsheets were originally done by hand right by by bankers and um and statisticians um so you know that's that's still the fundamental question is will the machine be able to do things that we fundamentally cannot um I don't know yet I I have not yet seen anything on the open-ended side such as like with large language models there's nothing that they're doing that we fundamentally cannot they just do it faster um so then if if it's about speed um can we humans um outpace machine thought if the machines cost um if they're if they're two if they're too energetically expensive to run massively in parallel then just collectively we can outpace the machines so there is that um let's see because what you know what we what we always assume happens in those nightmare scenarios is that the machine wakes up and suddenly it takes over the world before we know what's going on right it relies those fear scenarios rely based on speed and that's why I emphasize speed it's all about speed and then what are the constraints of those speed of the speed which is uh primarily like watt per flops that is the that is the primary physical constraint on machine intelligence um okay then lastly the implicit Assumption of individual agency or what we might call ego why do we make this assumption we cannot help but anthropomorphize the machine so this is going to take a little bit more explaining but basically we humans are so used to thinking of intelligent entities like ourselves that they fundamentally have a finite sense of self just like us that they think in terms of I and me and this is what I want and this is what I'm going to do so my very next video is going to be an experiment where I I test this I test different agent models can we produce a machine that has a fundamentally different kind of agency or a fundamentally different kind of ego or what I call an agent model so this is an agent model um which is a an information system about the entity um so the information system about the entity what I mean by that is like I know that I am a human with two hands two feet and a brain I generally know what I know and what I'm capable of and I I also generally know what I um what I'm not capable of right like I can't jump over my house um that's part of my agent model um so to make it a little bit more specific a self-referential information system about the entity um so like what kinds of agent models of agent models are possible do they have to be I um anyways so I just wanted to set the stage my very next video will be about testing agent models and seeing how that affects um the way that uh and uh how that affects alignment now before I let you go there is one other thing that I wanted to um wanted to show you and this is this is much more recent so 36 alarming Automation and job statistics our robots and um let's go away machines and AI coming for your job so this is this is a uh more recent and it's from zippya you know take it with a grain of salt um since 2000 at least 2600 or sorry 260 000 jobs have been lost in the and the US due to automation so two percent of the country's manufacturing Workforce and they are only increasing exponentially again take it with a grain of salt automation is also predicted to create 58 million new jobs though automation could eliminate in the result of 73 million jobs so we're at where the the the the the yield curves to borrow a finance term have inverted where yes automation is creating new jobs but it's creating uh Automation and AI ER creating fewer new jobs than it's creating so a net loss of 15 million jobs that's a lot um so uh the reason that I bring this up is because my state of the industry video yesterday um there was an article about how um uh AI art is disrupting that industry and it's not just from an artistic perspective there are countless um uh graphic artists that you know could very soon be facing um job loss or job change and you know if if the net change is that a quarter you know like yes there are some new jobs because now there's going to be new jobs of people like you know content creators and marketers and whoever just using these tools great new jobs but then how many people are going to lose their job in the meantime and if they have like if they're not able to retrain um or if the net net uh change is fewer jobs then that means some people will by definition mathematically be permanently excluded from the job market and so because of that um I went and looked up some statistics just to see like you know is this is this uh true um you know again take it with a grain of salt um but so my work with auto Muse um I had some breakthroughs yesterday and I realized that I am very close to writing novel length fiction um that's going to be pretty coherent and then and and there's a few other things that I don't even want to say out loud because um because of these breakthroughs and um I don't want to put novelists or editors out of work um just because you can do something doesn't mean you should and I think about like I would lose all my friends if I did that if I if I created if I created a tool if I finished Auto Muse and it can just churn out novels decent enough novels um all of my best friends are writers and some of them are aspiring to do it full time and if I if I am capable of it then I know that someone else is going to be capable of it um before too long but you know I'm I'm ahead of the curve so basically I'm going to put a pause on my auto Muse work that's the that's the short version I'm going to keep doing it privately um just to see what what is possible um but yeah like I don't want to put people out of work like what's the point right what's why why are we here like I don't I understand that the point of capitalism and neoliberalism is to generate more efficiency provide goods and services um more efficiently but at the same time we are facing uh potentially very disruptive and and disruptive is a very soft word for painful um major economic disruptions are painful like people lose their jobs people lose their homes people go hungry um people forego major life decisions um so like disruption is a euphemism right and um so I realize that I am now in a place where I need to be careful with what I release and it also made me wonder if openai deliberately crippled Dolly so that it does not produce Fine Art Level uh Generations um so that it would be less disruptive I don't know like that's a that's a discussion that they would have had internally and they probably wouldn't have published it but someone did tell me that they deliberately crippled faces and they did it on ostensibly for safety right where eyes and eyes and mouths usually look a little bit weird on Dolly generations and I wonder if they did that not just for safety but out of a sense of Ethics like to to um to like hinder their own tool so that it is less likely to um to displace jobs I don't know I don't know just speculating but that's where I'm at so that's the state of the industry update for this morning thanks for watching like And subscribe and consider supporting me on patreon